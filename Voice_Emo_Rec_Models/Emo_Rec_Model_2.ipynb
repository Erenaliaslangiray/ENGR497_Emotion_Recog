{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Eren Ali Aslangiray\n",
    "\n",
    "import librosa\n",
    "from librosa import display\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "sr=22050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author's Notes:\n",
    "\n",
    "At part 1, I tried to build various models and with using different feature extraction methods to find best data extraction and model to use it on mass model building step. So part 1 is mini version and the exeperimental area of the whole work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------DATA PREPARING----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- RAVDESS DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1- Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/erenmac/Desktop/NEW_DATA_VOICE/Audio_Speech_Actors_01-24'\n",
    "lst = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if os.path.join(subdir,file) == \"/Users/erenmac/Desktop/NEW_DATA_VOICE/Audio_Speech_Actors_01-24/.DS_Store\":\n",
    "            continue\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file))\n",
    "        file = int(file[7:8]) - 1\n",
    "        arr = X, file\n",
    "        lst.append(arr)\n",
    "        if sample_rate != 22050:\n",
    "            print (sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = zip(*lst)\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_name = 'X.joblib'\n",
    "y_name = 'y.joblib'\n",
    "save_dir = '/Users/erenmac/Desktop/NEW_DATA_VOICE/Audio_Speech_Actors_01-24/Joblib_saves'\n",
    "\n",
    "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
    "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2- Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = joblib.load('/Users/erenmac/Desktop/NEW_DATA_VOICE/Audio_Speech_Actors_01-24/Joblib_saves/X.joblib')\n",
    "y = joblib.load('/Users/erenmac/Desktop/NEW_DATA_VOICE/Audio_Speech_Actors_01-24/Joblib_saves/y.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3- MEAN MFCCs KERAS MODEL (With 8 Emotion Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(128, 5,padding='same',\n",
    "                 input_shape=(40,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 5128      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 87,944\n",
      "Trainable params: 87,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(cnnhistory.history['acc'])\n",
    "plt.plot(cnnhistory.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(x_testcnn)\n",
    "new_Ytest = y_test.astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(new_Ytest, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(new_Ytest, predictions)\n",
    "print (matrix)\n",
    "\n",
    "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
    "save_dir = '/Users/erenmac/Desktop/ENGR498/Code/Voice_Emo_Rec_Models'\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model('Emotion_Voice_Detection_Model.h5')\n",
    "loaded_model.summary()\n",
    "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4- MEAN MFCCs KERAS MODEL (With reduced labels. Labels = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dont forget to load data from 1.2\n",
    "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised\n",
    "# 0 = neutral, 1 = angry, 2 = happy, 3 = sad\n",
    "#calm became neutral. fearful, disgust, surprised will be removed. \n",
    "i = 0\n",
    "for item in y:\n",
    "    if item == 1:\n",
    "        y[i]=0\n",
    "    i = i+1\n",
    "i = 0\n",
    "for item in y:\n",
    "    if item==4:\n",
    "        y[i]=1\n",
    "    i = i +1\n",
    "\n",
    "lista = list(X)\n",
    "listb = list(y)\n",
    "dellist = [5,6,7]\n",
    "\n",
    "for i in range (len(listb)-1,-1,-1):\n",
    "    if listb[i] in dellist:\n",
    "        lista.pop(i)\n",
    "        listb.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.asarray(lista), np.asarray(listb), test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DecisionTree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, y_train)\n",
    "predictions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86       378\n",
      "           1       0.91      0.87      0.89       238\n",
      "           2       0.85      0.81      0.83       267\n",
      "           3       0.75      0.82      0.79       243\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1126\n",
      "   macro avg       0.84      0.84      0.84      1126\n",
      "weighted avg       0.84      0.84      0.84      1126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Now its time for keras :)\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "x_traincnn = np.expand_dims(X_train, axis=2)\n",
    "x_testcnn = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(128, 5,padding='same',\n",
    "                 input_shape=(40,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2285 samples, validate on 1126 samples\n",
      "Epoch 1/1000\n",
      "2285/2285 [==============================] - 1s 570us/step - loss: 4.8675 - acc: 0.2713 - val_loss: 1.4530 - val_acc: 0.3242\n",
      "Epoch 2/1000\n",
      "2285/2285 [==============================] - 1s 394us/step - loss: 3.9055 - acc: 0.2893 - val_loss: 2.1263 - val_acc: 0.3650\n",
      "Epoch 3/1000\n",
      "2285/2285 [==============================] - 1s 414us/step - loss: 3.2906 - acc: 0.3103 - val_loss: 2.3639 - val_acc: 0.3579\n",
      "Epoch 4/1000\n",
      "2285/2285 [==============================] - 1s 434us/step - loss: 2.9058 - acc: 0.3151 - val_loss: 1.4374 - val_acc: 0.4165\n",
      "Epoch 5/1000\n",
      "2285/2285 [==============================] - 1s 402us/step - loss: 2.3511 - acc: 0.3554 - val_loss: 1.3819 - val_acc: 0.4503\n",
      "Epoch 6/1000\n",
      "2285/2285 [==============================] - 1s 402us/step - loss: 2.1364 - acc: 0.3488 - val_loss: 1.2185 - val_acc: 0.4609\n",
      "Epoch 7/1000\n",
      "2285/2285 [==============================] - 1s 404us/step - loss: 1.8759 - acc: 0.3751 - val_loss: 1.1921 - val_acc: 0.4867\n",
      "Epoch 8/1000\n",
      "2285/2285 [==============================] - 1s 479us/step - loss: 1.6602 - acc: 0.3886 - val_loss: 1.3041 - val_acc: 0.4574\n",
      "Epoch 9/1000\n",
      "2285/2285 [==============================] - 1s 507us/step - loss: 1.5155 - acc: 0.4101 - val_loss: 1.2000 - val_acc: 0.3961\n",
      "Epoch 10/1000\n",
      "2285/2285 [==============================] - 1s 595us/step - loss: 1.4284 - acc: 0.4389 - val_loss: 1.0902 - val_acc: 0.5622\n",
      "Epoch 11/1000\n",
      "2285/2285 [==============================] - 1s 481us/step - loss: 1.3576 - acc: 0.4420 - val_loss: 1.0874 - val_acc: 0.5027\n",
      "Epoch 12/1000\n",
      "2285/2285 [==============================] - 1s 407us/step - loss: 1.2815 - acc: 0.4613 - val_loss: 1.0735 - val_acc: 0.5417\n",
      "Epoch 13/1000\n",
      "2285/2285 [==============================] - 1s 420us/step - loss: 1.2440 - acc: 0.4635 - val_loss: 1.0893 - val_acc: 0.5684\n",
      "Epoch 14/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 1.2322 - acc: 0.4709 - val_loss: 1.0546 - val_acc: 0.5853\n",
      "Epoch 15/1000\n",
      "2285/2285 [==============================] - 1s 394us/step - loss: 1.2223 - acc: 0.4670 - val_loss: 1.0429 - val_acc: 0.5861\n",
      "Epoch 16/1000\n",
      "2285/2285 [==============================] - 1s 406us/step - loss: 1.1680 - acc: 0.4880 - val_loss: 1.0426 - val_acc: 0.5924\n",
      "Epoch 17/1000\n",
      "2285/2285 [==============================] - 1s 548us/step - loss: 1.1637 - acc: 0.5050 - val_loss: 1.0301 - val_acc: 0.5666\n",
      "Epoch 18/1000\n",
      "2285/2285 [==============================] - 1s 627us/step - loss: 1.1397 - acc: 0.5077 - val_loss: 1.0292 - val_acc: 0.5835\n",
      "Epoch 19/1000\n",
      "2285/2285 [==============================] - 1s 553us/step - loss: 1.1050 - acc: 0.5269 - val_loss: 1.0146 - val_acc: 0.6323\n",
      "Epoch 20/1000\n",
      "2285/2285 [==============================] - 1s 482us/step - loss: 1.0763 - acc: 0.5392 - val_loss: 1.0167 - val_acc: 0.6083\n",
      "Epoch 21/1000\n",
      "2285/2285 [==============================] - 1s 461us/step - loss: 1.0733 - acc: 0.5282 - val_loss: 0.9800 - val_acc: 0.6101\n",
      "Epoch 22/1000\n",
      "2285/2285 [==============================] - 1s 554us/step - loss: 1.0567 - acc: 0.5532 - val_loss: 1.0006 - val_acc: 0.6306\n",
      "Epoch 23/1000\n",
      "2285/2285 [==============================] - 1s 612us/step - loss: 1.0426 - acc: 0.5733 - val_loss: 1.0687 - val_acc: 0.5266\n",
      "Epoch 24/1000\n",
      "2285/2285 [==============================] - 1s 472us/step - loss: 1.0420 - acc: 0.5681 - val_loss: 0.9640 - val_acc: 0.5959\n",
      "Epoch 25/1000\n",
      "2285/2285 [==============================] - 1s 452us/step - loss: 1.0241 - acc: 0.5632 - val_loss: 0.9413 - val_acc: 0.6252\n",
      "Epoch 26/1000\n",
      "2285/2285 [==============================] - 1s 458us/step - loss: 1.0121 - acc: 0.5807 - val_loss: 0.9441 - val_acc: 0.5959\n",
      "Epoch 27/1000\n",
      "2285/2285 [==============================] - 1s 422us/step - loss: 1.0077 - acc: 0.5689 - val_loss: 0.9213 - val_acc: 0.6554\n",
      "Epoch 28/1000\n",
      "2285/2285 [==============================] - 1s 451us/step - loss: 1.0000 - acc: 0.5869 - val_loss: 0.9118 - val_acc: 0.6323\n",
      "Epoch 29/1000\n",
      "2285/2285 [==============================] - 1s 588us/step - loss: 0.9552 - acc: 0.6044 - val_loss: 0.8836 - val_acc: 0.6634\n",
      "Epoch 30/1000\n",
      "2285/2285 [==============================] - 1s 620us/step - loss: 0.9658 - acc: 0.6031 - val_loss: 0.8870 - val_acc: 0.6741\n",
      "Epoch 31/1000\n",
      "2285/2285 [==============================] - 1s 462us/step - loss: 0.9519 - acc: 0.6035 - val_loss: 0.8856 - val_acc: 0.6794\n",
      "Epoch 32/1000\n",
      "2285/2285 [==============================] - 1s 453us/step - loss: 0.9369 - acc: 0.6232 - val_loss: 0.8846 - val_acc: 0.6563\n",
      "Epoch 33/1000\n",
      "2285/2285 [==============================] - 1s 486us/step - loss: 0.9199 - acc: 0.6197 - val_loss: 0.8716 - val_acc: 0.6767\n",
      "Epoch 34/1000\n",
      "2285/2285 [==============================] - 1s 632us/step - loss: 0.9097 - acc: 0.6337 - val_loss: 0.8664 - val_acc: 0.6572\n",
      "Epoch 35/1000\n",
      "2285/2285 [==============================] - 1s 609us/step - loss: 0.9116 - acc: 0.6293 - val_loss: 0.8764 - val_acc: 0.6519\n",
      "Epoch 36/1000\n",
      "2285/2285 [==============================] - 2s 672us/step - loss: 0.8857 - acc: 0.6403 - val_loss: 0.8293 - val_acc: 0.6829\n",
      "Epoch 37/1000\n",
      "2285/2285 [==============================] - 1s 420us/step - loss: 0.8762 - acc: 0.6516 - val_loss: 0.8590 - val_acc: 0.6563\n",
      "Epoch 38/1000\n",
      "2285/2285 [==============================] - 1s 411us/step - loss: 0.8849 - acc: 0.6372 - val_loss: 0.8194 - val_acc: 0.7114\n",
      "Epoch 39/1000\n",
      "2285/2285 [==============================] - 1s 442us/step - loss: 0.8572 - acc: 0.6565 - val_loss: 0.8085 - val_acc: 0.7060\n",
      "Epoch 40/1000\n",
      "2285/2285 [==============================] - 1s 519us/step - loss: 0.8651 - acc: 0.6569 - val_loss: 0.8117 - val_acc: 0.7060\n",
      "Epoch 41/1000\n",
      "2285/2285 [==============================] - 1s 370us/step - loss: 0.8564 - acc: 0.6573 - val_loss: 0.8150 - val_acc: 0.6954\n",
      "Epoch 42/1000\n",
      "2285/2285 [==============================] - 1s 420us/step - loss: 0.8517 - acc: 0.6630 - val_loss: 0.8007 - val_acc: 0.6989\n",
      "Epoch 43/1000\n",
      "2285/2285 [==============================] - 1s 497us/step - loss: 0.8483 - acc: 0.6608 - val_loss: 0.7908 - val_acc: 0.7158\n",
      "Epoch 44/1000\n",
      "2285/2285 [==============================] - 1s 460us/step - loss: 0.8246 - acc: 0.6748 - val_loss: 0.7944 - val_acc: 0.6909\n",
      "Epoch 45/1000\n",
      "2285/2285 [==============================] - 1s 448us/step - loss: 0.8162 - acc: 0.6665 - val_loss: 0.7893 - val_acc: 0.6901\n",
      "Epoch 46/1000\n",
      "2285/2285 [==============================] - 1s 449us/step - loss: 0.8205 - acc: 0.6753 - val_loss: 0.7789 - val_acc: 0.7096\n",
      "Epoch 47/1000\n",
      "2285/2285 [==============================] - 1s 465us/step - loss: 0.7983 - acc: 0.6845 - val_loss: 0.7644 - val_acc: 0.7158\n",
      "Epoch 48/1000\n",
      "2285/2285 [==============================] - 1s 458us/step - loss: 0.8069 - acc: 0.6823 - val_loss: 0.7592 - val_acc: 0.7309\n",
      "Epoch 49/1000\n",
      "2285/2285 [==============================] - 1s 455us/step - loss: 0.7927 - acc: 0.6810 - val_loss: 0.7608 - val_acc: 0.7140\n",
      "Epoch 50/1000\n",
      "2285/2285 [==============================] - 1s 445us/step - loss: 0.7885 - acc: 0.6910 - val_loss: 0.7653 - val_acc: 0.7069\n",
      "Epoch 51/1000\n",
      "2285/2285 [==============================] - 1s 477us/step - loss: 0.7854 - acc: 0.6853 - val_loss: 0.7630 - val_acc: 0.7282\n",
      "Epoch 52/1000\n",
      "2285/2285 [==============================] - 1s 372us/step - loss: 0.7721 - acc: 0.6897 - val_loss: 0.7487 - val_acc: 0.7123\n",
      "Epoch 53/1000\n",
      "2285/2285 [==============================] - 1s 348us/step - loss: 0.7827 - acc: 0.6880 - val_loss: 0.7605 - val_acc: 0.6927\n",
      "Epoch 54/1000\n",
      "2285/2285 [==============================] - 1s 339us/step - loss: 0.7663 - acc: 0.6937 - val_loss: 0.7416 - val_acc: 0.7202\n",
      "Epoch 55/1000\n",
      "2285/2285 [==============================] - 1s 376us/step - loss: 0.7582 - acc: 0.7077 - val_loss: 0.7191 - val_acc: 0.7202\n",
      "Epoch 56/1000\n",
      "2285/2285 [==============================] - 1s 366us/step - loss: 0.7476 - acc: 0.7011 - val_loss: 0.7261 - val_acc: 0.7389\n",
      "Epoch 57/1000\n",
      "2285/2285 [==============================] - 1s 336us/step - loss: 0.7392 - acc: 0.7173 - val_loss: 0.7386 - val_acc: 0.7247\n",
      "Epoch 58/1000\n",
      "2285/2285 [==============================] - 1s 354us/step - loss: 0.7345 - acc: 0.7160 - val_loss: 0.7145 - val_acc: 0.7513\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 372us/step - loss: 0.7378 - acc: 0.7120 - val_loss: 0.7126 - val_acc: 0.7460\n",
      "Epoch 60/1000\n",
      "2285/2285 [==============================] - 1s 524us/step - loss: 0.7221 - acc: 0.7278 - val_loss: 0.7147 - val_acc: 0.7291\n",
      "Epoch 61/1000\n",
      "2285/2285 [==============================] - 1s 394us/step - loss: 0.7387 - acc: 0.7042 - val_loss: 0.6994 - val_acc: 0.7336\n",
      "Epoch 62/1000\n",
      "2285/2285 [==============================] - 1s 329us/step - loss: 0.7204 - acc: 0.7204 - val_loss: 0.6914 - val_acc: 0.7282\n",
      "Epoch 63/1000\n",
      "2285/2285 [==============================] - 1s 353us/step - loss: 0.7213 - acc: 0.7287 - val_loss: 0.6987 - val_acc: 0.7265\n",
      "Epoch 64/1000\n",
      "2285/2285 [==============================] - 1s 367us/step - loss: 0.7035 - acc: 0.7304 - val_loss: 0.6862 - val_acc: 0.7300\n",
      "Epoch 65/1000\n",
      "2285/2285 [==============================] - 1s 479us/step - loss: 0.6973 - acc: 0.7344 - val_loss: 0.6964 - val_acc: 0.7496\n",
      "Epoch 66/1000\n",
      "2285/2285 [==============================] - 1s 336us/step - loss: 0.7053 - acc: 0.7177 - val_loss: 0.6855 - val_acc: 0.7602\n",
      "Epoch 67/1000\n",
      "2285/2285 [==============================] - 1s 337us/step - loss: 0.6993 - acc: 0.7278 - val_loss: 0.7315 - val_acc: 0.6989\n",
      "Epoch 68/1000\n",
      "2285/2285 [==============================] - 1s 333us/step - loss: 0.6832 - acc: 0.7256 - val_loss: 0.6683 - val_acc: 0.7575\n",
      "Epoch 69/1000\n",
      "2285/2285 [==============================] - 1s 341us/step - loss: 0.6842 - acc: 0.7322 - val_loss: 0.6796 - val_acc: 0.7451\n",
      "Epoch 70/1000\n",
      "2285/2285 [==============================] - 1s 337us/step - loss: 0.6826 - acc: 0.7339 - val_loss: 0.6731 - val_acc: 0.7380\n",
      "Epoch 71/1000\n",
      "2285/2285 [==============================] - 1s 347us/step - loss: 0.6836 - acc: 0.7427 - val_loss: 0.6835 - val_acc: 0.7433\n",
      "Epoch 72/1000\n",
      "2285/2285 [==============================] - 1s 441us/step - loss: 0.6753 - acc: 0.7383 - val_loss: 0.6782 - val_acc: 0.7256\n",
      "Epoch 73/1000\n",
      "2285/2285 [==============================] - 1s 435us/step - loss: 0.6773 - acc: 0.7322 - val_loss: 0.6550 - val_acc: 0.7584\n",
      "Epoch 74/1000\n",
      "2285/2285 [==============================] - 1s 446us/step - loss: 0.6682 - acc: 0.7422 - val_loss: 0.6478 - val_acc: 0.7620\n",
      "Epoch 75/1000\n",
      "2285/2285 [==============================] - 1s 528us/step - loss: 0.6557 - acc: 0.7519 - val_loss: 0.6503 - val_acc: 0.7611\n",
      "Epoch 76/1000\n",
      "2285/2285 [==============================] - 1s 447us/step - loss: 0.6528 - acc: 0.7470 - val_loss: 0.6583 - val_acc: 0.7398\n",
      "Epoch 77/1000\n",
      "2285/2285 [==============================] - 1s 438us/step - loss: 0.6661 - acc: 0.7339 - val_loss: 0.6416 - val_acc: 0.7620\n",
      "Epoch 78/1000\n",
      "2285/2285 [==============================] - 1s 403us/step - loss: 0.6449 - acc: 0.7584 - val_loss: 0.6465 - val_acc: 0.7460\n",
      "Epoch 79/1000\n",
      "2285/2285 [==============================] - 1s 382us/step - loss: 0.6471 - acc: 0.7435 - val_loss: 0.6463 - val_acc: 0.7496\n",
      "Epoch 80/1000\n",
      "2285/2285 [==============================] - 1s 327us/step - loss: 0.6455 - acc: 0.7422 - val_loss: 0.6317 - val_acc: 0.7682\n",
      "Epoch 81/1000\n",
      "2285/2285 [==============================] - 1s 329us/step - loss: 0.6442 - acc: 0.7457 - val_loss: 0.6348 - val_acc: 0.7549\n",
      "Epoch 82/1000\n",
      "2285/2285 [==============================] - 1s 506us/step - loss: 0.6336 - acc: 0.7580 - val_loss: 0.6343 - val_acc: 0.7558\n",
      "Epoch 83/1000\n",
      "2285/2285 [==============================] - 1s 401us/step - loss: 0.6285 - acc: 0.7554 - val_loss: 0.6303 - val_acc: 0.7655\n",
      "Epoch 84/1000\n",
      "2285/2285 [==============================] - 1s 544us/step - loss: 0.6285 - acc: 0.7593 - val_loss: 0.6569 - val_acc: 0.7433\n",
      "Epoch 85/1000\n",
      "2285/2285 [==============================] - 1s 504us/step - loss: 0.6225 - acc: 0.7523 - val_loss: 0.6143 - val_acc: 0.7673\n",
      "Epoch 86/1000\n",
      "2285/2285 [==============================] - 1s 411us/step - loss: 0.6194 - acc: 0.7540 - val_loss: 0.6137 - val_acc: 0.7815\n",
      "Epoch 87/1000\n",
      "2285/2285 [==============================] - 1s 401us/step - loss: 0.6173 - acc: 0.7698 - val_loss: 0.6096 - val_acc: 0.7762\n",
      "Epoch 88/1000\n",
      "2285/2285 [==============================] - 1s 401us/step - loss: 0.6192 - acc: 0.7624 - val_loss: 0.6113 - val_acc: 0.7789\n",
      "Epoch 89/1000\n",
      "2285/2285 [==============================] - 1s 368us/step - loss: 0.6263 - acc: 0.7497 - val_loss: 0.6179 - val_acc: 0.7700\n",
      "Epoch 90/1000\n",
      "2285/2285 [==============================] - 1s 383us/step - loss: 0.6166 - acc: 0.7554 - val_loss: 0.6193 - val_acc: 0.7593\n",
      "Epoch 91/1000\n",
      "2285/2285 [==============================] - 1s 322us/step - loss: 0.6145 - acc: 0.7562 - val_loss: 0.6203 - val_acc: 0.7611\n",
      "Epoch 92/1000\n",
      "2285/2285 [==============================] - 1s 325us/step - loss: 0.5998 - acc: 0.7746 - val_loss: 0.6330 - val_acc: 0.7575\n",
      "Epoch 93/1000\n",
      "2285/2285 [==============================] - 1s 394us/step - loss: 0.5986 - acc: 0.7759 - val_loss: 0.6146 - val_acc: 0.7504\n",
      "Epoch 94/1000\n",
      "2285/2285 [==============================] - 1s 556us/step - loss: 0.5881 - acc: 0.7742 - val_loss: 0.6107 - val_acc: 0.7664\n",
      "Epoch 95/1000\n",
      "2285/2285 [==============================] - 1s 452us/step - loss: 0.5918 - acc: 0.7781 - val_loss: 0.5929 - val_acc: 0.7726\n",
      "Epoch 96/1000\n",
      "2285/2285 [==============================] - 1s 386us/step - loss: 0.5975 - acc: 0.7672 - val_loss: 0.6101 - val_acc: 0.7584\n",
      "Epoch 97/1000\n",
      "2285/2285 [==============================] - 1s 392us/step - loss: 0.6041 - acc: 0.7584 - val_loss: 0.5889 - val_acc: 0.7824\n",
      "Epoch 98/1000\n",
      "2285/2285 [==============================] - 1s 400us/step - loss: 0.5924 - acc: 0.7667 - val_loss: 0.5962 - val_acc: 0.7691\n",
      "Epoch 99/1000\n",
      "2285/2285 [==============================] - 1s 542us/step - loss: 0.5691 - acc: 0.7877 - val_loss: 0.5807 - val_acc: 0.7860\n",
      "Epoch 100/1000\n",
      "2285/2285 [==============================] - 2s 657us/step - loss: 0.5802 - acc: 0.7737 - val_loss: 0.5828 - val_acc: 0.7886\n",
      "Epoch 101/1000\n",
      "2285/2285 [==============================] - 1s 475us/step - loss: 0.5578 - acc: 0.7856 - val_loss: 0.5789 - val_acc: 0.7886\n",
      "Epoch 102/1000\n",
      "2285/2285 [==============================] - 1s 447us/step - loss: 0.5795 - acc: 0.7751 - val_loss: 0.5613 - val_acc: 0.7948\n",
      "Epoch 103/1000\n",
      "2285/2285 [==============================] - 1s 416us/step - loss: 0.5750 - acc: 0.7689 - val_loss: 0.5880 - val_acc: 0.7842\n",
      "Epoch 104/1000\n",
      "2285/2285 [==============================] - 1s 421us/step - loss: 0.5699 - acc: 0.7856 - val_loss: 0.5743 - val_acc: 0.7682\n",
      "Epoch 105/1000\n",
      "2285/2285 [==============================] - 1s 479us/step - loss: 0.5681 - acc: 0.7886 - val_loss: 0.5608 - val_acc: 0.7877\n",
      "Epoch 106/1000\n",
      "2285/2285 [==============================] - 1s 437us/step - loss: 0.5596 - acc: 0.7864 - val_loss: 0.5563 - val_acc: 0.7975\n",
      "Epoch 107/1000\n",
      "2285/2285 [==============================] - 1s 447us/step - loss: 0.5673 - acc: 0.7777 - val_loss: 0.5675 - val_acc: 0.7798\n",
      "Epoch 108/1000\n",
      "2285/2285 [==============================] - 1s 482us/step - loss: 0.5567 - acc: 0.7891 - val_loss: 0.5644 - val_acc: 0.7664\n",
      "Epoch 109/1000\n",
      "2285/2285 [==============================] - 1s 461us/step - loss: 0.5478 - acc: 0.7904 - val_loss: 0.5609 - val_acc: 0.7780\n",
      "Epoch 110/1000\n",
      "2285/2285 [==============================] - 1s 467us/step - loss: 0.5403 - acc: 0.7939 - val_loss: 0.5819 - val_acc: 0.7593\n",
      "Epoch 111/1000\n",
      "2285/2285 [==============================] - 1s 614us/step - loss: 0.5522 - acc: 0.7891 - val_loss: 0.5507 - val_acc: 0.7904\n",
      "Epoch 112/1000\n",
      "2285/2285 [==============================] - 1s 559us/step - loss: 0.5541 - acc: 0.7912 - val_loss: 0.5608 - val_acc: 0.7753\n",
      "Epoch 113/1000\n",
      "2285/2285 [==============================] - 1s 506us/step - loss: 0.5486 - acc: 0.7939 - val_loss: 0.5720 - val_acc: 0.7771\n",
      "Epoch 114/1000\n",
      "2285/2285 [==============================] - 1s 633us/step - loss: 0.5345 - acc: 0.7891 - val_loss: 0.5615 - val_acc: 0.7789\n",
      "Epoch 115/1000\n",
      "2285/2285 [==============================] - 1s 510us/step - loss: 0.5258 - acc: 0.7895 - val_loss: 0.5331 - val_acc: 0.7984\n",
      "Epoch 116/1000\n",
      "2285/2285 [==============================] - 1s 416us/step - loss: 0.5392 - acc: 0.7961 - val_loss: 0.5671 - val_acc: 0.7806\n",
      "Epoch 117/1000\n",
      "2285/2285 [==============================] - 1s 413us/step - loss: 0.5212 - acc: 0.8066 - val_loss: 0.5380 - val_acc: 0.7984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/1000\n",
      "2285/2285 [==============================] - 1s 475us/step - loss: 0.5299 - acc: 0.7982 - val_loss: 0.5385 - val_acc: 0.7895\n",
      "Epoch 119/1000\n",
      "2285/2285 [==============================] - 1s 505us/step - loss: 0.5190 - acc: 0.8057 - val_loss: 0.5426 - val_acc: 0.7895\n",
      "Epoch 120/1000\n",
      "2285/2285 [==============================] - 1s 557us/step - loss: 0.5231 - acc: 0.7939 - val_loss: 0.5574 - val_acc: 0.7753\n",
      "Epoch 121/1000\n",
      "2285/2285 [==============================] - 1s 636us/step - loss: 0.5325 - acc: 0.7891 - val_loss: 0.5524 - val_acc: 0.7895\n",
      "Epoch 122/1000\n",
      "2285/2285 [==============================] - 1s 585us/step - loss: 0.5237 - acc: 0.7956 - val_loss: 0.5402 - val_acc: 0.7886\n",
      "Epoch 123/1000\n",
      "2285/2285 [==============================] - 1s 553us/step - loss: 0.5109 - acc: 0.7956 - val_loss: 0.5399 - val_acc: 0.7753\n",
      "Epoch 124/1000\n",
      "2285/2285 [==============================] - 1s 561us/step - loss: 0.5249 - acc: 0.7961 - val_loss: 0.5403 - val_acc: 0.7913\n",
      "Epoch 125/1000\n",
      "2285/2285 [==============================] - 1s 429us/step - loss: 0.5070 - acc: 0.8057 - val_loss: 0.5357 - val_acc: 0.7806\n",
      "Epoch 126/1000\n",
      "2285/2285 [==============================] - 1s 381us/step - loss: 0.5014 - acc: 0.8031 - val_loss: 0.5274 - val_acc: 0.7842\n",
      "Epoch 127/1000\n",
      "2285/2285 [==============================] - 1s 377us/step - loss: 0.4992 - acc: 0.8092 - val_loss: 0.5389 - val_acc: 0.7966\n",
      "Epoch 128/1000\n",
      "2285/2285 [==============================] - 1s 367us/step - loss: 0.4998 - acc: 0.8031 - val_loss: 0.5380 - val_acc: 0.7735\n",
      "Epoch 129/1000\n",
      "2285/2285 [==============================] - 1s 376us/step - loss: 0.5050 - acc: 0.8118 - val_loss: 0.5330 - val_acc: 0.7815\n",
      "Epoch 130/1000\n",
      "2285/2285 [==============================] - 1s 377us/step - loss: 0.5015 - acc: 0.8074 - val_loss: 0.5233 - val_acc: 0.7931\n",
      "Epoch 131/1000\n",
      "2285/2285 [==============================] - 1s 402us/step - loss: 0.5037 - acc: 0.8035 - val_loss: 0.5066 - val_acc: 0.8046\n",
      "Epoch 132/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 0.4962 - acc: 0.8000 - val_loss: 0.5104 - val_acc: 0.8020\n",
      "Epoch 133/1000\n",
      "2285/2285 [==============================] - 1s 391us/step - loss: 0.4911 - acc: 0.8109 - val_loss: 0.5161 - val_acc: 0.7931\n",
      "Epoch 134/1000\n",
      "2285/2285 [==============================] - 1s 432us/step - loss: 0.4951 - acc: 0.8061 - val_loss: 0.5102 - val_acc: 0.8099\n",
      "Epoch 135/1000\n",
      "2285/2285 [==============================] - 1s 431us/step - loss: 0.4880 - acc: 0.8088 - val_loss: 0.5156 - val_acc: 0.7913\n",
      "Epoch 136/1000\n",
      "2285/2285 [==============================] - 1s 425us/step - loss: 0.4897 - acc: 0.8175 - val_loss: 0.5104 - val_acc: 0.7984\n",
      "Epoch 137/1000\n",
      "2285/2285 [==============================] - 1s 404us/step - loss: 0.4852 - acc: 0.8105 - val_loss: 0.5282 - val_acc: 0.7815\n",
      "Epoch 138/1000\n",
      "2285/2285 [==============================] - 1s 414us/step - loss: 0.4854 - acc: 0.8153 - val_loss: 0.5042 - val_acc: 0.8055\n",
      "Epoch 139/1000\n",
      "2285/2285 [==============================] - 1s 407us/step - loss: 0.4859 - acc: 0.8114 - val_loss: 0.5008 - val_acc: 0.8037\n",
      "Epoch 140/1000\n",
      "2285/2285 [==============================] - 1s 410us/step - loss: 0.4864 - acc: 0.8088 - val_loss: 0.5132 - val_acc: 0.7957\n",
      "Epoch 141/1000\n",
      "2285/2285 [==============================] - 1s 418us/step - loss: 0.4888 - acc: 0.8153 - val_loss: 0.5089 - val_acc: 0.8002\n",
      "Epoch 142/1000\n",
      "2285/2285 [==============================] - 1s 454us/step - loss: 0.4749 - acc: 0.8197 - val_loss: 0.5090 - val_acc: 0.7984\n",
      "Epoch 143/1000\n",
      "2285/2285 [==============================] - 1s 433us/step - loss: 0.4707 - acc: 0.8289 - val_loss: 0.4955 - val_acc: 0.8011\n",
      "Epoch 144/1000\n",
      "2285/2285 [==============================] - 1s 439us/step - loss: 0.4708 - acc: 0.8136 - val_loss: 0.4931 - val_acc: 0.8126\n",
      "Epoch 145/1000\n",
      "2285/2285 [==============================] - 1s 463us/step - loss: 0.4721 - acc: 0.8210 - val_loss: 0.5129 - val_acc: 0.7966\n",
      "Epoch 146/1000\n",
      "2285/2285 [==============================] - 1s 448us/step - loss: 0.4645 - acc: 0.8267 - val_loss: 0.4912 - val_acc: 0.8135\n",
      "Epoch 147/1000\n",
      "2285/2285 [==============================] - 1s 454us/step - loss: 0.4599 - acc: 0.8311 - val_loss: 0.5125 - val_acc: 0.7886\n",
      "Epoch 148/1000\n",
      "2285/2285 [==============================] - 1s 451us/step - loss: 0.4673 - acc: 0.8184 - val_loss: 0.4801 - val_acc: 0.8197\n",
      "Epoch 149/1000\n",
      "2285/2285 [==============================] - 1s 525us/step - loss: 0.4686 - acc: 0.8236 - val_loss: 0.4874 - val_acc: 0.8099\n",
      "Epoch 150/1000\n",
      "2285/2285 [==============================] - 1s 506us/step - loss: 0.4545 - acc: 0.8254 - val_loss: 0.5032 - val_acc: 0.8046\n",
      "Epoch 151/1000\n",
      "2285/2285 [==============================] - 1s 456us/step - loss: 0.4540 - acc: 0.8245 - val_loss: 0.5160 - val_acc: 0.7869\n",
      "Epoch 152/1000\n",
      "2285/2285 [==============================] - 1s 419us/step - loss: 0.4551 - acc: 0.8289 - val_loss: 0.4919 - val_acc: 0.8055\n",
      "Epoch 153/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.4411 - acc: 0.8346 - val_loss: 0.4722 - val_acc: 0.8135\n",
      "Epoch 154/1000\n",
      "2285/2285 [==============================] - 1s 432us/step - loss: 0.4364 - acc: 0.8289 - val_loss: 0.4809 - val_acc: 0.8064\n",
      "Epoch 155/1000\n",
      "2285/2285 [==============================] - 1s 416us/step - loss: 0.4566 - acc: 0.8254 - val_loss: 0.4750 - val_acc: 0.8108\n",
      "Epoch 156/1000\n",
      "2285/2285 [==============================] - 1s 465us/step - loss: 0.4504 - acc: 0.8232 - val_loss: 0.4781 - val_acc: 0.8126\n",
      "Epoch 157/1000\n",
      "2285/2285 [==============================] - 1s 411us/step - loss: 0.4433 - acc: 0.8306 - val_loss: 0.4698 - val_acc: 0.8171\n",
      "Epoch 158/1000\n",
      "2285/2285 [==============================] - 1s 497us/step - loss: 0.4417 - acc: 0.8241 - val_loss: 0.4672 - val_acc: 0.8162\n",
      "Epoch 159/1000\n",
      "2285/2285 [==============================] - 1s 595us/step - loss: 0.4281 - acc: 0.8372 - val_loss: 0.5528 - val_acc: 0.7789\n",
      "Epoch 160/1000\n",
      "2285/2285 [==============================] - 1s 533us/step - loss: 0.4347 - acc: 0.8328 - val_loss: 0.4707 - val_acc: 0.8162\n",
      "Epoch 161/1000\n",
      "2285/2285 [==============================] - 1s 481us/step - loss: 0.4411 - acc: 0.8337 - val_loss: 0.4679 - val_acc: 0.8224\n",
      "Epoch 162/1000\n",
      "2285/2285 [==============================] - 1s 589us/step - loss: 0.4238 - acc: 0.8346 - val_loss: 0.4654 - val_acc: 0.8135\n",
      "Epoch 163/1000\n",
      "2285/2285 [==============================] - 1s 559us/step - loss: 0.4339 - acc: 0.8363 - val_loss: 0.4558 - val_acc: 0.8268\n",
      "Epoch 164/1000\n",
      "2285/2285 [==============================] - 1s 541us/step - loss: 0.4276 - acc: 0.8354 - val_loss: 0.4692 - val_acc: 0.8011\n",
      "Epoch 165/1000\n",
      "2285/2285 [==============================] - 1s 616us/step - loss: 0.4373 - acc: 0.8328 - val_loss: 0.4925 - val_acc: 0.8055\n",
      "Epoch 166/1000\n",
      "2285/2285 [==============================] - 1s 538us/step - loss: 0.4240 - acc: 0.8420 - val_loss: 0.4654 - val_acc: 0.8082\n",
      "Epoch 167/1000\n",
      "2285/2285 [==============================] - 1s 482us/step - loss: 0.4313 - acc: 0.8254 - val_loss: 0.4716 - val_acc: 0.8099\n",
      "Epoch 168/1000\n",
      "2285/2285 [==============================] - 1s 544us/step - loss: 0.4266 - acc: 0.8319 - val_loss: 0.4571 - val_acc: 0.8144\n",
      "Epoch 169/1000\n",
      "2285/2285 [==============================] - 1s 543us/step - loss: 0.4286 - acc: 0.8446 - val_loss: 0.4601 - val_acc: 0.8117\n",
      "Epoch 170/1000\n",
      "2285/2285 [==============================] - 1s 512us/step - loss: 0.4314 - acc: 0.8346 - val_loss: 0.4476 - val_acc: 0.8268\n",
      "Epoch 171/1000\n",
      "2285/2285 [==============================] - 1s 474us/step - loss: 0.4209 - acc: 0.8368 - val_loss: 0.4483 - val_acc: 0.8250\n",
      "Epoch 172/1000\n",
      "2285/2285 [==============================] - 1s 445us/step - loss: 0.4302 - acc: 0.8306 - val_loss: 0.4445 - val_acc: 0.8206\n",
      "Epoch 173/1000\n",
      "2285/2285 [==============================] - 1s 450us/step - loss: 0.4098 - acc: 0.8354 - val_loss: 0.4467 - val_acc: 0.8206\n",
      "Epoch 174/1000\n",
      "2285/2285 [==============================] - 1s 406us/step - loss: 0.4250 - acc: 0.8289 - val_loss: 0.4643 - val_acc: 0.8206\n",
      "Epoch 175/1000\n",
      "2285/2285 [==============================] - 1s 537us/step - loss: 0.4044 - acc: 0.8468 - val_loss: 0.4629 - val_acc: 0.8144\n",
      "Epoch 176/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 628us/step - loss: 0.4097 - acc: 0.8446 - val_loss: 0.4431 - val_acc: 0.8215\n",
      "Epoch 177/1000\n",
      "2285/2285 [==============================] - 1s 559us/step - loss: 0.4161 - acc: 0.8468 - val_loss: 0.4620 - val_acc: 0.8197\n",
      "Epoch 178/1000\n",
      "2285/2285 [==============================] - 1s 448us/step - loss: 0.3965 - acc: 0.8468 - val_loss: 0.4496 - val_acc: 0.8099\n",
      "Epoch 179/1000\n",
      "2285/2285 [==============================] - 1s 401us/step - loss: 0.4095 - acc: 0.8438 - val_loss: 0.4454 - val_acc: 0.8286\n",
      "Epoch 180/1000\n",
      "2285/2285 [==============================] - 1s 459us/step - loss: 0.4194 - acc: 0.8363 - val_loss: 0.4560 - val_acc: 0.8153\n",
      "Epoch 181/1000\n",
      "2285/2285 [==============================] - 1s 451us/step - loss: 0.4093 - acc: 0.8473 - val_loss: 0.4584 - val_acc: 0.8037\n",
      "Epoch 182/1000\n",
      "2285/2285 [==============================] - 1s 627us/step - loss: 0.4016 - acc: 0.8407 - val_loss: 0.4422 - val_acc: 0.8242\n",
      "Epoch 183/1000\n",
      "2285/2285 [==============================] - 1s 494us/step - loss: 0.4033 - acc: 0.8468 - val_loss: 0.4378 - val_acc: 0.8197\n",
      "Epoch 184/1000\n",
      "2285/2285 [==============================] - 1s 406us/step - loss: 0.3891 - acc: 0.8556 - val_loss: 0.4508 - val_acc: 0.8268\n",
      "Epoch 185/1000\n",
      "2285/2285 [==============================] - 1s 441us/step - loss: 0.3894 - acc: 0.8508 - val_loss: 0.4399 - val_acc: 0.8339\n",
      "Epoch 186/1000\n",
      "2285/2285 [==============================] - 1s 431us/step - loss: 0.3973 - acc: 0.8460 - val_loss: 0.4350 - val_acc: 0.8268\n",
      "Epoch 187/1000\n",
      "2285/2285 [==============================] - 1s 436us/step - loss: 0.3973 - acc: 0.8433 - val_loss: 0.4381 - val_acc: 0.8188\n",
      "Epoch 188/1000\n",
      "2285/2285 [==============================] - 1s 446us/step - loss: 0.3948 - acc: 0.8512 - val_loss: 0.4312 - val_acc: 0.8330\n",
      "Epoch 189/1000\n",
      "2285/2285 [==============================] - 1s 406us/step - loss: 0.3875 - acc: 0.8534 - val_loss: 0.4241 - val_acc: 0.8286\n",
      "Epoch 190/1000\n",
      "2285/2285 [==============================] - 1s 399us/step - loss: 0.3801 - acc: 0.8608 - val_loss: 0.4413 - val_acc: 0.8286\n",
      "Epoch 191/1000\n",
      "2285/2285 [==============================] - 1s 377us/step - loss: 0.3851 - acc: 0.8573 - val_loss: 0.4418 - val_acc: 0.8153\n",
      "Epoch 192/1000\n",
      "2285/2285 [==============================] - 1s 371us/step - loss: 0.3926 - acc: 0.8521 - val_loss: 0.4366 - val_acc: 0.8393\n",
      "Epoch 193/1000\n",
      "2285/2285 [==============================] - 1s 405us/step - loss: 0.3812 - acc: 0.8578 - val_loss: 0.4265 - val_acc: 0.8330\n",
      "Epoch 194/1000\n",
      "2285/2285 [==============================] - 1s 427us/step - loss: 0.3911 - acc: 0.8490 - val_loss: 0.4228 - val_acc: 0.8313\n",
      "Epoch 195/1000\n",
      "2285/2285 [==============================] - 1s 438us/step - loss: 0.3927 - acc: 0.8560 - val_loss: 0.4299 - val_acc: 0.8268\n",
      "Epoch 196/1000\n",
      "2285/2285 [==============================] - 1s 622us/step - loss: 0.3735 - acc: 0.8613 - val_loss: 0.4216 - val_acc: 0.8393\n",
      "Epoch 197/1000\n",
      "2285/2285 [==============================] - 1s 546us/step - loss: 0.3792 - acc: 0.8560 - val_loss: 0.4636 - val_acc: 0.8197\n",
      "Epoch 198/1000\n",
      "2285/2285 [==============================] - 1s 436us/step - loss: 0.3792 - acc: 0.8547 - val_loss: 0.4132 - val_acc: 0.8410\n",
      "Epoch 199/1000\n",
      "2285/2285 [==============================] - 1s 449us/step - loss: 0.3742 - acc: 0.8635 - val_loss: 0.4329 - val_acc: 0.8153\n",
      "Epoch 200/1000\n",
      "2285/2285 [==============================] - 1s 424us/step - loss: 0.3726 - acc: 0.8530 - val_loss: 0.4523 - val_acc: 0.8171\n",
      "Epoch 201/1000\n",
      "2285/2285 [==============================] - 1s 425us/step - loss: 0.3875 - acc: 0.8560 - val_loss: 0.4289 - val_acc: 0.8259\n",
      "Epoch 202/1000\n",
      "2285/2285 [==============================] - 1s 418us/step - loss: 0.3641 - acc: 0.8635 - val_loss: 0.4142 - val_acc: 0.8375\n",
      "Epoch 203/1000\n",
      "2285/2285 [==============================] - 1s 430us/step - loss: 0.3759 - acc: 0.8586 - val_loss: 0.4087 - val_acc: 0.8250\n",
      "Epoch 204/1000\n",
      "2285/2285 [==============================] - 1s 417us/step - loss: 0.3723 - acc: 0.8578 - val_loss: 0.4243 - val_acc: 0.8259\n",
      "Epoch 205/1000\n",
      "2285/2285 [==============================] - 1s 384us/step - loss: 0.3620 - acc: 0.8648 - val_loss: 0.4333 - val_acc: 0.8250\n",
      "Epoch 206/1000\n",
      "2285/2285 [==============================] - 1s 391us/step - loss: 0.3669 - acc: 0.8656 - val_loss: 0.4162 - val_acc: 0.8286\n",
      "Epoch 207/1000\n",
      "2285/2285 [==============================] - 1s 399us/step - loss: 0.3677 - acc: 0.8661 - val_loss: 0.4254 - val_acc: 0.8197\n",
      "Epoch 208/1000\n",
      "2285/2285 [==============================] - 1s 381us/step - loss: 0.3529 - acc: 0.8670 - val_loss: 0.3956 - val_acc: 0.8410\n",
      "Epoch 209/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 0.3609 - acc: 0.8630 - val_loss: 0.4101 - val_acc: 0.8401\n",
      "Epoch 210/1000\n",
      "2285/2285 [==============================] - 1s 454us/step - loss: 0.3547 - acc: 0.8665 - val_loss: 0.4065 - val_acc: 0.8268\n",
      "Epoch 211/1000\n",
      "2285/2285 [==============================] - 1s 560us/step - loss: 0.3403 - acc: 0.8770 - val_loss: 0.4059 - val_acc: 0.8437\n",
      "Epoch 212/1000\n",
      "2285/2285 [==============================] - 1s 637us/step - loss: 0.3570 - acc: 0.8656 - val_loss: 0.3992 - val_acc: 0.8357\n",
      "Epoch 213/1000\n",
      "2285/2285 [==============================] - 1s 625us/step - loss: 0.3489 - acc: 0.8696 - val_loss: 0.4019 - val_acc: 0.8481\n",
      "Epoch 214/1000\n",
      "2285/2285 [==============================] - 1s 466us/step - loss: 0.3467 - acc: 0.8691 - val_loss: 0.3979 - val_acc: 0.8384\n",
      "Epoch 215/1000\n",
      "2285/2285 [==============================] - 2s 662us/step - loss: 0.3648 - acc: 0.8608 - val_loss: 0.3952 - val_acc: 0.8348\n",
      "Epoch 216/1000\n",
      "2285/2285 [==============================] - 1s 496us/step - loss: 0.3551 - acc: 0.8635 - val_loss: 0.4090 - val_acc: 0.8233\n",
      "Epoch 217/1000\n",
      "2285/2285 [==============================] - 1s 434us/step - loss: 0.3582 - acc: 0.8621 - val_loss: 0.4052 - val_acc: 0.8437\n",
      "Epoch 218/1000\n",
      "2285/2285 [==============================] - 1s 423us/step - loss: 0.3508 - acc: 0.8770 - val_loss: 0.4101 - val_acc: 0.8295\n",
      "Epoch 219/1000\n",
      "2285/2285 [==============================] - 1s 423us/step - loss: 0.3405 - acc: 0.8827 - val_loss: 0.4000 - val_acc: 0.8375\n",
      "Epoch 220/1000\n",
      "2285/2285 [==============================] - 1s 461us/step - loss: 0.3550 - acc: 0.8687 - val_loss: 0.4105 - val_acc: 0.8375\n",
      "Epoch 221/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.3507 - acc: 0.8687 - val_loss: 0.3993 - val_acc: 0.8393\n",
      "Epoch 222/1000\n",
      "2285/2285 [==============================] - 1s 414us/step - loss: 0.3570 - acc: 0.8621 - val_loss: 0.3870 - val_acc: 0.8437\n",
      "Epoch 223/1000\n",
      "2285/2285 [==============================] - 1s 455us/step - loss: 0.3444 - acc: 0.8735 - val_loss: 0.3947 - val_acc: 0.8321\n",
      "Epoch 224/1000\n",
      "2285/2285 [==============================] - 1s 403us/step - loss: 0.3442 - acc: 0.8757 - val_loss: 0.3972 - val_acc: 0.8321\n",
      "Epoch 225/1000\n",
      "2285/2285 [==============================] - 1s 447us/step - loss: 0.3410 - acc: 0.8735 - val_loss: 0.4109 - val_acc: 0.8259\n",
      "Epoch 226/1000\n",
      "2285/2285 [==============================] - 1s 412us/step - loss: 0.3325 - acc: 0.8757 - val_loss: 0.3928 - val_acc: 0.8481\n",
      "Epoch 227/1000\n",
      "2285/2285 [==============================] - 1s 428us/step - loss: 0.3443 - acc: 0.8652 - val_loss: 0.3843 - val_acc: 0.8393\n",
      "Epoch 228/1000\n",
      "2285/2285 [==============================] - 1s 447us/step - loss: 0.3382 - acc: 0.8700 - val_loss: 0.3780 - val_acc: 0.8508\n",
      "Epoch 229/1000\n",
      "2285/2285 [==============================] - 1s 403us/step - loss: 0.3484 - acc: 0.8674 - val_loss: 0.3938 - val_acc: 0.8437\n",
      "Epoch 230/1000\n",
      "2285/2285 [==============================] - 1s 436us/step - loss: 0.3251 - acc: 0.8775 - val_loss: 0.3906 - val_acc: 0.8393\n",
      "Epoch 231/1000\n",
      "2285/2285 [==============================] - 1s 449us/step - loss: 0.3304 - acc: 0.8779 - val_loss: 0.3974 - val_acc: 0.8242\n",
      "Epoch 232/1000\n",
      "2285/2285 [==============================] - 1s 592us/step - loss: 0.3317 - acc: 0.8796 - val_loss: 0.3761 - val_acc: 0.8419\n",
      "Epoch 233/1000\n",
      "2285/2285 [==============================] - 1s 642us/step - loss: 0.3280 - acc: 0.8753 - val_loss: 0.3864 - val_acc: 0.8517\n",
      "Epoch 234/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 494us/step - loss: 0.3312 - acc: 0.8788 - val_loss: 0.3830 - val_acc: 0.8446\n",
      "Epoch 235/1000\n",
      "2285/2285 [==============================] - 1s 375us/step - loss: 0.3230 - acc: 0.8832 - val_loss: 0.3908 - val_acc: 0.8339\n",
      "Epoch 236/1000\n",
      "2285/2285 [==============================] - 1s 431us/step - loss: 0.3287 - acc: 0.8761 - val_loss: 0.3975 - val_acc: 0.8224\n",
      "Epoch 237/1000\n",
      "2285/2285 [==============================] - 1s 393us/step - loss: 0.3246 - acc: 0.8832 - val_loss: 0.3774 - val_acc: 0.8437\n",
      "Epoch 238/1000\n",
      "2285/2285 [==============================] - 1s 381us/step - loss: 0.3121 - acc: 0.8827 - val_loss: 0.3852 - val_acc: 0.8384\n",
      "Epoch 239/1000\n",
      "2285/2285 [==============================] - 1s 383us/step - loss: 0.3295 - acc: 0.8713 - val_loss: 0.3976 - val_acc: 0.8286\n",
      "Epoch 240/1000\n",
      "2285/2285 [==============================] - 1s 373us/step - loss: 0.3325 - acc: 0.8779 - val_loss: 0.3617 - val_acc: 0.8455\n",
      "Epoch 241/1000\n",
      "2285/2285 [==============================] - 1s 388us/step - loss: 0.3134 - acc: 0.8902 - val_loss: 0.3884 - val_acc: 0.8437\n",
      "Epoch 242/1000\n",
      "2285/2285 [==============================] - 1s 423us/step - loss: 0.3153 - acc: 0.8792 - val_loss: 0.3806 - val_acc: 0.8419\n",
      "Epoch 243/1000\n",
      "2285/2285 [==============================] - 1s 381us/step - loss: 0.3150 - acc: 0.8766 - val_loss: 0.3718 - val_acc: 0.8490\n",
      "Epoch 244/1000\n",
      "2285/2285 [==============================] - 1s 431us/step - loss: 0.3196 - acc: 0.8783 - val_loss: 0.3744 - val_acc: 0.8615\n",
      "Epoch 245/1000\n",
      "2285/2285 [==============================] - 1s 419us/step - loss: 0.3217 - acc: 0.8805 - val_loss: 0.3720 - val_acc: 0.8410\n",
      "Epoch 246/1000\n",
      "2285/2285 [==============================] - 1s 437us/step - loss: 0.3231 - acc: 0.8779 - val_loss: 0.3669 - val_acc: 0.8535\n",
      "Epoch 247/1000\n",
      "2285/2285 [==============================] - 1s 435us/step - loss: 0.3065 - acc: 0.8862 - val_loss: 0.3916 - val_acc: 0.8446\n",
      "Epoch 248/1000\n",
      "2285/2285 [==============================] - 1s 451us/step - loss: 0.3075 - acc: 0.8810 - val_loss: 0.3845 - val_acc: 0.8606\n",
      "Epoch 249/1000\n",
      "2285/2285 [==============================] - 1s 409us/step - loss: 0.3087 - acc: 0.8823 - val_loss: 0.3657 - val_acc: 0.8544\n",
      "Epoch 250/1000\n",
      "2285/2285 [==============================] - 1s 395us/step - loss: 0.3043 - acc: 0.8910 - val_loss: 0.3709 - val_acc: 0.8481\n",
      "Epoch 251/1000\n",
      "2285/2285 [==============================] - 1s 405us/step - loss: 0.3086 - acc: 0.8770 - val_loss: 0.3830 - val_acc: 0.8544\n",
      "Epoch 252/1000\n",
      "2285/2285 [==============================] - 1s 534us/step - loss: 0.3036 - acc: 0.8945 - val_loss: 0.3672 - val_acc: 0.8579\n",
      "Epoch 253/1000\n",
      "2285/2285 [==============================] - 1s 631us/step - loss: 0.3078 - acc: 0.8814 - val_loss: 0.3777 - val_acc: 0.8561\n",
      "Epoch 254/1000\n",
      "2285/2285 [==============================] - 1s 477us/step - loss: 0.3068 - acc: 0.8910 - val_loss: 0.3511 - val_acc: 0.8606\n",
      "Epoch 255/1000\n",
      "2285/2285 [==============================] - 1s 458us/step - loss: 0.3073 - acc: 0.8923 - val_loss: 0.3644 - val_acc: 0.8561\n",
      "Epoch 256/1000\n",
      "2285/2285 [==============================] - 1s 444us/step - loss: 0.3156 - acc: 0.8862 - val_loss: 0.3585 - val_acc: 0.8499\n",
      "Epoch 257/1000\n",
      "2285/2285 [==============================] - 1s 421us/step - loss: 0.3093 - acc: 0.8845 - val_loss: 0.3725 - val_acc: 0.8552\n",
      "Epoch 258/1000\n",
      "2285/2285 [==============================] - 1s 541us/step - loss: 0.3034 - acc: 0.8867 - val_loss: 0.3683 - val_acc: 0.8561\n",
      "Epoch 259/1000\n",
      "2285/2285 [==============================] - 2s 672us/step - loss: 0.3061 - acc: 0.8871 - val_loss: 0.3653 - val_acc: 0.8499\n",
      "Epoch 260/1000\n",
      "2285/2285 [==============================] - 1s 579us/step - loss: 0.2887 - acc: 0.8941 - val_loss: 0.3725 - val_acc: 0.8508\n",
      "Epoch 261/1000\n",
      "2285/2285 [==============================] - 2s 689us/step - loss: 0.2979 - acc: 0.8858 - val_loss: 0.4231 - val_acc: 0.8330\n",
      "Epoch 262/1000\n",
      "2285/2285 [==============================] - 1s 570us/step - loss: 0.2959 - acc: 0.8976 - val_loss: 0.3881 - val_acc: 0.8588\n",
      "Epoch 263/1000\n",
      "2285/2285 [==============================] - 2s 668us/step - loss: 0.2871 - acc: 0.8910 - val_loss: 0.3470 - val_acc: 0.8677\n",
      "Epoch 264/1000\n",
      "2285/2285 [==============================] - 2s 661us/step - loss: 0.2883 - acc: 0.9011 - val_loss: 0.3539 - val_acc: 0.8570\n",
      "Epoch 265/1000\n",
      "2285/2285 [==============================] - 1s 646us/step - loss: 0.3032 - acc: 0.8858 - val_loss: 0.3784 - val_acc: 0.8490\n",
      "Epoch 266/1000\n",
      "2285/2285 [==============================] - 1s 642us/step - loss: 0.2982 - acc: 0.8937 - val_loss: 0.4101 - val_acc: 0.8277\n",
      "Epoch 267/1000\n",
      "2285/2285 [==============================] - 1s 560us/step - loss: 0.2894 - acc: 0.8945 - val_loss: 0.3456 - val_acc: 0.8615\n",
      "Epoch 268/1000\n",
      "2285/2285 [==============================] - 1s 481us/step - loss: 0.2841 - acc: 0.9020 - val_loss: 0.3591 - val_acc: 0.8668\n",
      "Epoch 269/1000\n",
      "2285/2285 [==============================] - 1s 546us/step - loss: 0.2864 - acc: 0.8985 - val_loss: 0.3424 - val_acc: 0.8650\n",
      "Epoch 270/1000\n",
      "2285/2285 [==============================] - 1s 541us/step - loss: 0.2869 - acc: 0.8941 - val_loss: 0.3556 - val_acc: 0.8570\n",
      "Epoch 271/1000\n",
      "2285/2285 [==============================] - 1s 573us/step - loss: 0.2852 - acc: 0.9046 - val_loss: 0.3398 - val_acc: 0.8641\n",
      "Epoch 272/1000\n",
      "2285/2285 [==============================] - 1s 530us/step - loss: 0.2890 - acc: 0.8963 - val_loss: 0.3651 - val_acc: 0.8632\n",
      "Epoch 273/1000\n",
      "2285/2285 [==============================] - 1s 614us/step - loss: 0.2815 - acc: 0.8941 - val_loss: 0.3614 - val_acc: 0.8579\n",
      "Epoch 274/1000\n",
      "2285/2285 [==============================] - 1s 509us/step - loss: 0.2819 - acc: 0.8989 - val_loss: 0.3360 - val_acc: 0.8721\n",
      "Epoch 275/1000\n",
      "2285/2285 [==============================] - 1s 547us/step - loss: 0.2899 - acc: 0.8958 - val_loss: 0.3375 - val_acc: 0.8766\n",
      "Epoch 276/1000\n",
      "2285/2285 [==============================] - 1s 520us/step - loss: 0.2714 - acc: 0.8976 - val_loss: 0.3509 - val_acc: 0.8623\n",
      "Epoch 277/1000\n",
      "2285/2285 [==============================] - 1s 555us/step - loss: 0.2675 - acc: 0.9050 - val_loss: 0.3691 - val_acc: 0.8615\n",
      "Epoch 278/1000\n",
      "2285/2285 [==============================] - 1s 598us/step - loss: 0.2783 - acc: 0.9028 - val_loss: 0.3405 - val_acc: 0.8686\n",
      "Epoch 279/1000\n",
      "2285/2285 [==============================] - 1s 627us/step - loss: 0.2686 - acc: 0.9037 - val_loss: 0.3485 - val_acc: 0.8615\n",
      "Epoch 280/1000\n",
      "2285/2285 [==============================] - 1s 519us/step - loss: 0.2774 - acc: 0.8958 - val_loss: 0.3404 - val_acc: 0.8588\n",
      "Epoch 281/1000\n",
      "2285/2285 [==============================] - 1s 492us/step - loss: 0.2800 - acc: 0.8906 - val_loss: 0.3382 - val_acc: 0.8774\n",
      "Epoch 282/1000\n",
      "2285/2285 [==============================] - 1s 497us/step - loss: 0.2799 - acc: 0.8941 - val_loss: 0.3244 - val_acc: 0.8650\n",
      "Epoch 283/1000\n",
      "2285/2285 [==============================] - 1s 497us/step - loss: 0.2811 - acc: 0.8985 - val_loss: 0.3357 - val_acc: 0.8792\n",
      "Epoch 284/1000\n",
      "2285/2285 [==============================] - 1s 508us/step - loss: 0.2718 - acc: 0.9024 - val_loss: 0.3377 - val_acc: 0.8606\n",
      "Epoch 285/1000\n",
      "2285/2285 [==============================] - 1s 514us/step - loss: 0.2820 - acc: 0.8967 - val_loss: 0.3305 - val_acc: 0.8677\n",
      "Epoch 286/1000\n",
      "2285/2285 [==============================] - 1s 495us/step - loss: 0.2710 - acc: 0.9094 - val_loss: 0.3251 - val_acc: 0.8721\n",
      "Epoch 287/1000\n",
      "2285/2285 [==============================] - 1s 506us/step - loss: 0.2761 - acc: 0.9011 - val_loss: 0.3320 - val_acc: 0.8828\n",
      "Epoch 288/1000\n",
      "2285/2285 [==============================] - 1s 502us/step - loss: 0.2678 - acc: 0.9015 - val_loss: 0.3322 - val_acc: 0.8597\n",
      "Epoch 289/1000\n",
      "2285/2285 [==============================] - 1s 497us/step - loss: 0.2710 - acc: 0.9068 - val_loss: 0.3224 - val_acc: 0.8748\n",
      "Epoch 290/1000\n",
      "2285/2285 [==============================] - 1s 504us/step - loss: 0.2705 - acc: 0.8989 - val_loss: 0.3309 - val_acc: 0.8668\n",
      "Epoch 291/1000\n",
      "2285/2285 [==============================] - 1s 499us/step - loss: 0.2601 - acc: 0.9063 - val_loss: 0.3258 - val_acc: 0.8739\n",
      "Epoch 292/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 448us/step - loss: 0.2584 - acc: 0.9050 - val_loss: 0.3214 - val_acc: 0.8739\n",
      "Epoch 293/1000\n",
      "2285/2285 [==============================] - 1s 439us/step - loss: 0.2516 - acc: 0.9107 - val_loss: 0.3540 - val_acc: 0.8561\n",
      "Epoch 294/1000\n",
      "2285/2285 [==============================] - 1s 429us/step - loss: 0.2711 - acc: 0.9068 - val_loss: 0.3371 - val_acc: 0.8508\n",
      "Epoch 295/1000\n",
      "2285/2285 [==============================] - 1s 430us/step - loss: 0.2596 - acc: 0.9103 - val_loss: 0.3232 - val_acc: 0.8819\n",
      "Epoch 296/1000\n",
      "2285/2285 [==============================] - 1s 433us/step - loss: 0.2672 - acc: 0.9007 - val_loss: 0.3367 - val_acc: 0.8757\n",
      "Epoch 297/1000\n",
      "2285/2285 [==============================] - 1s 437us/step - loss: 0.2631 - acc: 0.9002 - val_loss: 0.3219 - val_acc: 0.8783\n",
      "Epoch 298/1000\n",
      "2285/2285 [==============================] - 1s 441us/step - loss: 0.2625 - acc: 0.8950 - val_loss: 0.3188 - val_acc: 0.8837\n",
      "Epoch 299/1000\n",
      "2285/2285 [==============================] - 1s 431us/step - loss: 0.2650 - acc: 0.9020 - val_loss: 0.3230 - val_acc: 0.8774\n",
      "Epoch 300/1000\n",
      "2285/2285 [==============================] - 1s 446us/step - loss: 0.2639 - acc: 0.9068 - val_loss: 0.3279 - val_acc: 0.8712\n",
      "Epoch 301/1000\n",
      "2285/2285 [==============================] - 1s 434us/step - loss: 0.2519 - acc: 0.9024 - val_loss: 0.3142 - val_acc: 0.8766\n",
      "Epoch 302/1000\n",
      "2285/2285 [==============================] - 1s 433us/step - loss: 0.2533 - acc: 0.9094 - val_loss: 0.3133 - val_acc: 0.8739\n",
      "Epoch 303/1000\n",
      "2285/2285 [==============================] - 1s 449us/step - loss: 0.2503 - acc: 0.9107 - val_loss: 0.3137 - val_acc: 0.8801\n",
      "Epoch 304/1000\n",
      "2285/2285 [==============================] - 1s 440us/step - loss: 0.2521 - acc: 0.9120 - val_loss: 0.3247 - val_acc: 0.8783\n",
      "Epoch 305/1000\n",
      "2285/2285 [==============================] - 1s 446us/step - loss: 0.2515 - acc: 0.9055 - val_loss: 0.3100 - val_acc: 0.8730\n",
      "Epoch 306/1000\n",
      "2285/2285 [==============================] - 1s 434us/step - loss: 0.2550 - acc: 0.9072 - val_loss: 0.3120 - val_acc: 0.8792\n",
      "Epoch 307/1000\n",
      "2285/2285 [==============================] - 1s 452us/step - loss: 0.2537 - acc: 0.9055 - val_loss: 0.3179 - val_acc: 0.8801\n",
      "Epoch 308/1000\n",
      "2285/2285 [==============================] - 1s 450us/step - loss: 0.2428 - acc: 0.9173 - val_loss: 0.3226 - val_acc: 0.8748\n",
      "Epoch 309/1000\n",
      "2285/2285 [==============================] - 1s 451us/step - loss: 0.2383 - acc: 0.9186 - val_loss: 0.3116 - val_acc: 0.8810\n",
      "Epoch 310/1000\n",
      "2285/2285 [==============================] - 1s 452us/step - loss: 0.2392 - acc: 0.9190 - val_loss: 0.3154 - val_acc: 0.8632\n",
      "Epoch 311/1000\n",
      "2285/2285 [==============================] - 1s 455us/step - loss: 0.2296 - acc: 0.9199 - val_loss: 0.3092 - val_acc: 0.8854\n",
      "Epoch 312/1000\n",
      "2285/2285 [==============================] - 1s 460us/step - loss: 0.2491 - acc: 0.9116 - val_loss: 0.2962 - val_acc: 0.8828\n",
      "Epoch 313/1000\n",
      "2285/2285 [==============================] - 1s 460us/step - loss: 0.2457 - acc: 0.9107 - val_loss: 0.3250 - val_acc: 0.8757\n",
      "Epoch 314/1000\n",
      "2285/2285 [==============================] - 1s 458us/step - loss: 0.2418 - acc: 0.9094 - val_loss: 0.3075 - val_acc: 0.8801\n",
      "Epoch 315/1000\n",
      "2285/2285 [==============================] - 1s 467us/step - loss: 0.2497 - acc: 0.9042 - val_loss: 0.3082 - val_acc: 0.8828\n",
      "Epoch 316/1000\n",
      "2285/2285 [==============================] - 1s 464us/step - loss: 0.2371 - acc: 0.9164 - val_loss: 0.3150 - val_acc: 0.8757\n",
      "Epoch 317/1000\n",
      "2285/2285 [==============================] - 1s 463us/step - loss: 0.2549 - acc: 0.9072 - val_loss: 0.3250 - val_acc: 0.8801\n",
      "Epoch 318/1000\n",
      "2285/2285 [==============================] - 1s 434us/step - loss: 0.2436 - acc: 0.9094 - val_loss: 0.3123 - val_acc: 0.8792\n",
      "Epoch 319/1000\n",
      "2285/2285 [==============================] - 1s 443us/step - loss: 0.2421 - acc: 0.9068 - val_loss: 0.2955 - val_acc: 0.8881\n",
      "Epoch 320/1000\n",
      "2285/2285 [==============================] - 1s 444us/step - loss: 0.2328 - acc: 0.9217 - val_loss: 0.2981 - val_acc: 0.9023\n",
      "Epoch 321/1000\n",
      "2285/2285 [==============================] - 1s 432us/step - loss: 0.2373 - acc: 0.9160 - val_loss: 0.2995 - val_acc: 0.8863\n",
      "Epoch 322/1000\n",
      "2285/2285 [==============================] - 1s 439us/step - loss: 0.2416 - acc: 0.9138 - val_loss: 0.3175 - val_acc: 0.8712\n",
      "Epoch 323/1000\n",
      "2285/2285 [==============================] - 1s 441us/step - loss: 0.2436 - acc: 0.9090 - val_loss: 0.3042 - val_acc: 0.8908\n",
      "Epoch 324/1000\n",
      "2285/2285 [==============================] - 1s 442us/step - loss: 0.2251 - acc: 0.9204 - val_loss: 0.3017 - val_acc: 0.8890\n",
      "Epoch 325/1000\n",
      "2285/2285 [==============================] - 1s 450us/step - loss: 0.2370 - acc: 0.9164 - val_loss: 0.3020 - val_acc: 0.8890\n",
      "Epoch 326/1000\n",
      "2285/2285 [==============================] - 1s 455us/step - loss: 0.2352 - acc: 0.9177 - val_loss: 0.3066 - val_acc: 0.8792\n",
      "Epoch 327/1000\n",
      "2285/2285 [==============================] - 1s 478us/step - loss: 0.2296 - acc: 0.9173 - val_loss: 0.2908 - val_acc: 0.8908\n",
      "Epoch 328/1000\n",
      "2285/2285 [==============================] - 1s 463us/step - loss: 0.2272 - acc: 0.9186 - val_loss: 0.2955 - val_acc: 0.8917\n",
      "Epoch 329/1000\n",
      "2285/2285 [==============================] - 1s 465us/step - loss: 0.2309 - acc: 0.9230 - val_loss: 0.3018 - val_acc: 0.8917\n",
      "Epoch 330/1000\n",
      "2285/2285 [==============================] - 1s 465us/step - loss: 0.2326 - acc: 0.9138 - val_loss: 0.2984 - val_acc: 0.8881\n",
      "Epoch 331/1000\n",
      "2285/2285 [==============================] - 1s 458us/step - loss: 0.2257 - acc: 0.9182 - val_loss: 0.3065 - val_acc: 0.8810\n",
      "Epoch 332/1000\n",
      "2285/2285 [==============================] - 1s 474us/step - loss: 0.2321 - acc: 0.9164 - val_loss: 0.3110 - val_acc: 0.8908\n",
      "Epoch 333/1000\n",
      "2285/2285 [==============================] - 1s 473us/step - loss: 0.2258 - acc: 0.9195 - val_loss: 0.2955 - val_acc: 0.8943\n",
      "Epoch 334/1000\n",
      "2285/2285 [==============================] - 1s 464us/step - loss: 0.2298 - acc: 0.9186 - val_loss: 0.3014 - val_acc: 0.8863\n",
      "Epoch 335/1000\n",
      "2285/2285 [==============================] - 1s 467us/step - loss: 0.2217 - acc: 0.9243 - val_loss: 0.3105 - val_acc: 0.8890\n",
      "Epoch 336/1000\n",
      "2285/2285 [==============================] - 1s 458us/step - loss: 0.2305 - acc: 0.9177 - val_loss: 0.3084 - val_acc: 0.8783\n",
      "Epoch 337/1000\n",
      "2285/2285 [==============================] - 1s 475us/step - loss: 0.2291 - acc: 0.9173 - val_loss: 0.2975 - val_acc: 0.8863\n",
      "Epoch 338/1000\n",
      "2285/2285 [==============================] - 1s 455us/step - loss: 0.2211 - acc: 0.9260 - val_loss: 0.3136 - val_acc: 0.8845\n",
      "Epoch 339/1000\n",
      "2285/2285 [==============================] - 1s 476us/step - loss: 0.2210 - acc: 0.9230 - val_loss: 0.2789 - val_acc: 0.8979\n",
      "Epoch 340/1000\n",
      "2285/2285 [==============================] - 1s 519us/step - loss: 0.2226 - acc: 0.9278 - val_loss: 0.2996 - val_acc: 0.8881\n",
      "Epoch 341/1000\n",
      "2285/2285 [==============================] - 1s 480us/step - loss: 0.2216 - acc: 0.9199 - val_loss: 0.2854 - val_acc: 0.8970\n",
      "Epoch 342/1000\n",
      "2285/2285 [==============================] - 1s 480us/step - loss: 0.2211 - acc: 0.9186 - val_loss: 0.2824 - val_acc: 0.8828\n",
      "Epoch 343/1000\n",
      "2285/2285 [==============================] - 1s 478us/step - loss: 0.2190 - acc: 0.9256 - val_loss: 0.2922 - val_acc: 0.8881\n",
      "Epoch 344/1000\n",
      "2285/2285 [==============================] - 1s 479us/step - loss: 0.2225 - acc: 0.9208 - val_loss: 0.2923 - val_acc: 0.8917\n",
      "Epoch 345/1000\n",
      "2285/2285 [==============================] - 1s 480us/step - loss: 0.2117 - acc: 0.9234 - val_loss: 0.2974 - val_acc: 0.8943\n",
      "Epoch 346/1000\n",
      "2285/2285 [==============================] - 1s 477us/step - loss: 0.2177 - acc: 0.9204 - val_loss: 0.3121 - val_acc: 0.8872\n",
      "Epoch 347/1000\n",
      "2285/2285 [==============================] - 1s 475us/step - loss: 0.2162 - acc: 0.9247 - val_loss: 0.3059 - val_acc: 0.8854\n",
      "Epoch 348/1000\n",
      "2285/2285 [==============================] - 1s 476us/step - loss: 0.2170 - acc: 0.9252 - val_loss: 0.3065 - val_acc: 0.8934\n",
      "Epoch 349/1000\n",
      "2285/2285 [==============================] - 1s 481us/step - loss: 0.2089 - acc: 0.9295 - val_loss: 0.2780 - val_acc: 0.8979\n",
      "Epoch 350/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 446us/step - loss: 0.2245 - acc: 0.9212 - val_loss: 0.2935 - val_acc: 0.8837\n",
      "Epoch 351/1000\n",
      "2285/2285 [==============================] - 1s 432us/step - loss: 0.2002 - acc: 0.9317 - val_loss: 0.2953 - val_acc: 0.8854\n",
      "Epoch 352/1000\n",
      "2285/2285 [==============================] - 1s 436us/step - loss: 0.2143 - acc: 0.9234 - val_loss: 0.2952 - val_acc: 0.8872\n",
      "Epoch 353/1000\n",
      "2285/2285 [==============================] - 1s 440us/step - loss: 0.2294 - acc: 0.9116 - val_loss: 0.3056 - val_acc: 0.8881\n",
      "Epoch 354/1000\n",
      "2285/2285 [==============================] - 1s 426us/step - loss: 0.2135 - acc: 0.9186 - val_loss: 0.2944 - val_acc: 0.8899\n",
      "Epoch 355/1000\n",
      "2285/2285 [==============================] - 1s 447us/step - loss: 0.2103 - acc: 0.9309 - val_loss: 0.2956 - val_acc: 0.8881\n",
      "Epoch 356/1000\n",
      "2285/2285 [==============================] - 1s 430us/step - loss: 0.2134 - acc: 0.9274 - val_loss: 0.2851 - val_acc: 0.8890\n",
      "Epoch 357/1000\n",
      "2285/2285 [==============================] - 1s 444us/step - loss: 0.2178 - acc: 0.9186 - val_loss: 0.2733 - val_acc: 0.9032\n",
      "Epoch 358/1000\n",
      "2285/2285 [==============================] - 1s 443us/step - loss: 0.2117 - acc: 0.9274 - val_loss: 0.2757 - val_acc: 0.9059\n",
      "Epoch 359/1000\n",
      "2285/2285 [==============================] - 1s 445us/step - loss: 0.2064 - acc: 0.9204 - val_loss: 0.2883 - val_acc: 0.8872\n",
      "Epoch 360/1000\n",
      "2285/2285 [==============================] - 1s 448us/step - loss: 0.2037 - acc: 0.9304 - val_loss: 0.2773 - val_acc: 0.8961\n",
      "Epoch 361/1000\n",
      "2285/2285 [==============================] - 1s 446us/step - loss: 0.2013 - acc: 0.9278 - val_loss: 0.2810 - val_acc: 0.8908\n",
      "Epoch 362/1000\n",
      "2285/2285 [==============================] - 1s 442us/step - loss: 0.2032 - acc: 0.9304 - val_loss: 0.2753 - val_acc: 0.8970\n",
      "Epoch 363/1000\n",
      "2285/2285 [==============================] - 1s 450us/step - loss: 0.2113 - acc: 0.9252 - val_loss: 0.2829 - val_acc: 0.9005\n",
      "Epoch 364/1000\n",
      "2285/2285 [==============================] - 1s 444us/step - loss: 0.2097 - acc: 0.9300 - val_loss: 0.2817 - val_acc: 0.9059\n",
      "Epoch 365/1000\n",
      "2285/2285 [==============================] - 1s 444us/step - loss: 0.2060 - acc: 0.9243 - val_loss: 0.2967 - val_acc: 0.8837\n",
      "Epoch 366/1000\n",
      "2285/2285 [==============================] - 1s 468us/step - loss: 0.1955 - acc: 0.9357 - val_loss: 0.2737 - val_acc: 0.9050\n",
      "Epoch 367/1000\n",
      "2285/2285 [==============================] - 1s 450us/step - loss: 0.2049 - acc: 0.9274 - val_loss: 0.2632 - val_acc: 0.9050\n",
      "Epoch 368/1000\n",
      "2285/2285 [==============================] - 1s 468us/step - loss: 0.2075 - acc: 0.9269 - val_loss: 0.2786 - val_acc: 0.8934\n",
      "Epoch 369/1000\n",
      "2285/2285 [==============================] - 1s 451us/step - loss: 0.2040 - acc: 0.9278 - val_loss: 0.2730 - val_acc: 0.8988\n",
      "Epoch 370/1000\n",
      "2285/2285 [==============================] - 1s 450us/step - loss: 0.2083 - acc: 0.9265 - val_loss: 0.2878 - val_acc: 0.8881\n",
      "Epoch 371/1000\n",
      "2285/2285 [==============================] - 1s 449us/step - loss: 0.2082 - acc: 0.9256 - val_loss: 0.2735 - val_acc: 0.8934\n",
      "Epoch 372/1000\n",
      "2285/2285 [==============================] - 1s 452us/step - loss: 0.2056 - acc: 0.9260 - val_loss: 0.2625 - val_acc: 0.9005\n",
      "Epoch 373/1000\n",
      "2285/2285 [==============================] - 1s 455us/step - loss: 0.2013 - acc: 0.9352 - val_loss: 0.2704 - val_acc: 0.8988\n",
      "Epoch 374/1000\n",
      "2285/2285 [==============================] - 1s 456us/step - loss: 0.1962 - acc: 0.9295 - val_loss: 0.2641 - val_acc: 0.8943\n",
      "Epoch 375/1000\n",
      "2285/2285 [==============================] - 1s 448us/step - loss: 0.1898 - acc: 0.9317 - val_loss: 0.2708 - val_acc: 0.8863\n",
      "Epoch 376/1000\n",
      "2285/2285 [==============================] - 1s 470us/step - loss: 0.2004 - acc: 0.9265 - val_loss: 0.2712 - val_acc: 0.8979\n",
      "Epoch 377/1000\n",
      "2285/2285 [==============================] - 1s 445us/step - loss: 0.2002 - acc: 0.9295 - val_loss: 0.2804 - val_acc: 0.8979\n",
      "Epoch 378/1000\n",
      "2285/2285 [==============================] - 1s 456us/step - loss: 0.2024 - acc: 0.9278 - val_loss: 0.2706 - val_acc: 0.8979\n",
      "Epoch 379/1000\n",
      "2285/2285 [==============================] - 1s 453us/step - loss: 0.1961 - acc: 0.9330 - val_loss: 0.2586 - val_acc: 0.9067\n",
      "Epoch 380/1000\n",
      "2285/2285 [==============================] - 1s 462us/step - loss: 0.1839 - acc: 0.9383 - val_loss: 0.2767 - val_acc: 0.9005\n",
      "Epoch 381/1000\n",
      "2285/2285 [==============================] - 1s 456us/step - loss: 0.2046 - acc: 0.9230 - val_loss: 0.2718 - val_acc: 0.8979\n",
      "Epoch 382/1000\n",
      "2285/2285 [==============================] - 1s 453us/step - loss: 0.1989 - acc: 0.9287 - val_loss: 0.2771 - val_acc: 0.8890\n",
      "Epoch 383/1000\n",
      "2285/2285 [==============================] - 1s 458us/step - loss: 0.2026 - acc: 0.9291 - val_loss: 0.2675 - val_acc: 0.9023\n",
      "Epoch 384/1000\n",
      "2285/2285 [==============================] - 1s 474us/step - loss: 0.1967 - acc: 0.9304 - val_loss: 0.2635 - val_acc: 0.8854\n",
      "Epoch 385/1000\n",
      "2285/2285 [==============================] - 1s 456us/step - loss: 0.1906 - acc: 0.9330 - val_loss: 0.2566 - val_acc: 0.9139\n",
      "Epoch 386/1000\n",
      "2285/2285 [==============================] - 1s 476us/step - loss: 0.1878 - acc: 0.9348 - val_loss: 0.2764 - val_acc: 0.9067\n",
      "Epoch 387/1000\n",
      "2285/2285 [==============================] - 1s 459us/step - loss: 0.1841 - acc: 0.9392 - val_loss: 0.2710 - val_acc: 0.8952\n",
      "Epoch 388/1000\n",
      "2285/2285 [==============================] - 1s 451us/step - loss: 0.1857 - acc: 0.9352 - val_loss: 0.2513 - val_acc: 0.9085\n",
      "Epoch 389/1000\n",
      "2285/2285 [==============================] - 1s 463us/step - loss: 0.1905 - acc: 0.9339 - val_loss: 0.2471 - val_acc: 0.9112\n",
      "Epoch 390/1000\n",
      "2285/2285 [==============================] - 1s 461us/step - loss: 0.1928 - acc: 0.9304 - val_loss: 0.2527 - val_acc: 0.9139\n",
      "Epoch 391/1000\n",
      "2285/2285 [==============================] - 1s 463us/step - loss: 0.1874 - acc: 0.9374 - val_loss: 0.2638 - val_acc: 0.8934\n",
      "Epoch 392/1000\n",
      "2285/2285 [==============================] - 1s 474us/step - loss: 0.1869 - acc: 0.9339 - val_loss: 0.2525 - val_acc: 0.9085\n",
      "Epoch 393/1000\n",
      "2285/2285 [==============================] - 1s 460us/step - loss: 0.1829 - acc: 0.9352 - val_loss: 0.2568 - val_acc: 0.9014\n",
      "Epoch 394/1000\n",
      "2285/2285 [==============================] - 1s 476us/step - loss: 0.1878 - acc: 0.9282 - val_loss: 0.2548 - val_acc: 0.9067\n",
      "Epoch 395/1000\n",
      "2285/2285 [==============================] - 1s 465us/step - loss: 0.1885 - acc: 0.9330 - val_loss: 0.2560 - val_acc: 0.9023\n",
      "Epoch 396/1000\n",
      "2285/2285 [==============================] - 1s 475us/step - loss: 0.1820 - acc: 0.9379 - val_loss: 0.2691 - val_acc: 0.8979\n",
      "Epoch 397/1000\n",
      "2285/2285 [==============================] - 1s 466us/step - loss: 0.1784 - acc: 0.9422 - val_loss: 0.2484 - val_acc: 0.9112\n",
      "Epoch 398/1000\n",
      "2285/2285 [==============================] - 1s 457us/step - loss: 0.1953 - acc: 0.9252 - val_loss: 0.2735 - val_acc: 0.9067\n",
      "Epoch 399/1000\n",
      "2285/2285 [==============================] - 1s 463us/step - loss: 0.1864 - acc: 0.9365 - val_loss: 0.2525 - val_acc: 0.9059\n",
      "Epoch 400/1000\n",
      "2285/2285 [==============================] - 1s 462us/step - loss: 0.1734 - acc: 0.9396 - val_loss: 0.2445 - val_acc: 0.9121\n",
      "Epoch 401/1000\n",
      "2285/2285 [==============================] - 1s 477us/step - loss: 0.1827 - acc: 0.9365 - val_loss: 0.2470 - val_acc: 0.9103\n",
      "Epoch 402/1000\n",
      "2285/2285 [==============================] - 1s 476us/step - loss: 0.1817 - acc: 0.9361 - val_loss: 0.2503 - val_acc: 0.9094\n",
      "Epoch 403/1000\n",
      "2285/2285 [==============================] - 1s 482us/step - loss: 0.1790 - acc: 0.9357 - val_loss: 0.2743 - val_acc: 0.9041\n",
      "Epoch 404/1000\n",
      "2285/2285 [==============================] - 1s 480us/step - loss: 0.1810 - acc: 0.9374 - val_loss: 0.2480 - val_acc: 0.9121\n",
      "Epoch 405/1000\n",
      "2285/2285 [==============================] - 1s 477us/step - loss: 0.1765 - acc: 0.9414 - val_loss: 0.2486 - val_acc: 0.9103\n",
      "Epoch 406/1000\n",
      "2285/2285 [==============================] - 1s 479us/step - loss: 0.1851 - acc: 0.9379 - val_loss: 0.2674 - val_acc: 0.8988\n",
      "Epoch 407/1000\n",
      "2285/2285 [==============================] - 1s 465us/step - loss: 0.1718 - acc: 0.9405 - val_loss: 0.2994 - val_acc: 0.8881\n",
      "Epoch 408/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 429us/step - loss: 0.1766 - acc: 0.9344 - val_loss: 0.2418 - val_acc: 0.9085\n",
      "Epoch 409/1000\n",
      "2285/2285 [==============================] - 1s 429us/step - loss: 0.1722 - acc: 0.9383 - val_loss: 0.2943 - val_acc: 0.8925\n",
      "Epoch 410/1000\n",
      "2285/2285 [==============================] - 1s 430us/step - loss: 0.1794 - acc: 0.9396 - val_loss: 0.2400 - val_acc: 0.9103\n",
      "Epoch 411/1000\n",
      "2285/2285 [==============================] - 1s 429us/step - loss: 0.1793 - acc: 0.9374 - val_loss: 0.2524 - val_acc: 0.9023\n",
      "Epoch 412/1000\n",
      "2285/2285 [==============================] - 1s 445us/step - loss: 0.1743 - acc: 0.9374 - val_loss: 0.2474 - val_acc: 0.9112\n",
      "Epoch 413/1000\n",
      "2285/2285 [==============================] - 1s 428us/step - loss: 0.1730 - acc: 0.9405 - val_loss: 0.2522 - val_acc: 0.9094\n",
      "Epoch 414/1000\n",
      "2285/2285 [==============================] - 1s 435us/step - loss: 0.1733 - acc: 0.9370 - val_loss: 0.2431 - val_acc: 0.9112\n",
      "Epoch 415/1000\n",
      "2285/2285 [==============================] - 1s 436us/step - loss: 0.1575 - acc: 0.9492 - val_loss: 0.2432 - val_acc: 0.9130\n",
      "Epoch 416/1000\n",
      "2285/2285 [==============================] - 1s 430us/step - loss: 0.1618 - acc: 0.9435 - val_loss: 0.2436 - val_acc: 0.9147\n",
      "Epoch 417/1000\n",
      "2285/2285 [==============================] - 1s 434us/step - loss: 0.1694 - acc: 0.9440 - val_loss: 0.2527 - val_acc: 0.9023\n",
      "Epoch 418/1000\n",
      "2285/2285 [==============================] - 1s 441us/step - loss: 0.1751 - acc: 0.9405 - val_loss: 0.2437 - val_acc: 0.9103\n",
      "Epoch 419/1000\n",
      "2285/2285 [==============================] - 1s 433us/step - loss: 0.1701 - acc: 0.9414 - val_loss: 0.2503 - val_acc: 0.9156\n",
      "Epoch 420/1000\n",
      "2285/2285 [==============================] - 1s 445us/step - loss: 0.1711 - acc: 0.9409 - val_loss: 0.2597 - val_acc: 0.8988\n",
      "Epoch 421/1000\n",
      "2285/2285 [==============================] - 1s 435us/step - loss: 0.1585 - acc: 0.9501 - val_loss: 0.2451 - val_acc: 0.9094\n",
      "Epoch 422/1000\n",
      "2285/2285 [==============================] - 1s 433us/step - loss: 0.1711 - acc: 0.9405 - val_loss: 0.2324 - val_acc: 0.9121\n",
      "Epoch 423/1000\n",
      "2285/2285 [==============================] - 1s 453us/step - loss: 0.1654 - acc: 0.9400 - val_loss: 0.2587 - val_acc: 0.9050\n",
      "Epoch 424/1000\n",
      "2285/2285 [==============================] - 2s 696us/step - loss: 0.1742 - acc: 0.9396 - val_loss: 0.2584 - val_acc: 0.8925\n",
      "Epoch 425/1000\n",
      "2285/2285 [==============================] - 1s 636us/step - loss: 0.1706 - acc: 0.9370 - val_loss: 0.2454 - val_acc: 0.9103\n",
      "Epoch 426/1000\n",
      "2285/2285 [==============================] - 1s 536us/step - loss: 0.1644 - acc: 0.9449 - val_loss: 0.2600 - val_acc: 0.9023\n",
      "Epoch 427/1000\n",
      "2285/2285 [==============================] - 1s 556us/step - loss: 0.1589 - acc: 0.9422 - val_loss: 0.2450 - val_acc: 0.9059\n",
      "Epoch 428/1000\n",
      "2285/2285 [==============================] - 1s 651us/step - loss: 0.1725 - acc: 0.9400 - val_loss: 0.2523 - val_acc: 0.9041\n",
      "Epoch 429/1000\n",
      "2285/2285 [==============================] - 2s 707us/step - loss: 0.1702 - acc: 0.9392 - val_loss: 0.2355 - val_acc: 0.9174\n",
      "Epoch 430/1000\n",
      "2285/2285 [==============================] - 2s 737us/step - loss: 0.1633 - acc: 0.9440 - val_loss: 0.2451 - val_acc: 0.9094\n",
      "Epoch 431/1000\n",
      "2285/2285 [==============================] - 2s 667us/step - loss: 0.1746 - acc: 0.9400 - val_loss: 0.2349 - val_acc: 0.9192\n",
      "Epoch 432/1000\n",
      "2285/2285 [==============================] - 1s 536us/step - loss: 0.1559 - acc: 0.9519 - val_loss: 0.2245 - val_acc: 0.9254\n",
      "Epoch 433/1000\n",
      "2285/2285 [==============================] - 1s 563us/step - loss: 0.1608 - acc: 0.9501 - val_loss: 0.2343 - val_acc: 0.9201\n",
      "Epoch 434/1000\n",
      "2285/2285 [==============================] - 2s 689us/step - loss: 0.1561 - acc: 0.9453 - val_loss: 0.2550 - val_acc: 0.9059\n",
      "Epoch 435/1000\n",
      "2285/2285 [==============================] - 2s 760us/step - loss: 0.1645 - acc: 0.9479 - val_loss: 0.2547 - val_acc: 0.9094\n",
      "Epoch 436/1000\n",
      "2285/2285 [==============================] - 2s 705us/step - loss: 0.1587 - acc: 0.9431 - val_loss: 0.2303 - val_acc: 0.9139\n",
      "Epoch 437/1000\n",
      "2285/2285 [==============================] - 1s 595us/step - loss: 0.1524 - acc: 0.9470 - val_loss: 0.2334 - val_acc: 0.9112\n",
      "Epoch 438/1000\n",
      "2285/2285 [==============================] - 1s 425us/step - loss: 0.1601 - acc: 0.9427 - val_loss: 0.2215 - val_acc: 0.9281\n",
      "Epoch 439/1000\n",
      "2285/2285 [==============================] - 1s 404us/step - loss: 0.1519 - acc: 0.9545 - val_loss: 0.2296 - val_acc: 0.9218\n",
      "Epoch 440/1000\n",
      "2285/2285 [==============================] - 1s 501us/step - loss: 0.1589 - acc: 0.9449 - val_loss: 0.2370 - val_acc: 0.9103\n",
      "Epoch 441/1000\n",
      "2285/2285 [==============================] - 1s 554us/step - loss: 0.1581 - acc: 0.9466 - val_loss: 0.2421 - val_acc: 0.9183\n",
      "Epoch 442/1000\n",
      "2285/2285 [==============================] - 1s 467us/step - loss: 0.1543 - acc: 0.9497 - val_loss: 0.2247 - val_acc: 0.9218\n",
      "Epoch 443/1000\n",
      "2285/2285 [==============================] - 1s 591us/step - loss: 0.1563 - acc: 0.9523 - val_loss: 0.2553 - val_acc: 0.9130\n",
      "Epoch 444/1000\n",
      "2285/2285 [==============================] - 2s 660us/step - loss: 0.1600 - acc: 0.9414 - val_loss: 0.2449 - val_acc: 0.9112\n",
      "Epoch 445/1000\n",
      "2285/2285 [==============================] - 1s 564us/step - loss: 0.1577 - acc: 0.9431 - val_loss: 0.2478 - val_acc: 0.9139\n",
      "Epoch 446/1000\n",
      "2285/2285 [==============================] - 1s 462us/step - loss: 0.1474 - acc: 0.9510 - val_loss: 0.2339 - val_acc: 0.9183\n",
      "Epoch 447/1000\n",
      "2285/2285 [==============================] - 2s 672us/step - loss: 0.1474 - acc: 0.9488 - val_loss: 0.2371 - val_acc: 0.9112\n",
      "Epoch 448/1000\n",
      "2285/2285 [==============================] - 1s 580us/step - loss: 0.1552 - acc: 0.9470 - val_loss: 0.2218 - val_acc: 0.9227\n",
      "Epoch 449/1000\n",
      "2285/2285 [==============================] - 1s 508us/step - loss: 0.1446 - acc: 0.9497 - val_loss: 0.2371 - val_acc: 0.9094\n",
      "Epoch 450/1000\n",
      "2285/2285 [==============================] - 2s 668us/step - loss: 0.1606 - acc: 0.9431 - val_loss: 0.2332 - val_acc: 0.9183\n",
      "Epoch 451/1000\n",
      "2285/2285 [==============================] - 1s 452us/step - loss: 0.1596 - acc: 0.9444 - val_loss: 0.2333 - val_acc: 0.9103\n",
      "Epoch 452/1000\n",
      "2285/2285 [==============================] - 1s 420us/step - loss: 0.1634 - acc: 0.9475 - val_loss: 0.2232 - val_acc: 0.9254\n",
      "Epoch 453/1000\n",
      "2285/2285 [==============================] - 1s 439us/step - loss: 0.1462 - acc: 0.9510 - val_loss: 0.2259 - val_acc: 0.9218\n",
      "Epoch 454/1000\n",
      "2285/2285 [==============================] - 1s 441us/step - loss: 0.1571 - acc: 0.9484 - val_loss: 0.2256 - val_acc: 0.9156\n",
      "Epoch 455/1000\n",
      "2285/2285 [==============================] - 1s 445us/step - loss: 0.1452 - acc: 0.9484 - val_loss: 0.2257 - val_acc: 0.9183\n",
      "Epoch 456/1000\n",
      "2285/2285 [==============================] - 1s 422us/step - loss: 0.1445 - acc: 0.9497 - val_loss: 0.2382 - val_acc: 0.9112\n",
      "Epoch 457/1000\n",
      "2285/2285 [==============================] - 1s 419us/step - loss: 0.1630 - acc: 0.9449 - val_loss: 0.2200 - val_acc: 0.9298\n",
      "Epoch 458/1000\n",
      "2285/2285 [==============================] - 1s 422us/step - loss: 0.1440 - acc: 0.9510 - val_loss: 0.2265 - val_acc: 0.9156\n",
      "Epoch 459/1000\n",
      "2285/2285 [==============================] - 1s 423us/step - loss: 0.1484 - acc: 0.9505 - val_loss: 0.2182 - val_acc: 0.9227\n",
      "Epoch 460/1000\n",
      "2285/2285 [==============================] - 1s 438us/step - loss: 0.1482 - acc: 0.9497 - val_loss: 0.2365 - val_acc: 0.9192\n",
      "Epoch 461/1000\n",
      "2285/2285 [==============================] - 1s 428us/step - loss: 0.1420 - acc: 0.9497 - val_loss: 0.2099 - val_acc: 0.9290\n",
      "Epoch 462/1000\n",
      "2285/2285 [==============================] - 1s 474us/step - loss: 0.1429 - acc: 0.9523 - val_loss: 0.2359 - val_acc: 0.9254\n",
      "Epoch 463/1000\n",
      "2285/2285 [==============================] - 1s 439us/step - loss: 0.1561 - acc: 0.9427 - val_loss: 0.2268 - val_acc: 0.9227\n",
      "Epoch 464/1000\n",
      "2285/2285 [==============================] - 1s 497us/step - loss: 0.1451 - acc: 0.9501 - val_loss: 0.2130 - val_acc: 0.9281\n",
      "Epoch 465/1000\n",
      "2285/2285 [==============================] - 1s 476us/step - loss: 0.1487 - acc: 0.9510 - val_loss: 0.2363 - val_acc: 0.9183\n",
      "Epoch 466/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 492us/step - loss: 0.1455 - acc: 0.9519 - val_loss: 0.2500 - val_acc: 0.9076\n",
      "Epoch 467/1000\n",
      "2285/2285 [==============================] - 1s 513us/step - loss: 0.1508 - acc: 0.9488 - val_loss: 0.2266 - val_acc: 0.9156\n",
      "Epoch 468/1000\n",
      "2285/2285 [==============================] - 1s 442us/step - loss: 0.1409 - acc: 0.9514 - val_loss: 0.2731 - val_acc: 0.9094\n",
      "Epoch 469/1000\n",
      "2285/2285 [==============================] - 1s 438us/step - loss: 0.1442 - acc: 0.9484 - val_loss: 0.2177 - val_acc: 0.9298\n",
      "Epoch 470/1000\n",
      "2285/2285 [==============================] - 1s 411us/step - loss: 0.1465 - acc: 0.9470 - val_loss: 0.2265 - val_acc: 0.9156\n",
      "Epoch 471/1000\n",
      "2285/2285 [==============================] - 1s 430us/step - loss: 0.1447 - acc: 0.9510 - val_loss: 0.2062 - val_acc: 0.9316\n",
      "Epoch 472/1000\n",
      "2285/2285 [==============================] - 1s 393us/step - loss: 0.1402 - acc: 0.9523 - val_loss: 0.2074 - val_acc: 0.9245\n",
      "Epoch 473/1000\n",
      "2285/2285 [==============================] - 1s 376us/step - loss: 0.1486 - acc: 0.9532 - val_loss: 0.2208 - val_acc: 0.9245\n",
      "Epoch 474/1000\n",
      "2285/2285 [==============================] - 1s 395us/step - loss: 0.1435 - acc: 0.9470 - val_loss: 0.2254 - val_acc: 0.9201\n",
      "Epoch 475/1000\n",
      "2285/2285 [==============================] - 1s 401us/step - loss: 0.1395 - acc: 0.9510 - val_loss: 0.2335 - val_acc: 0.9130\n",
      "Epoch 476/1000\n",
      "2285/2285 [==============================] - 1s 439us/step - loss: 0.1408 - acc: 0.9514 - val_loss: 0.2201 - val_acc: 0.9298\n",
      "Epoch 477/1000\n",
      "2285/2285 [==============================] - 1s 403us/step - loss: 0.1353 - acc: 0.9532 - val_loss: 0.2180 - val_acc: 0.9210\n",
      "Epoch 478/1000\n",
      "2285/2285 [==============================] - 1s 409us/step - loss: 0.1465 - acc: 0.9479 - val_loss: 0.2113 - val_acc: 0.9272\n",
      "Epoch 479/1000\n",
      "2285/2285 [==============================] - 1s 419us/step - loss: 0.1507 - acc: 0.9457 - val_loss: 0.2208 - val_acc: 0.9218\n",
      "Epoch 480/1000\n",
      "2285/2285 [==============================] - 1s 404us/step - loss: 0.1416 - acc: 0.9475 - val_loss: 0.2117 - val_acc: 0.9307\n",
      "Epoch 481/1000\n",
      "2285/2285 [==============================] - 1s 401us/step - loss: 0.1380 - acc: 0.9540 - val_loss: 0.2523 - val_acc: 0.8988\n",
      "Epoch 482/1000\n",
      "2285/2285 [==============================] - 1s 405us/step - loss: 0.1420 - acc: 0.9545 - val_loss: 0.2190 - val_acc: 0.9227\n",
      "Epoch 483/1000\n",
      "2285/2285 [==============================] - 1s 419us/step - loss: 0.1326 - acc: 0.9519 - val_loss: 0.2365 - val_acc: 0.9112\n",
      "Epoch 484/1000\n",
      "2285/2285 [==============================] - 1s 420us/step - loss: 0.1309 - acc: 0.9602 - val_loss: 0.2219 - val_acc: 0.9147\n",
      "Epoch 485/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 0.1512 - acc: 0.9484 - val_loss: 0.2230 - val_acc: 0.9245\n",
      "Epoch 486/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 0.1397 - acc: 0.9545 - val_loss: 0.2481 - val_acc: 0.9067\n",
      "Epoch 487/1000\n",
      "2285/2285 [==============================] - 1s 391us/step - loss: 0.1332 - acc: 0.9523 - val_loss: 0.2081 - val_acc: 0.9307\n",
      "Epoch 488/1000\n",
      "2285/2285 [==============================] - 1s 407us/step - loss: 0.1417 - acc: 0.9519 - val_loss: 0.2236 - val_acc: 0.9236\n",
      "Epoch 489/1000\n",
      "2285/2285 [==============================] - 1s 456us/step - loss: 0.1379 - acc: 0.9540 - val_loss: 0.2239 - val_acc: 0.9254\n",
      "Epoch 490/1000\n",
      "2285/2285 [==============================] - 1s 412us/step - loss: 0.1375 - acc: 0.9505 - val_loss: 0.2279 - val_acc: 0.9112\n",
      "Epoch 491/1000\n",
      "2285/2285 [==============================] - 1s 423us/step - loss: 0.1372 - acc: 0.9523 - val_loss: 0.2186 - val_acc: 0.9156\n",
      "Epoch 492/1000\n",
      "2285/2285 [==============================] - 1s 417us/step - loss: 0.1233 - acc: 0.9589 - val_loss: 0.2263 - val_acc: 0.9210\n",
      "Epoch 493/1000\n",
      "2285/2285 [==============================] - 1s 406us/step - loss: 0.1294 - acc: 0.9589 - val_loss: 0.2038 - val_acc: 0.9290\n",
      "Epoch 494/1000\n",
      "2285/2285 [==============================] - 1s 394us/step - loss: 0.1366 - acc: 0.9514 - val_loss: 0.2078 - val_acc: 0.9227\n",
      "Epoch 495/1000\n",
      "2285/2285 [==============================] - 1s 469us/step - loss: 0.1387 - acc: 0.9540 - val_loss: 0.2263 - val_acc: 0.9201\n",
      "Epoch 496/1000\n",
      "2285/2285 [==============================] - 1s 399us/step - loss: 0.1434 - acc: 0.9497 - val_loss: 0.2336 - val_acc: 0.9192\n",
      "Epoch 497/1000\n",
      "2285/2285 [==============================] - 1s 455us/step - loss: 0.1380 - acc: 0.9545 - val_loss: 0.2319 - val_acc: 0.9112\n",
      "Epoch 498/1000\n",
      "2285/2285 [==============================] - 1s 430us/step - loss: 0.1363 - acc: 0.9510 - val_loss: 0.2151 - val_acc: 0.9263\n",
      "Epoch 499/1000\n",
      "2285/2285 [==============================] - 1s 424us/step - loss: 0.1325 - acc: 0.9575 - val_loss: 0.2239 - val_acc: 0.9183\n",
      "Epoch 500/1000\n",
      "2285/2285 [==============================] - 1s 437us/step - loss: 0.1174 - acc: 0.9650 - val_loss: 0.2175 - val_acc: 0.9236\n",
      "Epoch 501/1000\n",
      "2285/2285 [==============================] - 1s 420us/step - loss: 0.1295 - acc: 0.9540 - val_loss: 0.2245 - val_acc: 0.9174\n",
      "Epoch 502/1000\n",
      "2285/2285 [==============================] - 1s 429us/step - loss: 0.1247 - acc: 0.9580 - val_loss: 0.2204 - val_acc: 0.9227\n",
      "Epoch 503/1000\n",
      "2285/2285 [==============================] - 1s 425us/step - loss: 0.1232 - acc: 0.9575 - val_loss: 0.2248 - val_acc: 0.9290\n",
      "Epoch 504/1000\n",
      "2285/2285 [==============================] - 1s 404us/step - loss: 0.1302 - acc: 0.9571 - val_loss: 0.2002 - val_acc: 0.9325\n",
      "Epoch 505/1000\n",
      "2285/2285 [==============================] - 1s 422us/step - loss: 0.1346 - acc: 0.9505 - val_loss: 0.2066 - val_acc: 0.9316\n",
      "Epoch 506/1000\n",
      "2285/2285 [==============================] - 1s 446us/step - loss: 0.1276 - acc: 0.9580 - val_loss: 0.2433 - val_acc: 0.9156\n",
      "Epoch 507/1000\n",
      "2285/2285 [==============================] - 1s 450us/step - loss: 0.1313 - acc: 0.9580 - val_loss: 0.2248 - val_acc: 0.9254\n",
      "Epoch 508/1000\n",
      "2285/2285 [==============================] - 1s 416us/step - loss: 0.1255 - acc: 0.9584 - val_loss: 0.2196 - val_acc: 0.9263\n",
      "Epoch 509/1000\n",
      "2285/2285 [==============================] - 1s 449us/step - loss: 0.1260 - acc: 0.9584 - val_loss: 0.2226 - val_acc: 0.9210\n",
      "Epoch 510/1000\n",
      "2285/2285 [==============================] - 1s 458us/step - loss: 0.1200 - acc: 0.9589 - val_loss: 0.1993 - val_acc: 0.9307\n",
      "Epoch 511/1000\n",
      "2285/2285 [==============================] - 1s 439us/step - loss: 0.1304 - acc: 0.9584 - val_loss: 0.2015 - val_acc: 0.9290\n",
      "Epoch 512/1000\n",
      "2285/2285 [==============================] - 1s 439us/step - loss: 0.1327 - acc: 0.9562 - val_loss: 0.2110 - val_acc: 0.9272\n",
      "Epoch 513/1000\n",
      "2285/2285 [==============================] - 1s 454us/step - loss: 0.1301 - acc: 0.9554 - val_loss: 0.2042 - val_acc: 0.9298\n",
      "Epoch 514/1000\n",
      "2285/2285 [==============================] - 1s 455us/step - loss: 0.1305 - acc: 0.9558 - val_loss: 0.2597 - val_acc: 0.9059\n",
      "Epoch 515/1000\n",
      "2285/2285 [==============================] - 1s 421us/step - loss: 0.1261 - acc: 0.9575 - val_loss: 0.2138 - val_acc: 0.9254\n",
      "Epoch 516/1000\n",
      "2285/2285 [==============================] - 1s 437us/step - loss: 0.1361 - acc: 0.9532 - val_loss: 0.2108 - val_acc: 0.9316\n",
      "Epoch 517/1000\n",
      "2285/2285 [==============================] - 1s 451us/step - loss: 0.1266 - acc: 0.9562 - val_loss: 0.2208 - val_acc: 0.9263\n",
      "Epoch 518/1000\n",
      "2285/2285 [==============================] - 1s 409us/step - loss: 0.1224 - acc: 0.9575 - val_loss: 0.2179 - val_acc: 0.9272\n",
      "Epoch 519/1000\n",
      "2285/2285 [==============================] - 1s 413us/step - loss: 0.1235 - acc: 0.9536 - val_loss: 0.2249 - val_acc: 0.9076\n",
      "Epoch 520/1000\n",
      "2285/2285 [==============================] - 1s 398us/step - loss: 0.1192 - acc: 0.9558 - val_loss: 0.2224 - val_acc: 0.9263\n",
      "Epoch 521/1000\n",
      "2285/2285 [==============================] - 1s 453us/step - loss: 0.1164 - acc: 0.9593 - val_loss: 0.2082 - val_acc: 0.9254\n",
      "Epoch 522/1000\n",
      "2285/2285 [==============================] - 1s 437us/step - loss: 0.1195 - acc: 0.9632 - val_loss: 0.2259 - val_acc: 0.9130\n",
      "Epoch 523/1000\n",
      "2285/2285 [==============================] - 1s 468us/step - loss: 0.1241 - acc: 0.9562 - val_loss: 0.2032 - val_acc: 0.9343\n",
      "Epoch 524/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 413us/step - loss: 0.1282 - acc: 0.9484 - val_loss: 0.1925 - val_acc: 0.9369\n",
      "Epoch 525/1000\n",
      "2285/2285 [==============================] - 1s 403us/step - loss: 0.1224 - acc: 0.9536 - val_loss: 0.1944 - val_acc: 0.9307\n",
      "Epoch 526/1000\n",
      "2285/2285 [==============================] - 1s 393us/step - loss: 0.1203 - acc: 0.9611 - val_loss: 0.2037 - val_acc: 0.9192\n",
      "Epoch 527/1000\n",
      "2285/2285 [==============================] - 1s 439us/step - loss: 0.1262 - acc: 0.9584 - val_loss: 0.1983 - val_acc: 0.9281\n",
      "Epoch 528/1000\n",
      "2285/2285 [==============================] - 1s 419us/step - loss: 0.1277 - acc: 0.9589 - val_loss: 0.1984 - val_acc: 0.9334\n",
      "Epoch 529/1000\n",
      "2285/2285 [==============================] - 1s 419us/step - loss: 0.1193 - acc: 0.9597 - val_loss: 0.2175 - val_acc: 0.9325\n",
      "Epoch 530/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.1289 - acc: 0.9540 - val_loss: 0.1983 - val_acc: 0.9334\n",
      "Epoch 531/1000\n",
      "2285/2285 [==============================] - 1s 423us/step - loss: 0.1142 - acc: 0.9580 - val_loss: 0.2123 - val_acc: 0.9254\n",
      "Epoch 532/1000\n",
      "2285/2285 [==============================] - 1s 419us/step - loss: 0.1255 - acc: 0.9580 - val_loss: 0.1962 - val_acc: 0.9307\n",
      "Epoch 533/1000\n",
      "2285/2285 [==============================] - 1s 454us/step - loss: 0.1166 - acc: 0.9619 - val_loss: 0.1902 - val_acc: 0.9387\n",
      "Epoch 534/1000\n",
      "2285/2285 [==============================] - 1s 388us/step - loss: 0.1118 - acc: 0.9606 - val_loss: 0.2045 - val_acc: 0.9281\n",
      "Epoch 535/1000\n",
      "2285/2285 [==============================] - 1s 381us/step - loss: 0.1098 - acc: 0.9624 - val_loss: 0.1978 - val_acc: 0.9334\n",
      "Epoch 536/1000\n",
      "2285/2285 [==============================] - 1s 393us/step - loss: 0.1282 - acc: 0.9562 - val_loss: 0.2018 - val_acc: 0.9325\n",
      "Epoch 537/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.1252 - acc: 0.9549 - val_loss: 0.2182 - val_acc: 0.9254\n",
      "Epoch 538/1000\n",
      "2285/2285 [==============================] - 1s 433us/step - loss: 0.1224 - acc: 0.9654 - val_loss: 0.2182 - val_acc: 0.9192\n",
      "Epoch 539/1000\n",
      "2285/2285 [==============================] - 1s 427us/step - loss: 0.1261 - acc: 0.9584 - val_loss: 0.2061 - val_acc: 0.9334\n",
      "Epoch 540/1000\n",
      "2285/2285 [==============================] - 1s 438us/step - loss: 0.1090 - acc: 0.9611 - val_loss: 0.2067 - val_acc: 0.9254\n",
      "Epoch 541/1000\n",
      "2285/2285 [==============================] - 1s 438us/step - loss: 0.1202 - acc: 0.9597 - val_loss: 0.2061 - val_acc: 0.9325\n",
      "Epoch 542/1000\n",
      "2285/2285 [==============================] - 1s 505us/step - loss: 0.1191 - acc: 0.9571 - val_loss: 0.1956 - val_acc: 0.9361\n",
      "Epoch 543/1000\n",
      "2285/2285 [==============================] - 1s 556us/step - loss: 0.1149 - acc: 0.9667 - val_loss: 0.2104 - val_acc: 0.9307\n",
      "Epoch 544/1000\n",
      "2285/2285 [==============================] - 1s 448us/step - loss: 0.1210 - acc: 0.9589 - val_loss: 0.1845 - val_acc: 0.9378\n",
      "Epoch 545/1000\n",
      "2285/2285 [==============================] - 1s 445us/step - loss: 0.1173 - acc: 0.9624 - val_loss: 0.1999 - val_acc: 0.9236\n",
      "Epoch 546/1000\n",
      "2285/2285 [==============================] - 1s 422us/step - loss: 0.1146 - acc: 0.9646 - val_loss: 0.2008 - val_acc: 0.9316\n",
      "Epoch 547/1000\n",
      "2285/2285 [==============================] - 1s 421us/step - loss: 0.1255 - acc: 0.9554 - val_loss: 0.1842 - val_acc: 0.9405\n",
      "Epoch 548/1000\n",
      "2285/2285 [==============================] - 1s 386us/step - loss: 0.1161 - acc: 0.9611 - val_loss: 0.1889 - val_acc: 0.9369\n",
      "Epoch 549/1000\n",
      "2285/2285 [==============================] - 1s 417us/step - loss: 0.1128 - acc: 0.9615 - val_loss: 0.2045 - val_acc: 0.9290\n",
      "Epoch 550/1000\n",
      "2285/2285 [==============================] - 1s 392us/step - loss: 0.1131 - acc: 0.9646 - val_loss: 0.2014 - val_acc: 0.9307\n",
      "Epoch 551/1000\n",
      "2285/2285 [==============================] - 1s 430us/step - loss: 0.1121 - acc: 0.9624 - val_loss: 0.1955 - val_acc: 0.9325\n",
      "Epoch 552/1000\n",
      "2285/2285 [==============================] - 1s 421us/step - loss: 0.1171 - acc: 0.9571 - val_loss: 0.1890 - val_acc: 0.9369\n",
      "Epoch 553/1000\n",
      "2285/2285 [==============================] - 1s 482us/step - loss: 0.1175 - acc: 0.9602 - val_loss: 0.2087 - val_acc: 0.9307\n",
      "Epoch 554/1000\n",
      "2285/2285 [==============================] - 1s 501us/step - loss: 0.1099 - acc: 0.9619 - val_loss: 0.2190 - val_acc: 0.9210\n",
      "Epoch 555/1000\n",
      "2285/2285 [==============================] - 1s 428us/step - loss: 0.1131 - acc: 0.9597 - val_loss: 0.1886 - val_acc: 0.9405\n",
      "Epoch 556/1000\n",
      "2285/2285 [==============================] - 1s 437us/step - loss: 0.1131 - acc: 0.9637 - val_loss: 0.2016 - val_acc: 0.9352\n",
      "Epoch 557/1000\n",
      "2285/2285 [==============================] - 1s 432us/step - loss: 0.1145 - acc: 0.9624 - val_loss: 0.1909 - val_acc: 0.9378\n",
      "Epoch 558/1000\n",
      "2285/2285 [==============================] - 1s 437us/step - loss: 0.1030 - acc: 0.9667 - val_loss: 0.1932 - val_acc: 0.9236\n",
      "Epoch 559/1000\n",
      "2285/2285 [==============================] - 1s 444us/step - loss: 0.1269 - acc: 0.9584 - val_loss: 0.2160 - val_acc: 0.9290\n",
      "Epoch 560/1000\n",
      "2285/2285 [==============================] - 1s 426us/step - loss: 0.1145 - acc: 0.9663 - val_loss: 0.1997 - val_acc: 0.9298\n",
      "Epoch 561/1000\n",
      "2285/2285 [==============================] - 1s 430us/step - loss: 0.1133 - acc: 0.9619 - val_loss: 0.1794 - val_acc: 0.9458\n",
      "Epoch 562/1000\n",
      "2285/2285 [==============================] - 1s 433us/step - loss: 0.1178 - acc: 0.9580 - val_loss: 0.1859 - val_acc: 0.9414\n",
      "Epoch 563/1000\n",
      "2285/2285 [==============================] - 1s 446us/step - loss: 0.1050 - acc: 0.9694 - val_loss: 0.1886 - val_acc: 0.9387\n",
      "Epoch 564/1000\n",
      "2285/2285 [==============================] - 1s 433us/step - loss: 0.1124 - acc: 0.9597 - val_loss: 0.1913 - val_acc: 0.9387\n",
      "Epoch 565/1000\n",
      "2285/2285 [==============================] - 1s 451us/step - loss: 0.1103 - acc: 0.9632 - val_loss: 0.1879 - val_acc: 0.9361\n",
      "Epoch 566/1000\n",
      "2285/2285 [==============================] - 1s 432us/step - loss: 0.1106 - acc: 0.9637 - val_loss: 0.2003 - val_acc: 0.9290\n",
      "Epoch 567/1000\n",
      "2285/2285 [==============================] - 1s 448us/step - loss: 0.1060 - acc: 0.9646 - val_loss: 0.1723 - val_acc: 0.9405\n",
      "Epoch 568/1000\n",
      "2285/2285 [==============================] - 1s 458us/step - loss: 0.1097 - acc: 0.9650 - val_loss: 0.1926 - val_acc: 0.9316\n",
      "Epoch 569/1000\n",
      "2285/2285 [==============================] - 1s 444us/step - loss: 0.1183 - acc: 0.9589 - val_loss: 0.1949 - val_acc: 0.9316\n",
      "Epoch 570/1000\n",
      "2285/2285 [==============================] - 1s 450us/step - loss: 0.1065 - acc: 0.9650 - val_loss: 0.1861 - val_acc: 0.9414\n",
      "Epoch 571/1000\n",
      "2285/2285 [==============================] - 1s 452us/step - loss: 0.1048 - acc: 0.9646 - val_loss: 0.1912 - val_acc: 0.9369\n",
      "Epoch 572/1000\n",
      "2285/2285 [==============================] - 1s 446us/step - loss: 0.1138 - acc: 0.9641 - val_loss: 0.2031 - val_acc: 0.9272\n",
      "Epoch 573/1000\n",
      "2285/2285 [==============================] - 1s 451us/step - loss: 0.1086 - acc: 0.9694 - val_loss: 0.1837 - val_acc: 0.9316\n",
      "Epoch 574/1000\n",
      "2285/2285 [==============================] - 1s 465us/step - loss: 0.1125 - acc: 0.9654 - val_loss: 0.1876 - val_acc: 0.9307\n",
      "Epoch 575/1000\n",
      "2285/2285 [==============================] - 1s 434us/step - loss: 0.1122 - acc: 0.9615 - val_loss: 0.1828 - val_acc: 0.9423\n",
      "Epoch 576/1000\n",
      "2285/2285 [==============================] - 1s 511us/step - loss: 0.1064 - acc: 0.9672 - val_loss: 0.1877 - val_acc: 0.9396\n",
      "Epoch 577/1000\n",
      "2285/2285 [==============================] - 1s 652us/step - loss: 0.0962 - acc: 0.9707 - val_loss: 0.2030 - val_acc: 0.9343\n",
      "Epoch 578/1000\n",
      "2285/2285 [==============================] - 1s 560us/step - loss: 0.1187 - acc: 0.9602 - val_loss: 0.1886 - val_acc: 0.9343\n",
      "Epoch 579/1000\n",
      "2285/2285 [==============================] - 1s 604us/step - loss: 0.1087 - acc: 0.9611 - val_loss: 0.1858 - val_acc: 0.9423\n",
      "Epoch 580/1000\n",
      "2285/2285 [==============================] - 1s 459us/step - loss: 0.1005 - acc: 0.9702 - val_loss: 0.1976 - val_acc: 0.9272\n",
      "Epoch 581/1000\n",
      "2285/2285 [==============================] - 1s 613us/step - loss: 0.0987 - acc: 0.9681 - val_loss: 0.1864 - val_acc: 0.9414\n",
      "Epoch 582/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 654us/step - loss: 0.1062 - acc: 0.9632 - val_loss: 0.1893 - val_acc: 0.9396\n",
      "Epoch 583/1000\n",
      "2285/2285 [==============================] - 2s 698us/step - loss: 0.1036 - acc: 0.9637 - val_loss: 0.2080 - val_acc: 0.9263\n",
      "Epoch 584/1000\n",
      "2285/2285 [==============================] - 1s 580us/step - loss: 0.1020 - acc: 0.9689 - val_loss: 0.1798 - val_acc: 0.9485\n",
      "Epoch 585/1000\n",
      "2285/2285 [==============================] - 1s 596us/step - loss: 0.0896 - acc: 0.9702 - val_loss: 0.1782 - val_acc: 0.9458\n",
      "Epoch 586/1000\n",
      "2285/2285 [==============================] - 1s 522us/step - loss: 0.1008 - acc: 0.9707 - val_loss: 0.1913 - val_acc: 0.9334\n",
      "Epoch 587/1000\n",
      "2285/2285 [==============================] - 1s 594us/step - loss: 0.0990 - acc: 0.9676 - val_loss: 0.2065 - val_acc: 0.9334\n",
      "Epoch 588/1000\n",
      "2285/2285 [==============================] - 1s 563us/step - loss: 0.1029 - acc: 0.9646 - val_loss: 0.1982 - val_acc: 0.9378\n",
      "Epoch 589/1000\n",
      "2285/2285 [==============================] - 1s 435us/step - loss: 0.1137 - acc: 0.9597 - val_loss: 0.1812 - val_acc: 0.9361\n",
      "Epoch 590/1000\n",
      "2285/2285 [==============================] - 1s 448us/step - loss: 0.1014 - acc: 0.9702 - val_loss: 0.1925 - val_acc: 0.9316\n",
      "Epoch 591/1000\n",
      "2285/2285 [==============================] - 1s 469us/step - loss: 0.1009 - acc: 0.9694 - val_loss: 0.1917 - val_acc: 0.9272\n",
      "Epoch 592/1000\n",
      "2285/2285 [==============================] - 1s 452us/step - loss: 0.0976 - acc: 0.9667 - val_loss: 0.1848 - val_acc: 0.9458\n",
      "Epoch 593/1000\n",
      "2285/2285 [==============================] - 1s 478us/step - loss: 0.1104 - acc: 0.9624 - val_loss: 0.1936 - val_acc: 0.9361\n",
      "Epoch 594/1000\n",
      "2285/2285 [==============================] - 1s 452us/step - loss: 0.1068 - acc: 0.9646 - val_loss: 0.1795 - val_acc: 0.9405\n",
      "Epoch 595/1000\n",
      "2285/2285 [==============================] - 1s 453us/step - loss: 0.1022 - acc: 0.9646 - val_loss: 0.1792 - val_acc: 0.9467\n",
      "Epoch 596/1000\n",
      "2285/2285 [==============================] - 1s 447us/step - loss: 0.0951 - acc: 0.9707 - val_loss: 0.1953 - val_acc: 0.9361\n",
      "Epoch 597/1000\n",
      "2285/2285 [==============================] - 1s 450us/step - loss: 0.0962 - acc: 0.9711 - val_loss: 0.1810 - val_acc: 0.9423\n",
      "Epoch 598/1000\n",
      "2285/2285 [==============================] - 1s 452us/step - loss: 0.1123 - acc: 0.9597 - val_loss: 0.1881 - val_acc: 0.9414\n",
      "Epoch 599/1000\n",
      "2285/2285 [==============================] - 1s 457us/step - loss: 0.0925 - acc: 0.9689 - val_loss: 0.1920 - val_acc: 0.9369\n",
      "Epoch 600/1000\n",
      "2285/2285 [==============================] - 1s 453us/step - loss: 0.1050 - acc: 0.9650 - val_loss: 0.1777 - val_acc: 0.9405\n",
      "Epoch 601/1000\n",
      "2285/2285 [==============================] - 1s 459us/step - loss: 0.1007 - acc: 0.9707 - val_loss: 0.1684 - val_acc: 0.9547\n",
      "Epoch 602/1000\n",
      "2285/2285 [==============================] - 1s 457us/step - loss: 0.0871 - acc: 0.9746 - val_loss: 0.1796 - val_acc: 0.9485\n",
      "Epoch 603/1000\n",
      "2285/2285 [==============================] - 1s 455us/step - loss: 0.1021 - acc: 0.9628 - val_loss: 0.1890 - val_acc: 0.9405\n",
      "Epoch 604/1000\n",
      "2285/2285 [==============================] - 1s 458us/step - loss: 0.1026 - acc: 0.9628 - val_loss: 0.1931 - val_acc: 0.9361\n",
      "Epoch 605/1000\n",
      "2285/2285 [==============================] - 1s 455us/step - loss: 0.1090 - acc: 0.9632 - val_loss: 0.1743 - val_acc: 0.9423\n",
      "Epoch 606/1000\n",
      "2285/2285 [==============================] - 1s 456us/step - loss: 0.1009 - acc: 0.9663 - val_loss: 0.1725 - val_acc: 0.9467\n",
      "Epoch 607/1000\n",
      "2285/2285 [==============================] - 1s 457us/step - loss: 0.0941 - acc: 0.9681 - val_loss: 0.1831 - val_acc: 0.9378\n",
      "Epoch 608/1000\n",
      "2285/2285 [==============================] - 1s 458us/step - loss: 0.0921 - acc: 0.9667 - val_loss: 0.1928 - val_acc: 0.9361\n",
      "Epoch 609/1000\n",
      "2285/2285 [==============================] - 1s 479us/step - loss: 0.1008 - acc: 0.9689 - val_loss: 0.1632 - val_acc: 0.9458\n",
      "Epoch 610/1000\n",
      "2285/2285 [==============================] - 1s 461us/step - loss: 0.1001 - acc: 0.9694 - val_loss: 0.1836 - val_acc: 0.9352\n",
      "Epoch 611/1000\n",
      "2285/2285 [==============================] - 1s 472us/step - loss: 0.1064 - acc: 0.9650 - val_loss: 0.1774 - val_acc: 0.9414\n",
      "Epoch 612/1000\n",
      "2285/2285 [==============================] - 1s 459us/step - loss: 0.0954 - acc: 0.9654 - val_loss: 0.1809 - val_acc: 0.9405\n",
      "Epoch 613/1000\n",
      "2285/2285 [==============================] - 1s 458us/step - loss: 0.0859 - acc: 0.9694 - val_loss: 0.1857 - val_acc: 0.9440\n",
      "Epoch 614/1000\n",
      "2285/2285 [==============================] - 1s 474us/step - loss: 0.0994 - acc: 0.9615 - val_loss: 0.1774 - val_acc: 0.9396\n",
      "Epoch 615/1000\n",
      "2285/2285 [==============================] - 1s 473us/step - loss: 0.0948 - acc: 0.9733 - val_loss: 0.1942 - val_acc: 0.9334\n",
      "Epoch 616/1000\n",
      "2285/2285 [==============================] - 1s 469us/step - loss: 0.0918 - acc: 0.9702 - val_loss: 0.2093 - val_acc: 0.9272\n",
      "Epoch 617/1000\n",
      "2285/2285 [==============================] - 1s 474us/step - loss: 0.0979 - acc: 0.9637 - val_loss: 0.1847 - val_acc: 0.9423\n",
      "Epoch 618/1000\n",
      "2285/2285 [==============================] - 1s 464us/step - loss: 0.0885 - acc: 0.9716 - val_loss: 0.1910 - val_acc: 0.9316\n",
      "Epoch 619/1000\n",
      "2285/2285 [==============================] - 1s 461us/step - loss: 0.0962 - acc: 0.9672 - val_loss: 0.1957 - val_acc: 0.9361\n",
      "Epoch 620/1000\n",
      "2285/2285 [==============================] - 1s 475us/step - loss: 0.1016 - acc: 0.9650 - val_loss: 0.1910 - val_acc: 0.9387\n",
      "Epoch 621/1000\n",
      "2285/2285 [==============================] - 1s 479us/step - loss: 0.0957 - acc: 0.9681 - val_loss: 0.1808 - val_acc: 0.9387\n",
      "Epoch 622/1000\n",
      "2285/2285 [==============================] - 1s 474us/step - loss: 0.0967 - acc: 0.9694 - val_loss: 0.1875 - val_acc: 0.9396\n",
      "Epoch 623/1000\n",
      "2285/2285 [==============================] - 1s 475us/step - loss: 0.0942 - acc: 0.9716 - val_loss: 0.1872 - val_acc: 0.9378\n",
      "Epoch 624/1000\n",
      "2285/2285 [==============================] - 1s 466us/step - loss: 0.0926 - acc: 0.9689 - val_loss: 0.1707 - val_acc: 0.9485\n",
      "Epoch 625/1000\n",
      "2285/2285 [==============================] - 1s 484us/step - loss: 0.0941 - acc: 0.9685 - val_loss: 0.1774 - val_acc: 0.9405\n",
      "Epoch 626/1000\n",
      "2285/2285 [==============================] - 1s 479us/step - loss: 0.0960 - acc: 0.9672 - val_loss: 0.1914 - val_acc: 0.9325\n",
      "Epoch 627/1000\n",
      "2285/2285 [==============================] - 1s 475us/step - loss: 0.0936 - acc: 0.9667 - val_loss: 0.1736 - val_acc: 0.9432\n",
      "Epoch 628/1000\n",
      "2285/2285 [==============================] - 1s 478us/step - loss: 0.0970 - acc: 0.9650 - val_loss: 0.1733 - val_acc: 0.9387\n",
      "Epoch 629/1000\n",
      "2285/2285 [==============================] - 1s 477us/step - loss: 0.0857 - acc: 0.9724 - val_loss: 0.1725 - val_acc: 0.9432\n",
      "Epoch 630/1000\n",
      "2285/2285 [==============================] - 1s 524us/step - loss: 0.0909 - acc: 0.9672 - val_loss: 0.1863 - val_acc: 0.9387\n",
      "Epoch 631/1000\n",
      "2285/2285 [==============================] - 1s 500us/step - loss: 0.1016 - acc: 0.9615 - val_loss: 0.1819 - val_acc: 0.9378\n",
      "Epoch 632/1000\n",
      "2285/2285 [==============================] - 1s 529us/step - loss: 0.0951 - acc: 0.9716 - val_loss: 0.1726 - val_acc: 0.9449\n",
      "Epoch 633/1000\n",
      "2285/2285 [==============================] - 1s 507us/step - loss: 0.0959 - acc: 0.9681 - val_loss: 0.1672 - val_acc: 0.9458\n",
      "Epoch 634/1000\n",
      "2285/2285 [==============================] - 1s 618us/step - loss: 0.0906 - acc: 0.9685 - val_loss: 0.1796 - val_acc: 0.9432\n",
      "Epoch 635/1000\n",
      "2285/2285 [==============================] - 1s 345us/step - loss: 0.0903 - acc: 0.9689 - val_loss: 0.1747 - val_acc: 0.9378\n",
      "Epoch 636/1000\n",
      "2285/2285 [==============================] - 1s 473us/step - loss: 0.0840 - acc: 0.9751 - val_loss: 0.1655 - val_acc: 0.9467\n",
      "Epoch 637/1000\n",
      "2285/2285 [==============================] - 1s 376us/step - loss: 0.0985 - acc: 0.9672 - val_loss: 0.1672 - val_acc: 0.9520\n",
      "Epoch 638/1000\n",
      "2285/2285 [==============================] - 1s 324us/step - loss: 0.0805 - acc: 0.9772 - val_loss: 0.1705 - val_acc: 0.9432\n",
      "Epoch 639/1000\n",
      "2285/2285 [==============================] - 1s 334us/step - loss: 0.0902 - acc: 0.9707 - val_loss: 0.1649 - val_acc: 0.9494\n",
      "Epoch 640/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 387us/step - loss: 0.0887 - acc: 0.9720 - val_loss: 0.1866 - val_acc: 0.9440\n",
      "Epoch 641/1000\n",
      "2285/2285 [==============================] - 1s 352us/step - loss: 0.0836 - acc: 0.9724 - val_loss: 0.1667 - val_acc: 0.9494\n",
      "Epoch 642/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.0881 - acc: 0.9698 - val_loss: 0.1696 - val_acc: 0.9476\n",
      "Epoch 643/1000\n",
      "2285/2285 [==============================] - 1s 379us/step - loss: 0.0862 - acc: 0.9689 - val_loss: 0.1610 - val_acc: 0.9458\n",
      "Epoch 644/1000\n",
      "2285/2285 [==============================] - 1s 347us/step - loss: 0.0938 - acc: 0.9681 - val_loss: 0.1710 - val_acc: 0.9449\n",
      "Epoch 645/1000\n",
      "2285/2285 [==============================] - 1s 476us/step - loss: 0.0807 - acc: 0.9720 - val_loss: 0.1674 - val_acc: 0.9467\n",
      "Epoch 646/1000\n",
      "2285/2285 [==============================] - 2s 754us/step - loss: 0.0847 - acc: 0.9751 - val_loss: 0.1754 - val_acc: 0.9369\n",
      "Epoch 647/1000\n",
      "2285/2285 [==============================] - 1s 492us/step - loss: 0.0906 - acc: 0.9676 - val_loss: 0.1843 - val_acc: 0.9476\n",
      "Epoch 648/1000\n",
      "2285/2285 [==============================] - 2s 693us/step - loss: 0.0903 - acc: 0.9698 - val_loss: 0.1806 - val_acc: 0.9396\n",
      "Epoch 649/1000\n",
      "2285/2285 [==============================] - 1s 588us/step - loss: 0.0882 - acc: 0.9698 - val_loss: 0.1844 - val_acc: 0.9423\n",
      "Epoch 650/1000\n",
      "2285/2285 [==============================] - 1s 568us/step - loss: 0.0967 - acc: 0.9689 - val_loss: 0.1740 - val_acc: 0.9440\n",
      "Epoch 651/1000\n",
      "2285/2285 [==============================] - 1s 489us/step - loss: 0.0906 - acc: 0.9698 - val_loss: 0.1769 - val_acc: 0.9476\n",
      "Epoch 652/1000\n",
      "2285/2285 [==============================] - 1s 483us/step - loss: 0.0926 - acc: 0.9720 - val_loss: 0.1835 - val_acc: 0.9352\n",
      "Epoch 653/1000\n",
      "2285/2285 [==============================] - 1s 578us/step - loss: 0.0775 - acc: 0.9720 - val_loss: 0.1824 - val_acc: 0.9387\n",
      "Epoch 654/1000\n",
      "2285/2285 [==============================] - 1s 572us/step - loss: 0.0856 - acc: 0.9676 - val_loss: 0.1764 - val_acc: 0.9440\n",
      "Epoch 655/1000\n",
      "2285/2285 [==============================] - 2s 661us/step - loss: 0.0840 - acc: 0.9746 - val_loss: 0.1861 - val_acc: 0.9405\n",
      "Epoch 656/1000\n",
      "2285/2285 [==============================] - 1s 473us/step - loss: 0.0831 - acc: 0.9746 - val_loss: 0.1826 - val_acc: 0.9405\n",
      "Epoch 657/1000\n",
      "2285/2285 [==============================] - 1s 336us/step - loss: 0.0854 - acc: 0.9724 - val_loss: 0.1693 - val_acc: 0.9467\n",
      "Epoch 658/1000\n",
      "2285/2285 [==============================] - 1s 332us/step - loss: 0.0762 - acc: 0.9755 - val_loss: 0.2083 - val_acc: 0.9263\n",
      "Epoch 659/1000\n",
      "2285/2285 [==============================] - 1s 324us/step - loss: 0.0927 - acc: 0.9707 - val_loss: 0.1896 - val_acc: 0.9432\n",
      "Epoch 660/1000\n",
      "2285/2285 [==============================] - 1s 550us/step - loss: 0.0835 - acc: 0.9755 - val_loss: 0.1638 - val_acc: 0.9520\n",
      "Epoch 661/1000\n",
      "2285/2285 [==============================] - 1s 582us/step - loss: 0.0879 - acc: 0.9676 - val_loss: 0.1766 - val_acc: 0.9396\n",
      "Epoch 662/1000\n",
      "2285/2285 [==============================] - 1s 356us/step - loss: 0.0827 - acc: 0.9742 - val_loss: 0.1757 - val_acc: 0.9432\n",
      "Epoch 663/1000\n",
      "2285/2285 [==============================] - 1s 376us/step - loss: 0.0806 - acc: 0.9729 - val_loss: 0.1863 - val_acc: 0.9414\n",
      "Epoch 664/1000\n",
      "2285/2285 [==============================] - 1s 366us/step - loss: 0.0835 - acc: 0.9790 - val_loss: 0.1673 - val_acc: 0.9414\n",
      "Epoch 665/1000\n",
      "2285/2285 [==============================] - 1s 345us/step - loss: 0.0847 - acc: 0.9729 - val_loss: 0.1701 - val_acc: 0.9449\n",
      "Epoch 666/1000\n",
      "2285/2285 [==============================] - 1s 339us/step - loss: 0.0912 - acc: 0.9663 - val_loss: 0.1640 - val_acc: 0.9503\n",
      "Epoch 667/1000\n",
      "2285/2285 [==============================] - 1s 339us/step - loss: 0.0860 - acc: 0.9720 - val_loss: 0.1660 - val_acc: 0.9503\n",
      "Epoch 668/1000\n",
      "2285/2285 [==============================] - 1s 385us/step - loss: 0.0897 - acc: 0.9711 - val_loss: 0.1637 - val_acc: 0.9449\n",
      "Epoch 669/1000\n",
      "2285/2285 [==============================] - 1s 390us/step - loss: 0.0907 - acc: 0.9672 - val_loss: 0.1862 - val_acc: 0.9387\n",
      "Epoch 670/1000\n",
      "2285/2285 [==============================] - 1s 329us/step - loss: 0.0787 - acc: 0.9729 - val_loss: 0.1728 - val_acc: 0.9414\n",
      "Epoch 671/1000\n",
      "2285/2285 [==============================] - 1s 362us/step - loss: 0.0815 - acc: 0.9733 - val_loss: 0.2103 - val_acc: 0.9254\n",
      "Epoch 672/1000\n",
      "2285/2285 [==============================] - 1s 352us/step - loss: 0.0812 - acc: 0.9768 - val_loss: 0.1858 - val_acc: 0.9316\n",
      "Epoch 673/1000\n",
      "2285/2285 [==============================] - 1s 345us/step - loss: 0.0783 - acc: 0.9737 - val_loss: 0.1549 - val_acc: 0.9512\n",
      "Epoch 674/1000\n",
      "2285/2285 [==============================] - 1s 317us/step - loss: 0.0915 - acc: 0.9702 - val_loss: 0.1736 - val_acc: 0.9414\n",
      "Epoch 675/1000\n",
      "2285/2285 [==============================] - 1s 417us/step - loss: 0.0821 - acc: 0.9729 - val_loss: 0.1769 - val_acc: 0.9378\n",
      "Epoch 676/1000\n",
      "2285/2285 [==============================] - 1s 369us/step - loss: 0.0916 - acc: 0.9694 - val_loss: 0.1692 - val_acc: 0.9449\n",
      "Epoch 677/1000\n",
      "2285/2285 [==============================] - 1s 372us/step - loss: 0.0807 - acc: 0.9746 - val_loss: 0.1792 - val_acc: 0.9405\n",
      "Epoch 678/1000\n",
      "2285/2285 [==============================] - 1s 364us/step - loss: 0.0950 - acc: 0.9654 - val_loss: 0.1851 - val_acc: 0.9369\n",
      "Epoch 679/1000\n",
      "2285/2285 [==============================] - 1s 373us/step - loss: 0.0798 - acc: 0.9790 - val_loss: 0.1764 - val_acc: 0.9405\n",
      "Epoch 680/1000\n",
      "2285/2285 [==============================] - 1s 326us/step - loss: 0.0839 - acc: 0.9707 - val_loss: 0.1691 - val_acc: 0.9440\n",
      "Epoch 681/1000\n",
      "2285/2285 [==============================] - 1s 315us/step - loss: 0.0823 - acc: 0.9764 - val_loss: 0.1802 - val_acc: 0.9414\n",
      "Epoch 682/1000\n",
      "2285/2285 [==============================] - 1s 405us/step - loss: 0.0847 - acc: 0.9742 - val_loss: 0.1835 - val_acc: 0.9334\n",
      "Epoch 683/1000\n",
      "2285/2285 [==============================] - 1s 373us/step - loss: 0.0825 - acc: 0.9716 - val_loss: 0.1819 - val_acc: 0.9343\n",
      "Epoch 684/1000\n",
      "2285/2285 [==============================] - 1s 325us/step - loss: 0.0833 - acc: 0.9755 - val_loss: 0.1796 - val_acc: 0.9387\n",
      "Epoch 685/1000\n",
      "2285/2285 [==============================] - 1s 317us/step - loss: 0.0794 - acc: 0.9742 - val_loss: 0.1685 - val_acc: 0.9449\n",
      "Epoch 686/1000\n",
      "2285/2285 [==============================] - 1s 318us/step - loss: 0.0847 - acc: 0.9716 - val_loss: 0.1658 - val_acc: 0.9458\n",
      "Epoch 687/1000\n",
      "2285/2285 [==============================] - 1s 322us/step - loss: 0.0796 - acc: 0.9729 - val_loss: 0.2031 - val_acc: 0.9316\n",
      "Epoch 688/1000\n",
      "2285/2285 [==============================] - 1s 317us/step - loss: 0.0835 - acc: 0.9751 - val_loss: 0.1644 - val_acc: 0.9485\n",
      "Epoch 689/1000\n",
      "2285/2285 [==============================] - 1s 314us/step - loss: 0.1021 - acc: 0.9659 - val_loss: 0.1705 - val_acc: 0.9396\n",
      "Epoch 690/1000\n",
      "2285/2285 [==============================] - 1s 318us/step - loss: 0.0800 - acc: 0.9746 - val_loss: 0.1612 - val_acc: 0.9449\n",
      "Epoch 691/1000\n",
      "2285/2285 [==============================] - 1s 316us/step - loss: 0.0678 - acc: 0.9834 - val_loss: 0.1684 - val_acc: 0.9494\n",
      "Epoch 692/1000\n",
      "2285/2285 [==============================] - 1s 314us/step - loss: 0.0750 - acc: 0.9746 - val_loss: 0.1711 - val_acc: 0.9476\n",
      "Epoch 693/1000\n",
      "2285/2285 [==============================] - 1s 316us/step - loss: 0.0714 - acc: 0.9799 - val_loss: 0.1687 - val_acc: 0.9423\n",
      "Epoch 694/1000\n",
      "2285/2285 [==============================] - 1s 465us/step - loss: 0.0727 - acc: 0.9777 - val_loss: 0.1781 - val_acc: 0.9458\n",
      "Epoch 695/1000\n",
      "2285/2285 [==============================] - 1s 334us/step - loss: 0.0802 - acc: 0.9764 - val_loss: 0.1873 - val_acc: 0.9387\n",
      "Epoch 696/1000\n",
      "2285/2285 [==============================] - 1s 326us/step - loss: 0.0823 - acc: 0.9733 - val_loss: 0.1741 - val_acc: 0.9458\n",
      "Epoch 697/1000\n",
      "2285/2285 [==============================] - 1s 315us/step - loss: 0.0731 - acc: 0.9746 - val_loss: 0.1736 - val_acc: 0.9414\n",
      "Epoch 698/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 312us/step - loss: 0.0789 - acc: 0.9729 - val_loss: 0.1630 - val_acc: 0.9494\n",
      "Epoch 699/1000\n",
      "2285/2285 [==============================] - 1s 316us/step - loss: 0.0807 - acc: 0.9720 - val_loss: 0.1933 - val_acc: 0.9254\n",
      "Epoch 700/1000\n",
      "2285/2285 [==============================] - 1s 313us/step - loss: 0.0823 - acc: 0.9737 - val_loss: 0.1680 - val_acc: 0.9440\n",
      "Epoch 701/1000\n",
      "2285/2285 [==============================] - 1s 315us/step - loss: 0.0819 - acc: 0.9755 - val_loss: 0.1976 - val_acc: 0.9254\n",
      "Epoch 702/1000\n",
      "2285/2285 [==============================] - 1s 356us/step - loss: 0.0794 - acc: 0.9751 - val_loss: 0.1625 - val_acc: 0.9440\n",
      "Epoch 703/1000\n",
      "2285/2285 [==============================] - 1s 352us/step - loss: 0.0843 - acc: 0.9702 - val_loss: 0.1774 - val_acc: 0.9423\n",
      "Epoch 704/1000\n",
      "2285/2285 [==============================] - 1s 327us/step - loss: 0.0719 - acc: 0.9777 - val_loss: 0.1611 - val_acc: 0.9485\n",
      "Epoch 705/1000\n",
      "2285/2285 [==============================] - 1s 314us/step - loss: 0.0700 - acc: 0.9790 - val_loss: 0.1626 - val_acc: 0.9440\n",
      "Epoch 706/1000\n",
      "2285/2285 [==============================] - 1s 329us/step - loss: 0.0811 - acc: 0.9711 - val_loss: 0.1691 - val_acc: 0.9449\n",
      "Epoch 707/1000\n",
      "2285/2285 [==============================] - 1s 316us/step - loss: 0.0683 - acc: 0.9777 - val_loss: 0.1682 - val_acc: 0.9529\n",
      "Epoch 708/1000\n",
      "2285/2285 [==============================] - 1s 334us/step - loss: 0.0761 - acc: 0.9755 - val_loss: 0.1756 - val_acc: 0.9449\n",
      "Epoch 709/1000\n",
      "2285/2285 [==============================] - 1s 317us/step - loss: 0.0809 - acc: 0.9733 - val_loss: 0.1684 - val_acc: 0.9440\n",
      "Epoch 710/1000\n",
      "2285/2285 [==============================] - 1s 312us/step - loss: 0.0877 - acc: 0.9663 - val_loss: 0.1659 - val_acc: 0.9520\n",
      "Epoch 711/1000\n",
      "2285/2285 [==============================] - 1s 314us/step - loss: 0.0837 - acc: 0.9711 - val_loss: 0.1944 - val_acc: 0.9352\n",
      "Epoch 712/1000\n",
      "2285/2285 [==============================] - 1s 315us/step - loss: 0.0754 - acc: 0.9724 - val_loss: 0.1657 - val_acc: 0.9432\n",
      "Epoch 713/1000\n",
      "2285/2285 [==============================] - 1s 314us/step - loss: 0.0745 - acc: 0.9742 - val_loss: 0.1574 - val_acc: 0.9467\n",
      "Epoch 714/1000\n",
      "2285/2285 [==============================] - 1s 318us/step - loss: 0.0694 - acc: 0.9781 - val_loss: 0.1802 - val_acc: 0.9369\n",
      "Epoch 715/1000\n",
      "2285/2285 [==============================] - 1s 314us/step - loss: 0.0797 - acc: 0.9720 - val_loss: 0.1646 - val_acc: 0.9512\n",
      "Epoch 716/1000\n",
      "2285/2285 [==============================] - 1s 313us/step - loss: 0.0771 - acc: 0.9716 - val_loss: 0.1709 - val_acc: 0.9485\n",
      "Epoch 717/1000\n",
      "2285/2285 [==============================] - 1s 315us/step - loss: 0.0749 - acc: 0.9768 - val_loss: 0.1770 - val_acc: 0.9467\n",
      "Epoch 718/1000\n",
      "2285/2285 [==============================] - 1s 315us/step - loss: 0.0762 - acc: 0.9737 - val_loss: 0.1722 - val_acc: 0.9467\n",
      "Epoch 719/1000\n",
      "2285/2285 [==============================] - 1s 317us/step - loss: 0.0689 - acc: 0.9781 - val_loss: 0.1627 - val_acc: 0.9467\n",
      "Epoch 720/1000\n",
      "2285/2285 [==============================] - 1s 330us/step - loss: 0.0787 - acc: 0.9755 - val_loss: 0.1598 - val_acc: 0.9449\n",
      "Epoch 721/1000\n",
      "2285/2285 [==============================] - 1s 317us/step - loss: 0.0734 - acc: 0.9768 - val_loss: 0.1509 - val_acc: 0.9494\n",
      "Epoch 722/1000\n",
      "2285/2285 [==============================] - 1s 317us/step - loss: 0.0763 - acc: 0.9737 - val_loss: 0.1690 - val_acc: 0.9467\n",
      "Epoch 723/1000\n",
      "2285/2285 [==============================] - 1s 399us/step - loss: 0.0762 - acc: 0.9733 - val_loss: 0.1715 - val_acc: 0.9449\n",
      "Epoch 724/1000\n",
      "2285/2285 [==============================] - 1s 435us/step - loss: 0.0687 - acc: 0.9751 - val_loss: 0.1587 - val_acc: 0.9503\n",
      "Epoch 725/1000\n",
      "2285/2285 [==============================] - 1s 472us/step - loss: 0.0824 - acc: 0.9729 - val_loss: 0.1828 - val_acc: 0.9378\n",
      "Epoch 726/1000\n",
      "2285/2285 [==============================] - 1s 380us/step - loss: 0.0739 - acc: 0.9786 - val_loss: 0.1642 - val_acc: 0.9529\n",
      "Epoch 727/1000\n",
      "2285/2285 [==============================] - 1s 369us/step - loss: 0.0692 - acc: 0.9772 - val_loss: 0.1528 - val_acc: 0.9556\n",
      "Epoch 728/1000\n",
      "2285/2285 [==============================] - 1s 328us/step - loss: 0.0696 - acc: 0.9737 - val_loss: 0.1684 - val_acc: 0.9423\n",
      "Epoch 729/1000\n",
      "2285/2285 [==============================] - 1s 354us/step - loss: 0.0678 - acc: 0.9799 - val_loss: 0.1682 - val_acc: 0.9467\n",
      "Epoch 730/1000\n",
      "2285/2285 [==============================] - 1s 347us/step - loss: 0.0727 - acc: 0.9772 - val_loss: 0.1508 - val_acc: 0.9494\n",
      "Epoch 731/1000\n",
      "2285/2285 [==============================] - 1s 318us/step - loss: 0.0679 - acc: 0.9794 - val_loss: 0.1745 - val_acc: 0.9485\n",
      "Epoch 732/1000\n",
      "2285/2285 [==============================] - 1s 466us/step - loss: 0.0675 - acc: 0.9821 - val_loss: 0.1570 - val_acc: 0.9503\n",
      "Epoch 733/1000\n",
      "2285/2285 [==============================] - 1s 374us/step - loss: 0.0638 - acc: 0.9803 - val_loss: 0.1655 - val_acc: 0.9512\n",
      "Epoch 734/1000\n",
      "2285/2285 [==============================] - 1s 326us/step - loss: 0.0566 - acc: 0.9825 - val_loss: 0.1663 - val_acc: 0.9485\n",
      "Epoch 735/1000\n",
      "2285/2285 [==============================] - 1s 335us/step - loss: 0.0721 - acc: 0.9807 - val_loss: 0.1677 - val_acc: 0.9449\n",
      "Epoch 736/1000\n",
      "2285/2285 [==============================] - 1s 314us/step - loss: 0.0737 - acc: 0.9764 - val_loss: 0.1734 - val_acc: 0.9485\n",
      "Epoch 737/1000\n",
      "2285/2285 [==============================] - 1s 350us/step - loss: 0.0733 - acc: 0.9755 - val_loss: 0.1853 - val_acc: 0.9396\n",
      "Epoch 738/1000\n",
      "2285/2285 [==============================] - 1s 606us/step - loss: 0.0792 - acc: 0.9755 - val_loss: 0.1590 - val_acc: 0.9440\n",
      "Epoch 739/1000\n",
      "2285/2285 [==============================] - 1s 352us/step - loss: 0.0690 - acc: 0.9803 - val_loss: 0.1608 - val_acc: 0.9494\n",
      "Epoch 740/1000\n",
      "2285/2285 [==============================] - 1s 324us/step - loss: 0.0722 - acc: 0.9759 - val_loss: 0.1498 - val_acc: 0.9503\n",
      "Epoch 741/1000\n",
      "2285/2285 [==============================] - 1s 317us/step - loss: 0.0715 - acc: 0.9742 - val_loss: 0.1539 - val_acc: 0.9449\n",
      "Epoch 742/1000\n",
      "2285/2285 [==============================] - 1s 315us/step - loss: 0.0748 - acc: 0.9755 - val_loss: 0.1654 - val_acc: 0.9396\n",
      "Epoch 743/1000\n",
      "2285/2285 [==============================] - 1s 343us/step - loss: 0.0751 - acc: 0.9729 - val_loss: 0.1626 - val_acc: 0.9432\n",
      "Epoch 744/1000\n",
      "2285/2285 [==============================] - 1s 339us/step - loss: 0.0648 - acc: 0.9794 - val_loss: 0.1678 - val_acc: 0.9440\n",
      "Epoch 745/1000\n",
      "2285/2285 [==============================] - 1s 330us/step - loss: 0.0708 - acc: 0.9755 - val_loss: 0.1846 - val_acc: 0.9316\n",
      "Epoch 746/1000\n",
      "2285/2285 [==============================] - 1s 313us/step - loss: 0.0678 - acc: 0.9759 - val_loss: 0.1791 - val_acc: 0.9414\n",
      "Epoch 747/1000\n",
      "2285/2285 [==============================] - 1s 508us/step - loss: 0.0727 - acc: 0.9786 - val_loss: 0.1644 - val_acc: 0.9458\n",
      "Epoch 748/1000\n",
      "2285/2285 [==============================] - 1s 434us/step - loss: 0.0639 - acc: 0.9790 - val_loss: 0.1520 - val_acc: 0.9529\n",
      "Epoch 749/1000\n",
      "2285/2285 [==============================] - 1s 530us/step - loss: 0.0626 - acc: 0.9794 - val_loss: 0.1517 - val_acc: 0.9485\n",
      "Epoch 750/1000\n",
      "2285/2285 [==============================] - 2s 668us/step - loss: 0.0783 - acc: 0.9720 - val_loss: 0.1636 - val_acc: 0.9405\n",
      "Epoch 751/1000\n",
      "2285/2285 [==============================] - 1s 645us/step - loss: 0.0774 - acc: 0.9772 - val_loss: 0.1818 - val_acc: 0.9449\n",
      "Epoch 752/1000\n",
      "2285/2285 [==============================] - 2s 696us/step - loss: 0.0732 - acc: 0.9772 - val_loss: 0.1510 - val_acc: 0.9476\n",
      "Epoch 753/1000\n",
      "2285/2285 [==============================] - 1s 643us/step - loss: 0.0731 - acc: 0.9751 - val_loss: 0.1592 - val_acc: 0.9556\n",
      "Epoch 754/1000\n",
      "2285/2285 [==============================] - 2s 693us/step - loss: 0.0623 - acc: 0.9821 - val_loss: 0.1604 - val_acc: 0.9476\n",
      "Epoch 755/1000\n",
      "2285/2285 [==============================] - 1s 619us/step - loss: 0.0668 - acc: 0.9803 - val_loss: 0.1594 - val_acc: 0.9440\n",
      "Epoch 756/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 411us/step - loss: 0.0738 - acc: 0.9772 - val_loss: 0.1594 - val_acc: 0.9467\n",
      "Epoch 757/1000\n",
      "2285/2285 [==============================] - 1s 462us/step - loss: 0.0670 - acc: 0.9803 - val_loss: 0.1441 - val_acc: 0.9512\n",
      "Epoch 758/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.0670 - acc: 0.9755 - val_loss: 0.1703 - val_acc: 0.9440\n",
      "Epoch 759/1000\n",
      "2285/2285 [==============================] - 1s 401us/step - loss: 0.0747 - acc: 0.9737 - val_loss: 0.1898 - val_acc: 0.9325\n",
      "Epoch 760/1000\n",
      "2285/2285 [==============================] - 1s 400us/step - loss: 0.0619 - acc: 0.9816 - val_loss: 0.1654 - val_acc: 0.9467\n",
      "Epoch 761/1000\n",
      "2285/2285 [==============================] - 1s 400us/step - loss: 0.0651 - acc: 0.9803 - val_loss: 0.1512 - val_acc: 0.9529\n",
      "Epoch 762/1000\n",
      "2285/2285 [==============================] - 1s 447us/step - loss: 0.0684 - acc: 0.9772 - val_loss: 0.1457 - val_acc: 0.9485\n",
      "Epoch 763/1000\n",
      "2285/2285 [==============================] - 1s 398us/step - loss: 0.0677 - acc: 0.9759 - val_loss: 0.1608 - val_acc: 0.9449\n",
      "Epoch 764/1000\n",
      "2285/2285 [==============================] - 1s 397us/step - loss: 0.0714 - acc: 0.9781 - val_loss: 0.1762 - val_acc: 0.9449\n",
      "Epoch 765/1000\n",
      "2285/2285 [==============================] - 1s 403us/step - loss: 0.0726 - acc: 0.9781 - val_loss: 0.1491 - val_acc: 0.9520\n",
      "Epoch 766/1000\n",
      "2285/2285 [==============================] - 1s 456us/step - loss: 0.0687 - acc: 0.9794 - val_loss: 0.1607 - val_acc: 0.9440\n",
      "Epoch 767/1000\n",
      "2285/2285 [==============================] - 1s 400us/step - loss: 0.0648 - acc: 0.9807 - val_loss: 0.1575 - val_acc: 0.9440\n",
      "Epoch 768/1000\n",
      "2285/2285 [==============================] - 1s 459us/step - loss: 0.0649 - acc: 0.9751 - val_loss: 0.1591 - val_acc: 0.9458\n",
      "Epoch 769/1000\n",
      "2285/2285 [==============================] - 1s 423us/step - loss: 0.0638 - acc: 0.9794 - val_loss: 0.1470 - val_acc: 0.9476\n",
      "Epoch 770/1000\n",
      "2285/2285 [==============================] - 1s 404us/step - loss: 0.0700 - acc: 0.9772 - val_loss: 0.1852 - val_acc: 0.9405\n",
      "Epoch 771/1000\n",
      "2285/2285 [==============================] - 1s 467us/step - loss: 0.0642 - acc: 0.9803 - val_loss: 0.1564 - val_acc: 0.9494\n",
      "Epoch 772/1000\n",
      "2285/2285 [==============================] - 1s 409us/step - loss: 0.0629 - acc: 0.9781 - val_loss: 0.1500 - val_acc: 0.9503\n",
      "Epoch 773/1000\n",
      "2285/2285 [==============================] - 1s 439us/step - loss: 0.0786 - acc: 0.9689 - val_loss: 0.1662 - val_acc: 0.9467\n",
      "Epoch 774/1000\n",
      "2285/2285 [==============================] - 1s 404us/step - loss: 0.0603 - acc: 0.9786 - val_loss: 0.1535 - val_acc: 0.9458\n",
      "Epoch 775/1000\n",
      "2285/2285 [==============================] - 1s 409us/step - loss: 0.0584 - acc: 0.9816 - val_loss: 0.1474 - val_acc: 0.9538\n",
      "Epoch 776/1000\n",
      "2285/2285 [==============================] - 1s 464us/step - loss: 0.0664 - acc: 0.9786 - val_loss: 0.1484 - val_acc: 0.9512\n",
      "Epoch 777/1000\n",
      "2285/2285 [==============================] - 1s 403us/step - loss: 0.0585 - acc: 0.9829 - val_loss: 0.1546 - val_acc: 0.9538\n",
      "Epoch 778/1000\n",
      "2285/2285 [==============================] - 1s 407us/step - loss: 0.0709 - acc: 0.9777 - val_loss: 0.1497 - val_acc: 0.9423\n",
      "Epoch 779/1000\n",
      "2285/2285 [==============================] - 1s 406us/step - loss: 0.0662 - acc: 0.9777 - val_loss: 0.1513 - val_acc: 0.9512\n",
      "Epoch 780/1000\n",
      "2285/2285 [==============================] - 1s 484us/step - loss: 0.0648 - acc: 0.9807 - val_loss: 0.1673 - val_acc: 0.9476\n",
      "Epoch 781/1000\n",
      "2285/2285 [==============================] - 1s 392us/step - loss: 0.0698 - acc: 0.9794 - val_loss: 0.1498 - val_acc: 0.9503\n",
      "Epoch 782/1000\n",
      "2285/2285 [==============================] - 1s 415us/step - loss: 0.0646 - acc: 0.9777 - val_loss: 0.1577 - val_acc: 0.9529\n",
      "Epoch 783/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 0.0562 - acc: 0.9794 - val_loss: 0.1513 - val_acc: 0.9529\n",
      "Epoch 784/1000\n",
      "2285/2285 [==============================] - 1s 406us/step - loss: 0.0735 - acc: 0.9803 - val_loss: 0.1660 - val_acc: 0.9440\n",
      "Epoch 785/1000\n",
      "2285/2285 [==============================] - 1s 459us/step - loss: 0.0637 - acc: 0.9812 - val_loss: 0.1533 - val_acc: 0.9503\n",
      "Epoch 786/1000\n",
      "2285/2285 [==============================] - 1s 403us/step - loss: 0.0663 - acc: 0.9759 - val_loss: 0.1429 - val_acc: 0.9547\n",
      "Epoch 787/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.0577 - acc: 0.9816 - val_loss: 0.1412 - val_acc: 0.9538\n",
      "Epoch 788/1000\n",
      "2285/2285 [==============================] - 1s 402us/step - loss: 0.0676 - acc: 0.9786 - val_loss: 0.1604 - val_acc: 0.9494\n",
      "Epoch 789/1000\n",
      "2285/2285 [==============================] - 1s 434us/step - loss: 0.0693 - acc: 0.9772 - val_loss: 0.1873 - val_acc: 0.9414\n",
      "Epoch 790/1000\n",
      "2285/2285 [==============================] - 1s 424us/step - loss: 0.0662 - acc: 0.9768 - val_loss: 0.1609 - val_acc: 0.9512\n",
      "Epoch 791/1000\n",
      "2285/2285 [==============================] - 1s 480us/step - loss: 0.0698 - acc: 0.9790 - val_loss: 0.1511 - val_acc: 0.9467\n",
      "Epoch 792/1000\n",
      "2285/2285 [==============================] - 1s 616us/step - loss: 0.0633 - acc: 0.9768 - val_loss: 0.1627 - val_acc: 0.9414\n",
      "Epoch 793/1000\n",
      "2285/2285 [==============================] - 2s 709us/step - loss: 0.0606 - acc: 0.9807 - val_loss: 0.1681 - val_acc: 0.9512\n",
      "Epoch 794/1000\n",
      "2285/2285 [==============================] - 1s 655us/step - loss: 0.0661 - acc: 0.9794 - val_loss: 0.1508 - val_acc: 0.9467\n",
      "Epoch 795/1000\n",
      "2285/2285 [==============================] - 1s 580us/step - loss: 0.0570 - acc: 0.9847 - val_loss: 0.1468 - val_acc: 0.9503\n",
      "Epoch 796/1000\n",
      "2285/2285 [==============================] - 1s 593us/step - loss: 0.0592 - acc: 0.9803 - val_loss: 0.1531 - val_acc: 0.9485\n",
      "Epoch 797/1000\n",
      "2285/2285 [==============================] - 1s 416us/step - loss: 0.0618 - acc: 0.9803 - val_loss: 0.1415 - val_acc: 0.9485\n",
      "Epoch 798/1000\n",
      "2285/2285 [==============================] - 1s 462us/step - loss: 0.0593 - acc: 0.9834 - val_loss: 0.1457 - val_acc: 0.9547\n",
      "Epoch 799/1000\n",
      "2285/2285 [==============================] - 1s 403us/step - loss: 0.0662 - acc: 0.9812 - val_loss: 0.1650 - val_acc: 0.9467\n",
      "Epoch 800/1000\n",
      "2285/2285 [==============================] - 1s 407us/step - loss: 0.0627 - acc: 0.9807 - val_loss: 0.1511 - val_acc: 0.9485\n",
      "Epoch 801/1000\n",
      "2285/2285 [==============================] - 1s 405us/step - loss: 0.0544 - acc: 0.9842 - val_loss: 0.1499 - val_acc: 0.9538\n",
      "Epoch 802/1000\n",
      "2285/2285 [==============================] - 1s 410us/step - loss: 0.0615 - acc: 0.9790 - val_loss: 0.1539 - val_acc: 0.9485\n",
      "Epoch 803/1000\n",
      "2285/2285 [==============================] - 1s 458us/step - loss: 0.0581 - acc: 0.9842 - val_loss: 0.1505 - val_acc: 0.9538\n",
      "Epoch 804/1000\n",
      "2285/2285 [==============================] - 1s 404us/step - loss: 0.0558 - acc: 0.9829 - val_loss: 0.1491 - val_acc: 0.9520\n",
      "Epoch 805/1000\n",
      "2285/2285 [==============================] - 1s 409us/step - loss: 0.0662 - acc: 0.9781 - val_loss: 0.1485 - val_acc: 0.9538\n",
      "Epoch 806/1000\n",
      "2285/2285 [==============================] - 1s 409us/step - loss: 0.0616 - acc: 0.9794 - val_loss: 0.1678 - val_acc: 0.9432\n",
      "Epoch 807/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.0584 - acc: 0.9790 - val_loss: 0.1499 - val_acc: 0.9494\n",
      "Epoch 808/1000\n",
      "2285/2285 [==============================] - 1s 443us/step - loss: 0.0589 - acc: 0.9794 - val_loss: 0.1426 - val_acc: 0.9485\n",
      "Epoch 809/1000\n",
      "2285/2285 [==============================] - 1s 416us/step - loss: 0.0579 - acc: 0.9816 - val_loss: 0.1463 - val_acc: 0.9529\n",
      "Epoch 810/1000\n",
      "2285/2285 [==============================] - 1s 402us/step - loss: 0.0620 - acc: 0.9799 - val_loss: 0.1544 - val_acc: 0.9494\n",
      "Epoch 811/1000\n",
      "2285/2285 [==============================] - 1s 415us/step - loss: 0.0655 - acc: 0.9790 - val_loss: 0.1572 - val_acc: 0.9529\n",
      "Epoch 812/1000\n",
      "2285/2285 [==============================] - 1s 398us/step - loss: 0.0626 - acc: 0.9794 - val_loss: 0.1889 - val_acc: 0.9423\n",
      "Epoch 813/1000\n",
      "2285/2285 [==============================] - 1s 405us/step - loss: 0.0734 - acc: 0.9755 - val_loss: 0.1689 - val_acc: 0.9440\n",
      "Epoch 814/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 419us/step - loss: 0.0613 - acc: 0.9812 - val_loss: 0.1695 - val_acc: 0.9440\n",
      "Epoch 815/1000\n",
      "2285/2285 [==============================] - 1s 401us/step - loss: 0.0763 - acc: 0.9786 - val_loss: 0.1577 - val_acc: 0.9503\n",
      "Epoch 816/1000\n",
      "2285/2285 [==============================] - 1s 404us/step - loss: 0.0639 - acc: 0.9790 - val_loss: 0.1652 - val_acc: 0.9494\n",
      "Epoch 817/1000\n",
      "2285/2285 [==============================] - 1s 405us/step - loss: 0.0655 - acc: 0.9772 - val_loss: 0.1589 - val_acc: 0.9467\n",
      "Epoch 818/1000\n",
      "2285/2285 [==============================] - 1s 399us/step - loss: 0.0710 - acc: 0.9781 - val_loss: 0.1465 - val_acc: 0.9520\n",
      "Epoch 819/1000\n",
      "2285/2285 [==============================] - 1s 411us/step - loss: 0.0644 - acc: 0.9816 - val_loss: 0.1648 - val_acc: 0.9494\n",
      "Epoch 820/1000\n",
      "2285/2285 [==============================] - 1s 399us/step - loss: 0.0609 - acc: 0.9790 - val_loss: 0.1518 - val_acc: 0.9494\n",
      "Epoch 821/1000\n",
      "2285/2285 [==============================] - 1s 392us/step - loss: 0.0601 - acc: 0.9812 - val_loss: 0.1920 - val_acc: 0.9361\n",
      "Epoch 822/1000\n",
      "2285/2285 [==============================] - 1s 397us/step - loss: 0.0613 - acc: 0.9786 - val_loss: 0.1582 - val_acc: 0.9494\n",
      "Epoch 823/1000\n",
      "2285/2285 [==============================] - 1s 397us/step - loss: 0.0550 - acc: 0.9821 - val_loss: 0.1565 - val_acc: 0.9467\n",
      "Epoch 824/1000\n",
      "2285/2285 [==============================] - 1s 394us/step - loss: 0.0541 - acc: 0.9825 - val_loss: 0.1646 - val_acc: 0.9458\n",
      "Epoch 825/1000\n",
      "2285/2285 [==============================] - 1s 403us/step - loss: 0.0621 - acc: 0.9799 - val_loss: 0.1524 - val_acc: 0.9485\n",
      "Epoch 826/1000\n",
      "2285/2285 [==============================] - 1s 419us/step - loss: 0.0567 - acc: 0.9825 - val_loss: 0.1576 - val_acc: 0.9467\n",
      "Epoch 827/1000\n",
      "2285/2285 [==============================] - 1s 421us/step - loss: 0.0654 - acc: 0.9816 - val_loss: 0.1606 - val_acc: 0.9414\n",
      "Epoch 828/1000\n",
      "2285/2285 [==============================] - 1s 401us/step - loss: 0.0575 - acc: 0.9799 - val_loss: 0.1691 - val_acc: 0.9396\n",
      "Epoch 829/1000\n",
      "2285/2285 [==============================] - 1s 412us/step - loss: 0.0593 - acc: 0.9803 - val_loss: 0.1680 - val_acc: 0.9520\n",
      "Epoch 830/1000\n",
      "2285/2285 [==============================] - 1s 404us/step - loss: 0.0648 - acc: 0.9759 - val_loss: 0.1499 - val_acc: 0.9520\n",
      "Epoch 831/1000\n",
      "2285/2285 [==============================] - 1s 398us/step - loss: 0.0568 - acc: 0.9812 - val_loss: 0.1470 - val_acc: 0.9485\n",
      "Epoch 832/1000\n",
      "2285/2285 [==============================] - 1s 405us/step - loss: 0.0608 - acc: 0.9812 - val_loss: 0.1575 - val_acc: 0.9476\n",
      "Epoch 833/1000\n",
      "2285/2285 [==============================] - 1s 399us/step - loss: 0.0560 - acc: 0.9821 - val_loss: 0.1887 - val_acc: 0.9361\n",
      "Epoch 834/1000\n",
      "2285/2285 [==============================] - 1s 410us/step - loss: 0.0578 - acc: 0.9821 - val_loss: 0.1798 - val_acc: 0.9414\n",
      "Epoch 835/1000\n",
      "2285/2285 [==============================] - 1s 403us/step - loss: 0.0576 - acc: 0.9834 - val_loss: 0.1504 - val_acc: 0.9494\n",
      "Epoch 836/1000\n",
      "2285/2285 [==============================] - 1s 393us/step - loss: 0.0576 - acc: 0.9825 - val_loss: 0.1639 - val_acc: 0.9387\n",
      "Epoch 837/1000\n",
      "2285/2285 [==============================] - 1s 400us/step - loss: 0.0661 - acc: 0.9781 - val_loss: 0.1687 - val_acc: 0.9485\n",
      "Epoch 838/1000\n",
      "2285/2285 [==============================] - 1s 398us/step - loss: 0.0583 - acc: 0.9799 - val_loss: 0.1506 - val_acc: 0.9485\n",
      "Epoch 839/1000\n",
      "2285/2285 [==============================] - 1s 389us/step - loss: 0.0599 - acc: 0.9807 - val_loss: 0.1568 - val_acc: 0.9405\n",
      "Epoch 840/1000\n",
      "2285/2285 [==============================] - 1s 389us/step - loss: 0.0554 - acc: 0.9821 - val_loss: 0.1551 - val_acc: 0.9503\n",
      "Epoch 841/1000\n",
      "2285/2285 [==============================] - 1s 392us/step - loss: 0.0587 - acc: 0.9812 - val_loss: 0.1482 - val_acc: 0.9512\n",
      "Epoch 842/1000\n",
      "2285/2285 [==============================] - 1s 435us/step - loss: 0.0617 - acc: 0.9812 - val_loss: 0.1490 - val_acc: 0.9476\n",
      "Epoch 843/1000\n",
      "2285/2285 [==============================] - 1s 420us/step - loss: 0.0570 - acc: 0.9794 - val_loss: 0.1516 - val_acc: 0.9512\n",
      "Epoch 844/1000\n",
      "2285/2285 [==============================] - 1s 397us/step - loss: 0.0586 - acc: 0.9834 - val_loss: 0.1631 - val_acc: 0.9494\n",
      "Epoch 845/1000\n",
      "2285/2285 [==============================] - 1s 394us/step - loss: 0.0526 - acc: 0.9821 - val_loss: 0.1764 - val_acc: 0.9396\n",
      "Epoch 846/1000\n",
      "2285/2285 [==============================] - 1s 390us/step - loss: 0.0623 - acc: 0.9764 - val_loss: 0.1512 - val_acc: 0.9476\n",
      "Epoch 847/1000\n",
      "2285/2285 [==============================] - 1s 390us/step - loss: 0.0617 - acc: 0.9794 - val_loss: 0.1569 - val_acc: 0.9432\n",
      "Epoch 848/1000\n",
      "2285/2285 [==============================] - 1s 404us/step - loss: 0.0521 - acc: 0.9829 - val_loss: 0.1635 - val_acc: 0.9458\n",
      "Epoch 849/1000\n",
      "2285/2285 [==============================] - 1s 387us/step - loss: 0.0578 - acc: 0.9799 - val_loss: 0.1491 - val_acc: 0.9547\n",
      "Epoch 850/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 0.0493 - acc: 0.9860 - val_loss: 0.1413 - val_acc: 0.9547\n",
      "Epoch 851/1000\n",
      "2285/2285 [==============================] - 1s 395us/step - loss: 0.0552 - acc: 0.9816 - val_loss: 0.1477 - val_acc: 0.9476\n",
      "Epoch 852/1000\n",
      "2285/2285 [==============================] - 1s 404us/step - loss: 0.0565 - acc: 0.9825 - val_loss: 0.1607 - val_acc: 0.9476\n",
      "Epoch 853/1000\n",
      "2285/2285 [==============================] - 1s 394us/step - loss: 0.0539 - acc: 0.9860 - val_loss: 0.1406 - val_acc: 0.9476\n",
      "Epoch 854/1000\n",
      "2285/2285 [==============================] - 1s 403us/step - loss: 0.0517 - acc: 0.9842 - val_loss: 0.1437 - val_acc: 0.9467\n",
      "Epoch 855/1000\n",
      "2285/2285 [==============================] - 1s 398us/step - loss: 0.0559 - acc: 0.9807 - val_loss: 0.1375 - val_acc: 0.9494\n",
      "Epoch 856/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 0.0539 - acc: 0.9812 - val_loss: 0.1563 - val_acc: 0.9503\n",
      "Epoch 857/1000\n",
      "2285/2285 [==============================] - 1s 399us/step - loss: 0.0550 - acc: 0.9816 - val_loss: 0.1382 - val_acc: 0.9494\n",
      "Epoch 858/1000\n",
      "2285/2285 [==============================] - 1s 394us/step - loss: 0.0527 - acc: 0.9829 - val_loss: 0.1575 - val_acc: 0.9476\n",
      "Epoch 859/1000\n",
      "2285/2285 [==============================] - 1s 399us/step - loss: 0.0592 - acc: 0.9781 - val_loss: 0.1466 - val_acc: 0.9512\n",
      "Epoch 860/1000\n",
      "2285/2285 [==============================] - 1s 405us/step - loss: 0.0521 - acc: 0.9847 - val_loss: 0.1489 - val_acc: 0.9512\n",
      "Epoch 861/1000\n",
      "2285/2285 [==============================] - 1s 475us/step - loss: 0.0550 - acc: 0.9821 - val_loss: 0.1446 - val_acc: 0.9520\n",
      "Epoch 862/1000\n",
      "2285/2285 [==============================] - 1s 448us/step - loss: 0.0512 - acc: 0.9847 - val_loss: 0.1446 - val_acc: 0.9529\n",
      "Epoch 863/1000\n",
      "2285/2285 [==============================] - 1s 409us/step - loss: 0.0631 - acc: 0.9772 - val_loss: 0.1598 - val_acc: 0.9414\n",
      "Epoch 864/1000\n",
      "2285/2285 [==============================] - 1s 405us/step - loss: 0.0629 - acc: 0.9772 - val_loss: 0.1531 - val_acc: 0.9494\n",
      "Epoch 865/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.0543 - acc: 0.9847 - val_loss: 0.1648 - val_acc: 0.9458\n",
      "Epoch 866/1000\n",
      "2285/2285 [==============================] - 1s 395us/step - loss: 0.0600 - acc: 0.9777 - val_loss: 0.1733 - val_acc: 0.9432\n",
      "Epoch 867/1000\n",
      "2285/2285 [==============================] - 1s 391us/step - loss: 0.0610 - acc: 0.9772 - val_loss: 0.1706 - val_acc: 0.9449\n",
      "Epoch 868/1000\n",
      "2285/2285 [==============================] - 1s 394us/step - loss: 0.0514 - acc: 0.9856 - val_loss: 0.1519 - val_acc: 0.9458\n",
      "Epoch 869/1000\n",
      "2285/2285 [==============================] - 1s 388us/step - loss: 0.0519 - acc: 0.9821 - val_loss: 0.1433 - val_acc: 0.9538\n",
      "Epoch 870/1000\n",
      "2285/2285 [==============================] - 1s 401us/step - loss: 0.0517 - acc: 0.9829 - val_loss: 0.1484 - val_acc: 0.9494\n",
      "Epoch 871/1000\n",
      "2285/2285 [==============================] - 1s 398us/step - loss: 0.0501 - acc: 0.9834 - val_loss: 0.1637 - val_acc: 0.9458\n",
      "Epoch 872/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 393us/step - loss: 0.0599 - acc: 0.9799 - val_loss: 0.1522 - val_acc: 0.9565\n",
      "Epoch 873/1000\n",
      "2285/2285 [==============================] - 1s 395us/step - loss: 0.0596 - acc: 0.9821 - val_loss: 0.1649 - val_acc: 0.9467\n",
      "Epoch 874/1000\n",
      "2285/2285 [==============================] - 1s 400us/step - loss: 0.0581 - acc: 0.9851 - val_loss: 0.1658 - val_acc: 0.9352\n",
      "Epoch 875/1000\n",
      "2285/2285 [==============================] - 1s 406us/step - loss: 0.0533 - acc: 0.9786 - val_loss: 0.1408 - val_acc: 0.9538\n",
      "Epoch 876/1000\n",
      "2285/2285 [==============================] - 1s 406us/step - loss: 0.0538 - acc: 0.9816 - val_loss: 0.1587 - val_acc: 0.9467\n",
      "Epoch 877/1000\n",
      "2285/2285 [==============================] - 1s 398us/step - loss: 0.0520 - acc: 0.9829 - val_loss: 0.1521 - val_acc: 0.9538\n",
      "Epoch 878/1000\n",
      "2285/2285 [==============================] - 1s 405us/step - loss: 0.0555 - acc: 0.9838 - val_loss: 0.1536 - val_acc: 0.9503\n",
      "Epoch 879/1000\n",
      "2285/2285 [==============================] - 1s 395us/step - loss: 0.0542 - acc: 0.9794 - val_loss: 0.1516 - val_acc: 0.9485\n",
      "Epoch 880/1000\n",
      "2285/2285 [==============================] - 1s 398us/step - loss: 0.0515 - acc: 0.9812 - val_loss: 0.1472 - val_acc: 0.9538\n",
      "Epoch 881/1000\n",
      "2285/2285 [==============================] - 1s 398us/step - loss: 0.0445 - acc: 0.9891 - val_loss: 0.1630 - val_acc: 0.9476\n",
      "Epoch 882/1000\n",
      "2285/2285 [==============================] - 1s 394us/step - loss: 0.0474 - acc: 0.9869 - val_loss: 0.1547 - val_acc: 0.9520\n",
      "Epoch 883/1000\n",
      "2285/2285 [==============================] - 1s 398us/step - loss: 0.0490 - acc: 0.9851 - val_loss: 0.1601 - val_acc: 0.9449\n",
      "Epoch 884/1000\n",
      "2285/2285 [==============================] - 1s 418us/step - loss: 0.0509 - acc: 0.9851 - val_loss: 0.1549 - val_acc: 0.9414\n",
      "Epoch 885/1000\n",
      "2285/2285 [==============================] - 1s 401us/step - loss: 0.0490 - acc: 0.9847 - val_loss: 0.1692 - val_acc: 0.9458\n",
      "Epoch 886/1000\n",
      "2285/2285 [==============================] - 1s 398us/step - loss: 0.0474 - acc: 0.9851 - val_loss: 0.1495 - val_acc: 0.9467\n",
      "Epoch 887/1000\n",
      "2285/2285 [==============================] - 1s 402us/step - loss: 0.0481 - acc: 0.9829 - val_loss: 0.1552 - val_acc: 0.9440\n",
      "Epoch 888/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 0.0561 - acc: 0.9825 - val_loss: 0.1510 - val_acc: 0.9503\n",
      "Epoch 889/1000\n",
      "2285/2285 [==============================] - 1s 393us/step - loss: 0.0557 - acc: 0.9825 - val_loss: 0.1501 - val_acc: 0.9467\n",
      "Epoch 890/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 0.0579 - acc: 0.9794 - val_loss: 0.1591 - val_acc: 0.9503\n",
      "Epoch 891/1000\n",
      "2285/2285 [==============================] - 1s 395us/step - loss: 0.0518 - acc: 0.9829 - val_loss: 0.1561 - val_acc: 0.9503\n",
      "Epoch 892/1000\n",
      "2285/2285 [==============================] - 1s 389us/step - loss: 0.0462 - acc: 0.9873 - val_loss: 0.1353 - val_acc: 0.9520\n",
      "Epoch 893/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 0.0607 - acc: 0.9777 - val_loss: 0.1412 - val_acc: 0.9494\n",
      "Epoch 894/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 0.0468 - acc: 0.9842 - val_loss: 0.1445 - val_acc: 0.9529\n",
      "Epoch 895/1000\n",
      "2285/2285 [==============================] - 1s 409us/step - loss: 0.0490 - acc: 0.9847 - val_loss: 0.1478 - val_acc: 0.9574\n",
      "Epoch 896/1000\n",
      "2285/2285 [==============================] - 1s 394us/step - loss: 0.0381 - acc: 0.9873 - val_loss: 0.1527 - val_acc: 0.9476\n",
      "Epoch 897/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 0.0563 - acc: 0.9812 - val_loss: 0.1412 - val_acc: 0.9512\n",
      "Epoch 898/1000\n",
      "2285/2285 [==============================] - 1s 392us/step - loss: 0.0591 - acc: 0.9790 - val_loss: 0.1692 - val_acc: 0.9467\n",
      "Epoch 899/1000\n",
      "2285/2285 [==============================] - 1s 401us/step - loss: 0.0498 - acc: 0.9829 - val_loss: 0.1711 - val_acc: 0.9423\n",
      "Epoch 900/1000\n",
      "2285/2285 [==============================] - 1s 393us/step - loss: 0.0549 - acc: 0.9816 - val_loss: 0.1439 - val_acc: 0.9556\n",
      "Epoch 901/1000\n",
      "2285/2285 [==============================] - 1s 399us/step - loss: 0.0447 - acc: 0.9882 - val_loss: 0.1559 - val_acc: 0.9565\n",
      "Epoch 902/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 0.0512 - acc: 0.9851 - val_loss: 0.1669 - val_acc: 0.9476\n",
      "Epoch 903/1000\n",
      "2285/2285 [==============================] - 1s 392us/step - loss: 0.0596 - acc: 0.9812 - val_loss: 0.1498 - val_acc: 0.9565\n",
      "Epoch 904/1000\n",
      "2285/2285 [==============================] - 1s 399us/step - loss: 0.0488 - acc: 0.9834 - val_loss: 0.1554 - val_acc: 0.9494\n",
      "Epoch 905/1000\n",
      "2285/2285 [==============================] - 1s 395us/step - loss: 0.0552 - acc: 0.9821 - val_loss: 0.1409 - val_acc: 0.9520\n",
      "Epoch 906/1000\n",
      "2285/2285 [==============================] - 1s 424us/step - loss: 0.0528 - acc: 0.9842 - val_loss: 0.1534 - val_acc: 0.9520\n",
      "Epoch 907/1000\n",
      "2285/2285 [==============================] - 1s 395us/step - loss: 0.0408 - acc: 0.9838 - val_loss: 0.1706 - val_acc: 0.9405\n",
      "Epoch 908/1000\n",
      "2285/2285 [==============================] - 1s 393us/step - loss: 0.0606 - acc: 0.9803 - val_loss: 0.1498 - val_acc: 0.9574\n",
      "Epoch 909/1000\n",
      "2285/2285 [==============================] - 1s 401us/step - loss: 0.0492 - acc: 0.9834 - val_loss: 0.1586 - val_acc: 0.9512\n",
      "Epoch 910/1000\n",
      "2285/2285 [==============================] - 1s 389us/step - loss: 0.0496 - acc: 0.9864 - val_loss: 0.1511 - val_acc: 0.9512\n",
      "Epoch 911/1000\n",
      "2285/2285 [==============================] - 1s 396us/step - loss: 0.0440 - acc: 0.9856 - val_loss: 0.1459 - val_acc: 0.9574\n",
      "Epoch 912/1000\n",
      "2285/2285 [==============================] - 1s 392us/step - loss: 0.0512 - acc: 0.9825 - val_loss: 0.1470 - val_acc: 0.9503\n",
      "Epoch 913/1000\n",
      "2285/2285 [==============================] - 1s 397us/step - loss: 0.0523 - acc: 0.9821 - val_loss: 0.1436 - val_acc: 0.9494\n",
      "Epoch 914/1000\n",
      "2285/2285 [==============================] - 1s 392us/step - loss: 0.0570 - acc: 0.9812 - val_loss: 0.1415 - val_acc: 0.9520\n",
      "Epoch 915/1000\n",
      "2285/2285 [==============================] - 1s 391us/step - loss: 0.0445 - acc: 0.9838 - val_loss: 0.1796 - val_acc: 0.9378\n",
      "Epoch 916/1000\n",
      "2285/2285 [==============================] - 1s 391us/step - loss: 0.0464 - acc: 0.9838 - val_loss: 0.1616 - val_acc: 0.9476\n",
      "Epoch 917/1000\n",
      "2285/2285 [==============================] - 1s 398us/step - loss: 0.0495 - acc: 0.9838 - val_loss: 0.1457 - val_acc: 0.9503\n",
      "Epoch 918/1000\n",
      "2285/2285 [==============================] - 1s 393us/step - loss: 0.0524 - acc: 0.9851 - val_loss: 0.1518 - val_acc: 0.9476\n",
      "Epoch 919/1000\n",
      "2285/2285 [==============================] - 1s 423us/step - loss: 0.0489 - acc: 0.9834 - val_loss: 0.1446 - val_acc: 0.9512\n",
      "Epoch 920/1000\n",
      "2285/2285 [==============================] - 1s 452us/step - loss: 0.0504 - acc: 0.9838 - val_loss: 0.1474 - val_acc: 0.9512\n",
      "Epoch 921/1000\n",
      "2285/2285 [==============================] - 1s 636us/step - loss: 0.0459 - acc: 0.9864 - val_loss: 0.1416 - val_acc: 0.9494\n",
      "Epoch 922/1000\n",
      "2285/2285 [==============================] - 1s 642us/step - loss: 0.0448 - acc: 0.9864 - val_loss: 0.1403 - val_acc: 0.9494\n",
      "Epoch 923/1000\n",
      "2285/2285 [==============================] - 1s 619us/step - loss: 0.0606 - acc: 0.9829 - val_loss: 0.1472 - val_acc: 0.9485\n",
      "Epoch 924/1000\n",
      "2285/2285 [==============================] - 1s 548us/step - loss: 0.0439 - acc: 0.9869 - val_loss: 0.1690 - val_acc: 0.9432\n",
      "Epoch 925/1000\n",
      "2285/2285 [==============================] - 1s 567us/step - loss: 0.0479 - acc: 0.9860 - val_loss: 0.1575 - val_acc: 0.9556\n",
      "Epoch 926/1000\n",
      "2285/2285 [==============================] - 1s 618us/step - loss: 0.0428 - acc: 0.9860 - val_loss: 0.1722 - val_acc: 0.9449\n",
      "Epoch 927/1000\n",
      "2285/2285 [==============================] - 1s 570us/step - loss: 0.0501 - acc: 0.9825 - val_loss: 0.1491 - val_acc: 0.9520\n",
      "Epoch 928/1000\n",
      "2285/2285 [==============================] - 1s 610us/step - loss: 0.0481 - acc: 0.9829 - val_loss: 0.1541 - val_acc: 0.9512\n",
      "Epoch 929/1000\n",
      "2285/2285 [==============================] - 1s 545us/step - loss: 0.0599 - acc: 0.9768 - val_loss: 0.1654 - val_acc: 0.9449\n",
      "Epoch 930/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 628us/step - loss: 0.0462 - acc: 0.9847 - val_loss: 0.1417 - val_acc: 0.9529\n",
      "Epoch 931/1000\n",
      "2285/2285 [==============================] - 1s 378us/step - loss: 0.0671 - acc: 0.9777 - val_loss: 0.1581 - val_acc: 0.9520\n",
      "Epoch 932/1000\n",
      "2285/2285 [==============================] - 1s 623us/step - loss: 0.0448 - acc: 0.9873 - val_loss: 0.1528 - val_acc: 0.9494\n",
      "Epoch 933/1000\n",
      "2285/2285 [==============================] - 1s 592us/step - loss: 0.0554 - acc: 0.9777 - val_loss: 0.1486 - val_acc: 0.9512\n",
      "Epoch 934/1000\n",
      "2285/2285 [==============================] - 1s 592us/step - loss: 0.0421 - acc: 0.9856 - val_loss: 0.1750 - val_acc: 0.9449\n",
      "Epoch 935/1000\n",
      "2285/2285 [==============================] - 1s 575us/step - loss: 0.0485 - acc: 0.9829 - val_loss: 0.1641 - val_acc: 0.9494\n",
      "Epoch 936/1000\n",
      "2285/2285 [==============================] - 1s 571us/step - loss: 0.0433 - acc: 0.9869 - val_loss: 0.1562 - val_acc: 0.9476\n",
      "Epoch 937/1000\n",
      "2285/2285 [==============================] - 1s 495us/step - loss: 0.0494 - acc: 0.9851 - val_loss: 0.1561 - val_acc: 0.9520\n",
      "Epoch 938/1000\n",
      "2285/2285 [==============================] - 1s 530us/step - loss: 0.0504 - acc: 0.9829 - val_loss: 0.1596 - val_acc: 0.9503\n",
      "Epoch 939/1000\n",
      "2285/2285 [==============================] - 1s 495us/step - loss: 0.0513 - acc: 0.9829 - val_loss: 0.1503 - val_acc: 0.9503\n",
      "Epoch 940/1000\n",
      "2285/2285 [==============================] - 1s 478us/step - loss: 0.0489 - acc: 0.9838 - val_loss: 0.1514 - val_acc: 0.9512\n",
      "Epoch 941/1000\n",
      "2285/2285 [==============================] - 1s 534us/step - loss: 0.0503 - acc: 0.9834 - val_loss: 0.1436 - val_acc: 0.9520\n",
      "Epoch 942/1000\n",
      "2285/2285 [==============================] - 1s 526us/step - loss: 0.0516 - acc: 0.9838 - val_loss: 0.1499 - val_acc: 0.9476\n",
      "Epoch 943/1000\n",
      "2285/2285 [==============================] - 1s 481us/step - loss: 0.0497 - acc: 0.9877 - val_loss: 0.1606 - val_acc: 0.9449\n",
      "Epoch 944/1000\n",
      "2285/2285 [==============================] - 1s 416us/step - loss: 0.0470 - acc: 0.9825 - val_loss: 0.1456 - val_acc: 0.9583\n",
      "Epoch 945/1000\n",
      "2285/2285 [==============================] - 1s 562us/step - loss: 0.0481 - acc: 0.9873 - val_loss: 0.1583 - val_acc: 0.9467\n",
      "Epoch 946/1000\n",
      "2285/2285 [==============================] - 1s 520us/step - loss: 0.0506 - acc: 0.9807 - val_loss: 0.1531 - val_acc: 0.9503\n",
      "Epoch 947/1000\n",
      "2285/2285 [==============================] - 1s 463us/step - loss: 0.0459 - acc: 0.9864 - val_loss: 0.1691 - val_acc: 0.9476\n",
      "Epoch 948/1000\n",
      "2285/2285 [==============================] - 1s 555us/step - loss: 0.0465 - acc: 0.9842 - val_loss: 0.1621 - val_acc: 0.9503\n",
      "Epoch 949/1000\n",
      "2285/2285 [==============================] - 1s 559us/step - loss: 0.0578 - acc: 0.9786 - val_loss: 0.1582 - val_acc: 0.9423\n",
      "Epoch 950/1000\n",
      "2285/2285 [==============================] - 1s 536us/step - loss: 0.0460 - acc: 0.9834 - val_loss: 0.1655 - val_acc: 0.9494\n",
      "Epoch 951/1000\n",
      "2285/2285 [==============================] - 1s 445us/step - loss: 0.0366 - acc: 0.9899 - val_loss: 0.1511 - val_acc: 0.9476\n",
      "Epoch 952/1000\n",
      "2285/2285 [==============================] - 1s 373us/step - loss: 0.0524 - acc: 0.9834 - val_loss: 0.1487 - val_acc: 0.9503\n",
      "Epoch 953/1000\n",
      "2285/2285 [==============================] - 1s 381us/step - loss: 0.0413 - acc: 0.9856 - val_loss: 0.1477 - val_acc: 0.9529\n",
      "Epoch 954/1000\n",
      "2285/2285 [==============================] - 1s 324us/step - loss: 0.0456 - acc: 0.9856 - val_loss: 0.1379 - val_acc: 0.9494\n",
      "Epoch 955/1000\n",
      "2285/2285 [==============================] - 1s 324us/step - loss: 0.0431 - acc: 0.9864 - val_loss: 0.1589 - val_acc: 0.9458\n",
      "Epoch 956/1000\n",
      "2285/2285 [==============================] - 1s 489us/step - loss: 0.0427 - acc: 0.9869 - val_loss: 0.1570 - val_acc: 0.9512\n",
      "Epoch 957/1000\n",
      "2285/2285 [==============================] - 1s 561us/step - loss: 0.0399 - acc: 0.9886 - val_loss: 0.1644 - val_acc: 0.9485\n",
      "Epoch 958/1000\n",
      "2285/2285 [==============================] - 1s 473us/step - loss: 0.0446 - acc: 0.9873 - val_loss: 0.1403 - val_acc: 0.9574\n",
      "Epoch 959/1000\n",
      "2285/2285 [==============================] - 1s 484us/step - loss: 0.0395 - acc: 0.9891 - val_loss: 0.1621 - val_acc: 0.9538\n",
      "Epoch 960/1000\n",
      "2285/2285 [==============================] - 1s 406us/step - loss: 0.0470 - acc: 0.9816 - val_loss: 0.1473 - val_acc: 0.9520\n",
      "Epoch 961/1000\n",
      "2285/2285 [==============================] - 1s 422us/step - loss: 0.0396 - acc: 0.9869 - val_loss: 0.1416 - val_acc: 0.9529\n",
      "Epoch 962/1000\n",
      "2285/2285 [==============================] - 1s 455us/step - loss: 0.0477 - acc: 0.9842 - val_loss: 0.1446 - val_acc: 0.9529\n",
      "Epoch 963/1000\n",
      "2285/2285 [==============================] - 2s 730us/step - loss: 0.0468 - acc: 0.9856 - val_loss: 0.1528 - val_acc: 0.9449\n",
      "Epoch 964/1000\n",
      "2285/2285 [==============================] - 1s 367us/step - loss: 0.0405 - acc: 0.9882 - val_loss: 0.1487 - val_acc: 0.9476\n",
      "Epoch 965/1000\n",
      "2285/2285 [==============================] - 1s 403us/step - loss: 0.0583 - acc: 0.9790 - val_loss: 0.1587 - val_acc: 0.9512\n",
      "Epoch 966/1000\n",
      "2285/2285 [==============================] - 1s 344us/step - loss: 0.0436 - acc: 0.9873 - val_loss: 0.1597 - val_acc: 0.9529\n",
      "Epoch 967/1000\n",
      "2285/2285 [==============================] - 1s 349us/step - loss: 0.0435 - acc: 0.9842 - val_loss: 0.1688 - val_acc: 0.9467\n",
      "Epoch 968/1000\n",
      "2285/2285 [==============================] - 1s 342us/step - loss: 0.0529 - acc: 0.9821 - val_loss: 0.1465 - val_acc: 0.9512\n",
      "Epoch 969/1000\n",
      "2285/2285 [==============================] - 1s 416us/step - loss: 0.0427 - acc: 0.9873 - val_loss: 0.1606 - val_acc: 0.9458\n",
      "Epoch 970/1000\n",
      "2285/2285 [==============================] - 1s 395us/step - loss: 0.0482 - acc: 0.9842 - val_loss: 0.1453 - val_acc: 0.9512\n",
      "Epoch 971/1000\n",
      "2285/2285 [==============================] - 1s 389us/step - loss: 0.0558 - acc: 0.9807 - val_loss: 0.1574 - val_acc: 0.9565\n",
      "Epoch 972/1000\n",
      "2285/2285 [==============================] - 1s 358us/step - loss: 0.0410 - acc: 0.9891 - val_loss: 0.1477 - val_acc: 0.9538\n",
      "Epoch 973/1000\n",
      "2285/2285 [==============================] - 1s 435us/step - loss: 0.0417 - acc: 0.9856 - val_loss: 0.1553 - val_acc: 0.9494\n",
      "Epoch 974/1000\n",
      "2285/2285 [==============================] - 1s 371us/step - loss: 0.0512 - acc: 0.9816 - val_loss: 0.1520 - val_acc: 0.9467\n",
      "Epoch 975/1000\n",
      "2285/2285 [==============================] - 1s 464us/step - loss: 0.0488 - acc: 0.9812 - val_loss: 0.1525 - val_acc: 0.9520\n",
      "Epoch 976/1000\n",
      "2285/2285 [==============================] - 1s 477us/step - loss: 0.0434 - acc: 0.9877 - val_loss: 0.1324 - val_acc: 0.9547\n",
      "Epoch 977/1000\n",
      "2285/2285 [==============================] - 1s 414us/step - loss: 0.0382 - acc: 0.9869 - val_loss: 0.1529 - val_acc: 0.9520\n",
      "Epoch 978/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.0527 - acc: 0.9842 - val_loss: 0.1721 - val_acc: 0.9467\n",
      "Epoch 979/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.0379 - acc: 0.9904 - val_loss: 0.1563 - val_acc: 0.9503\n",
      "Epoch 980/1000\n",
      "2285/2285 [==============================] - 1s 421us/step - loss: 0.0472 - acc: 0.9856 - val_loss: 0.1553 - val_acc: 0.9529\n",
      "Epoch 981/1000\n",
      "2285/2285 [==============================] - 1s 411us/step - loss: 0.0488 - acc: 0.9834 - val_loss: 0.1524 - val_acc: 0.9485\n",
      "Epoch 982/1000\n",
      "2285/2285 [==============================] - 1s 409us/step - loss: 0.0588 - acc: 0.9812 - val_loss: 0.1451 - val_acc: 0.9574\n",
      "Epoch 983/1000\n",
      "2285/2285 [==============================] - 1s 409us/step - loss: 0.0463 - acc: 0.9860 - val_loss: 0.1527 - val_acc: 0.9512\n",
      "Epoch 984/1000\n",
      "2285/2285 [==============================] - 1s 385us/step - loss: 0.0433 - acc: 0.9834 - val_loss: 0.1379 - val_acc: 0.9547\n",
      "Epoch 985/1000\n",
      "2285/2285 [==============================] - 1s 399us/step - loss: 0.0346 - acc: 0.9904 - val_loss: 0.1667 - val_acc: 0.9476\n",
      "Epoch 986/1000\n",
      "2285/2285 [==============================] - 1s 416us/step - loss: 0.0521 - acc: 0.9860 - val_loss: 0.1554 - val_acc: 0.9503\n",
      "Epoch 987/1000\n",
      "2285/2285 [==============================] - 1s 409us/step - loss: 0.0422 - acc: 0.9864 - val_loss: 0.1483 - val_acc: 0.9574\n",
      "Epoch 988/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285/2285 [==============================] - 1s 410us/step - loss: 0.0297 - acc: 0.9926 - val_loss: 0.1627 - val_acc: 0.9556\n",
      "Epoch 989/1000\n",
      "2285/2285 [==============================] - 1s 406us/step - loss: 0.0424 - acc: 0.9882 - val_loss: 0.1531 - val_acc: 0.9556\n",
      "Epoch 990/1000\n",
      "2285/2285 [==============================] - 1s 407us/step - loss: 0.0483 - acc: 0.9864 - val_loss: 0.1632 - val_acc: 0.9512\n",
      "Epoch 991/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.0557 - acc: 0.9821 - val_loss: 0.1462 - val_acc: 0.9512\n",
      "Epoch 992/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.0371 - acc: 0.9882 - val_loss: 0.1500 - val_acc: 0.9547\n",
      "Epoch 993/1000\n",
      "2285/2285 [==============================] - 1s 417us/step - loss: 0.0392 - acc: 0.9882 - val_loss: 0.1588 - val_acc: 0.9485\n",
      "Epoch 994/1000\n",
      "2285/2285 [==============================] - 1s 405us/step - loss: 0.0395 - acc: 0.9873 - val_loss: 0.1594 - val_acc: 0.9467\n",
      "Epoch 995/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.0422 - acc: 0.9877 - val_loss: 0.1417 - val_acc: 0.9520\n",
      "Epoch 996/1000\n",
      "2285/2285 [==============================] - 1s 412us/step - loss: 0.0377 - acc: 0.9882 - val_loss: 0.1539 - val_acc: 0.9503\n",
      "Epoch 997/1000\n",
      "2285/2285 [==============================] - 1s 409us/step - loss: 0.0496 - acc: 0.9838 - val_loss: 0.1812 - val_acc: 0.9423\n",
      "Epoch 998/1000\n",
      "2285/2285 [==============================] - 1s 409us/step - loss: 0.0383 - acc: 0.9882 - val_loss: 0.1714 - val_acc: 0.9405\n",
      "Epoch 999/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.0395 - acc: 0.9886 - val_loss: 0.1644 - val_acc: 0.9378\n",
      "Epoch 1000/1000\n",
      "2285/2285 [==============================] - 1s 408us/step - loss: 0.0397 - acc: 0.9869 - val_loss: 0.1500 - val_acc: 0.9529\n"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=1000, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(cnnhistory.history['acc'])\n",
    "plt.plot(cnnhistory.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       378\n",
      "           1       0.96      0.92      0.94       238\n",
      "           2       0.95      0.94      0.94       267\n",
      "           3       0.94      0.96      0.95       243\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1126\n",
      "   macro avg       0.95      0.95      0.95      1126\n",
      "weighted avg       0.95      0.95      0.95      1126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_classes(x_testcnn)\n",
    "new_Ytest = y_test.astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(new_Ytest, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[371   0   3   4]\n",
      " [  5 219  10   4]\n",
      " [  4   7 250   6]\n",
      " [  8   2   0 233]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(new_Ytest, predictions)\n",
    "print (matrix)\n",
    "# 0 = neutral, 1 = angry, 2 = happy, 3 = sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /Users/erenmac/Desktop/ENGR498/Code/Voice_Emo_Rec_Models/Emotion_Voice_Detection_Model_Reduced_Labels.h5 \n"
     ]
    }
   ],
   "source": [
    "model_name = 'Emotion_Voice_Detection_Model_Reduced_Labels.h5'\n",
    "save_dir = '/Users/erenmac/Desktop/ENGR498/Code/Voice_Emo_Rec_Models'\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126/1126 [==============================] - 0s 173us/step\n",
      "Restored model, accuracy: 95.29%\n"
     ]
    }
   ],
   "source": [
    "loaded_model = keras.models.load_model('Emotion_Voice_Detection_Model_Reduced_Labels.h5')\n",
    "loss, acc = loaded_model.evaluate(x_testcnn, y_test)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5- MEAN DB AMPLITUDE MODEL AND .T VERSION (Soruce Data - LESS Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = joblib.load('/Users/erenmac/Desktop/NEW_DATA_VOICE/Audio_Speech_Actors_01-24/Joblib_saves/X_wave.joblib')\n",
    "y = joblib.load('/Users/erenmac/Desktop/NEW_DATA_VOICE/Audio_Speech_Actors_01-24/Joblib_saves/y_wave.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440\n"
     ]
    }
   ],
   "source": [
    "X_db = []\n",
    "for item in X:\n",
    "    S = librosa.feature.melspectrogram(y=item, sr=sr)\n",
    "    k = np.mean(librosa.amplitude_to_db(S, ref=np.max).T,axis=0)\n",
    "    X_db.append(k)\n",
    "    \n",
    "print (len(X_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.asarray(X_db), y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DecisionTree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, y_train)\n",
    "predictions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.34      0.31        32\n",
      "           1       0.53      0.49      0.51        59\n",
      "           2       0.24      0.21      0.22        67\n",
      "           3       0.24      0.22      0.23        59\n",
      "           4       0.25      0.22      0.23        68\n",
      "           5       0.26      0.28      0.27        61\n",
      "           6       0.30      0.33      0.31        64\n",
      "           7       0.33      0.36      0.35        66\n",
      "\n",
      "   micro avg       0.30      0.30      0.30       476\n",
      "   macro avg       0.30      0.31      0.30       476\n",
      "weighted avg       0.30      0.30      0.30       476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- SAVEE DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1- Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/erenmac/Desktop/NEW_DATA_VOICE/AudioData/1'\n",
    "lst = []\n",
    "numlist = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if os.path.join(subdir,file).endswith('.DS_Store'):\n",
    "            continue\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file))\n",
    "        file = file[0:2]\n",
    "        if file[1] in numlist:\n",
    "            file = file[0]\n",
    "        arr = X, file\n",
    "        lst.append(arr)\n",
    "        if sample_rate != 22050:\n",
    "            print (sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = zip(*lst)\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised\n",
    "label_to_num = {\"a\":4,\"d\":6,\"f\":5,\"h\":2 ,\"n\":0 ,\"sa\":3 ,\"su\":7}\n",
    "newy = []\n",
    "for item in y:\n",
    "    newy.append(label_to_num[item])\n",
    "y = newy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_name = 'X.joblib'\n",
    "y_name = 'y.joblib'\n",
    "save_dir = '/Users/erenmac/Desktop/NEW_DATA_VOICE/AudioData/Joblib'\n",
    "\n",
    "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
    "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2- Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = joblib.load('/Users/erenmac/Desktop/NEW_DATA_VOICE/AudioData/Joblib/X.joblib')\n",
    "y = joblib.load('/Users/erenmac/Desktop/NEW_DATA_VOICE/AudioData/Joblib/y.joblib')\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- GERMAN DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1- Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/erenmac/Desktop/NEW_DATA_VOICE/German_DB/wav'\n",
    "lst = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if os.path.join(subdir,file).endswith('.DS_Store'):\n",
    "            continue\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file))\n",
    "        file = file[5]\n",
    "        arr = X, file\n",
    "        lst.append(arr)\n",
    "        if sample_rate != 22050:\n",
    "            print (sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = zip(*lst)\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised\n",
    "label_to_num = {\"W\":4,\"L\":1,\"E\":6,\"A\":5 ,\"F\":2 ,\"T\":3,\"N\":0}\n",
    "newy = []\n",
    "for item in y:\n",
    "    newy.append(label_to_num[item])\n",
    "y = newy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_name = 'X.joblib'\n",
    "y_name = 'y.joblib'\n",
    "save_dir = '/Users/erenmac/Desktop/NEW_DATA_VOICE/German_DB/Joblib'\n",
    "\n",
    "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
    "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2- Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = joblib.load('/Users/erenmac/Desktop/NEW_DATA_VOICE/German_DB/Joblib/X.joblib')\n",
    "y = joblib.load('/Users/erenmac/Desktop/NEW_DATA_VOICE/German_DB/Joblib/y.joblib')\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- TESS DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1- Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1- Web Crawling (Database items are available at different links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ssl\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "import shutil\n",
    "import requests\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and\n",
    "    getattr(ssl, '_create_unverified_context', None)): \n",
    "    ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "starturl = \"https://tspace.library.utoronto.ca/handle/1807/24501\"\n",
    "initbaseurl = \"https://tspace.library.utoronto.ca/handle/1807/\"\n",
    "baseurl = \"https://tspace.library.utoronto.ca\"\n",
    "urllist = []\n",
    "x = int(starturl[-5:])\n",
    "while x > 24487:\n",
    "    urllist.append(initbaseurl + str(x))\n",
    "    x = x-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_urls = []\n",
    "\n",
    "for item in urllist:\n",
    "    r = requests.get(item)\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data)\n",
    "    for link in soup.find_all('a'):\n",
    "        if link.get('href').endswith(\".wav\"):\n",
    "            data_urls.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised\n",
    "label_to_num = {\"ps\":7,\"angry\":4,\"fear\":5,\"neurtal\":0 ,\"happy\":2 ,\"disgust\":6 ,\"sad\":3}\n",
    "labellist = [\"ps.wav\",\"angry.wav\",\"fear.wav\",\"neutral.wav\",\"happy.wav\",\"disgust.wav\",\"sad.wav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_file_path = \"/Users/erenmac/Desktop/NEW_DATA_VOICE/TESS_Database/\"\n",
    "i = 0\n",
    "for data in data_urls:\n",
    "    for item in labellist:\n",
    "        if item in data:\n",
    "            url = baseurl + data\n",
    "            output_file = output_file_path+str(i)+\"_\"+ item\n",
    "            with urllib.request.urlopen(url) as response, open(output_file, 'wb') as out_file:\n",
    "                shutil.copyfileobj(response, out_file)\n",
    "            i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2- Data Extraction & Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/erenmac/Desktop/NEW_DATA_VOICE/TESS_Database'\n",
    "lst = []\n",
    "# 0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised\n",
    "label_to_num = {\"ps\":7,\"angry\":4,\"fear\":5,\"neutral\":0 ,\"happy\":2 ,\"disgust\":6 ,\"sad\":3}\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if os.path.join(subdir,file).endswith('.DS_Store'):\n",
    "            continue\n",
    "        X, sample_rate = librosa.load(os.path.join(subdir,file))\n",
    "        for item in label_to_num:\n",
    "            if item in str(file):\n",
    "                file = label_to_num[item]\n",
    "        arr = X, file\n",
    "        lst.append(arr)\n",
    "        if sample_rate != 22050:\n",
    "            print (sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = zip(*lst)\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_name = 'X.joblib'\n",
    "y_name = 'y.joblib'\n",
    "save_dir = '/Users/erenmac/Desktop/NEW_DATA_VOICE/TESS_Database/Joblib'\n",
    "\n",
    "savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
    "savedy = joblib.dump(y, os.path.join(save_dir, y_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2- Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = joblib.load('/Users/erenmac/Desktop/NEW_DATA_VOICE/TESS_Database/Joblib/X.joblib')\n",
    "y = joblib.load('/Users/erenmac/Desktop/NEW_DATA_VOICE/TESS_Database/Joblib/y.joblib')\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------MASS MODEL BUILDING----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Data Preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1- Gathering All The Datasets Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2- Saving The Big Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3- Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1- MEAN MFCCs KERAS MODEL (/W using RAVDESS big data and other databases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2- MEAN MFCCs REDUCED KERAS MODEL (/W using RAVDESS big data and other databases.) (4 LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3- MEAN DB AMPLITUDE MODEL AND .T VERSION (/W using RAVDESS small data and other databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4- WAVELET DATA EXPLORATION & MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5- SAVING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bla bla bla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Model Testing & Evaluating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bla bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
